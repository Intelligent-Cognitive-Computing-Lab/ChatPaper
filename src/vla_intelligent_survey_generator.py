#!/usr/bin/env python3
"""
VLAæ™ºèƒ½ç»¼è¿°ç”Ÿæˆå™¨ - AI-Powered Survey Generator
ç»“åˆå¤§æ¨¡å‹å’Œåˆ†æç»“æœï¼Œé€æ­¥ç”Ÿæˆé«˜è´¨é‡VLAç»¼è¿°

åŠŸèƒ½:
1. åŸºäºåˆ†æç»“æœçš„æ¡†æ¶ç”Ÿæˆ
2. æ™ºèƒ½å†…å®¹åˆ†ç±»å’Œèšç±»
3. é€ç« èŠ‚ç»¼è¿°æ’°å†™
4. æŠ€æœ¯æ¼”è¿›åˆ†æ
5. ç ”ç©¶è¶‹åŠ¿é¢„æµ‹
6. å®Œæ•´ç»¼è¿°æŠ¥å‘Šç”Ÿæˆ
"""

import pandas as pd
import numpy as np
import json
import os
import openai
from collections import defaultdict, Counter
import re
from datetime import datetime
import argparse
import configparser
from typing import Dict, List, Any, Optional
import time
import tiktoken

class VLAIntelligentSurveyGenerator:
    def __init__(self, csv_path: str, analysis_results_dir: str, config_path: str = "config/apikey.ini"):
        """åˆå§‹åŒ–æ™ºèƒ½ç»¼è¿°ç”Ÿæˆå™¨"""
        self.csv_path = csv_path
        self.analysis_results_dir = analysis_results_dir
        self.config_path = config_path
        self.df = pd.DataFrame()
        self.analysis_data = {}
        self.classification_data = {}
        self.client = None
        self.model_name = "gpt-4"
        
        # åŠ è½½æ•°æ®å’Œé…ç½®
        self.load_data()
        self.load_analysis_results()
        self.setup_openai()
        
    def load_data(self):
        """åŠ è½½åŸå§‹CSVæ•°æ®"""
        try:
            self.df = pd.read_csv(self.csv_path, encoding='utf-8')
            print(f"âœ… åŠ è½½åŸå§‹æ•°æ®: {len(self.df)} ç¯‡è®ºæ–‡")
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            
    def load_analysis_results(self):
        """åŠ è½½åˆ†æç»“æœ"""
        try:
            # åŠ è½½åˆ†ç±»æ€»ç»“JSON
            classification_path = os.path.join(self.analysis_results_dir, "vla_classification_summary.json")
            if os.path.exists(classification_path):
                with open(classification_path, 'r', encoding='utf-8') as f:
                    self.classification_data = json.load(f)
                print(f"âœ… åŠ è½½åˆ†ç±»æ•°æ®: {self.classification_data['metadata']['total_papers']} ç¯‡è®ºæ–‡")
            
            # è¯»å–ç»¼è¿°æŠ¥å‘Šä½œä¸ºå‚è€ƒ
            report_path = os.path.join(self.analysis_results_dir, "vla_survey_report.md")
            if os.path.exists(report_path):
                with open(report_path, 'r', encoding='utf-8') as f:
                    self.analysis_report = f.read()
                print("âœ… åŠ è½½åˆ†ææŠ¥å‘Š")
                
        except Exception as e:
            print(f"âš ï¸ åˆ†æç»“æœåŠ è½½å¤±è´¥: {e}")
            
    def setup_openai(self):
        """è®¾ç½®OpenAIå®¢æˆ·ç«¯"""
        try:
            config = configparser.ConfigParser()
            config.read(self.config_path)
            
            api_base = config.get('OpenAI', 'OPENAI_API_BASE')
            api_keys = eval(config.get('OpenAI', 'OPENAI_API_KEYS'))
            self.model_name = config.get('OpenAI', 'CHATGPT_MODEL', fallback='gpt-4')
            
            # è®¾ç½®openaié…ç½®
            openai.api_base = api_base
            openai.api_key = api_keys[0] if isinstance(api_keys, list) else api_keys
            
            print(f"âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {self.model_name}")
            
        except Exception as e:
            print(f"âŒ OpenAIé…ç½®å¤±è´¥: {e}")
            
    def call_llm(self, messages: List[Dict], max_tokens: int = 4000, temperature: float = 0.3) -> str:
        """è°ƒç”¨å¤§æ¨¡å‹"""
        try:
            # ä½¿ç”¨ä¸chat_paper_simple.pyç›¸åŒçš„è°ƒç”¨æ–¹å¼
            if hasattr(openai, 'api_type') and openai.api_type == 'azure':
                response = openai.ChatCompletion.create(
                    engine=self.model_name,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
            else:
                response = openai.ChatCompletion.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
            
            # è·å–å“åº”å†…å®¹
            result = ''
            for choice in response.choices:
                result += choice.message.content
            
            # æ˜¾ç¤ºtokenä½¿ç”¨æƒ…å†µ
            if hasattr(response, 'usage'):
                print(f"Tokenä½¿ç”¨ - è¾“å…¥: {response.usage.prompt_tokens}, è¾“å‡º: {response.usage.completion_tokens}, æ€»è®¡: {response.usage.total_tokens}")
            
            return result
                
        except Exception as e:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return f"[LLMè°ƒç”¨å¤±è´¥] é”™è¯¯: {str(e)}"
            
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°é‡"""
        try:
            encoding = tiktoken.encoding_for_model(self.model_name)
            return len(encoding.encode(text))
        except:
            # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
            english_chars = len(text) - chinese_chars
            return int(chinese_chars / 1.5 + english_chars / 4)
    
    def generate_survey_framework(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼è¿°æ¡†æ¶"""
        print("\nğŸ—ï¸ ç”Ÿæˆç»¼è¿°æ¡†æ¶...")
        
        # å‡†å¤‡æ•°æ®æ‘˜è¦
        architecture_dist = self.classification_data.get('architecture_classification', {})
        bottleneck_dist = self.classification_data.get('bottleneck_classification', {})
        overall_trends = self.classification_data.get('overall_trends', {})
        
        prompt = f"""åŸºäºä»¥ä¸‹VLAæ¨¡å‹ç ”ç©¶æ•°æ®ï¼Œè®¾è®¡ä¸€ä¸ªç³»ç»Ÿæ€§çš„ç»¼è¿°æ¡†æ¶ï¼š

## æ•°æ®æ¦‚è§ˆ
- æ€»è®ºæ–‡æ•°: {self.classification_data.get('metadata', {}).get('total_papers', 0)}ç¯‡
- ä¸»æµæ¶æ„: {overall_trends.get('most_common_architecture', 'æœªçŸ¥')}
- æ•°æ®ç“¶é¢ˆæ¯”ä¾‹: {overall_trends.get('bottleneck_distribution', {}).get('data_bottleneck_ratio', 0):.1%}
- ç®—åŠ›ç“¶é¢ˆæ¯”ä¾‹: {overall_trends.get('bottleneck_distribution', {}).get('compute_bottleneck_ratio', 0):.1%}

## æ¶æ„åˆ†å¸ƒ
{json.dumps(architecture_dist, ensure_ascii=False, indent=2)}

## ç“¶é¢ˆåˆ†ç±»
{json.dumps(bottleneck_dist, ensure_ascii=False, indent=2)}

è¯·è®¾è®¡ä¸€ä¸ªç»“æ„åŒ–çš„ç»¼è¿°æ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼š
1. ä¸»è¦ç« èŠ‚å’Œå­ç« èŠ‚
2. æ¯ä¸ªç« èŠ‚çš„æ ¸å¿ƒå†…å®¹å’Œç ”ç©¶é—®é¢˜
3. ç« èŠ‚é—´çš„é€»è¾‘å…³ç³»
4. é¢„æœŸçš„é¡µæ•°åˆ†é…

è¦æ±‚ï¼š
- çªå‡ºèµ„æºå—é™è¿™ä¸€æ ¸å¿ƒä¸»é¢˜
- ä½“ç°VLAæ¨¡å‹çš„æŠ€æœ¯æ¼”è¿›
- åŒ…å«å®šé‡åˆ†æå’Œå®šæ€§æ€»ç»“
- é€‚åˆé¡¶çº§æœŸåˆŠå‘è¡¨

è¯·ä»¥JSONæ ¼å¼è¿”å›æ¡†æ¶ç»“æ„ã€‚"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIç ”ç©¶å‘˜ï¼Œæ“…é•¿æ’°å†™é«˜è´¨é‡çš„ç»¼è¿°è®ºæ–‡ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        response = self.call_llm(messages, max_tokens=3000)
        
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                framework = json.loads(json_match.group())
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œä½¿ç”¨é»˜è®¤æ¡†æ¶
                framework = self._get_default_framework()
        except:
            framework = self._get_default_framework()
        
        print("âœ… ç»¼è¿°æ¡†æ¶ç”Ÿæˆå®Œæˆ")
        return framework
    
    def _get_default_framework(self) -> Dict[str, Any]:
        """é»˜è®¤ç»¼è¿°æ¡†æ¶"""
        return {
            "title": "Vision-Language-Action Models under Resource Constraints: A Comprehensive Survey",
            "sections": [
                {
                    "name": "Introduction",
                    "subsections": ["Background", "Motivation", "Contributions", "Organization"],
                    "pages": 3
                },
                {
                    "name": "Background and Related Work",
                    "subsections": ["VLA Model Evolution", "Resource Constraints in AI", "Efficiency Optimization Techniques"],
                    "pages": 4
                },
                {
                    "name": "VLA Architecture Taxonomy",
                    "subsections": ["End-to-End VLA", "Hierarchical VLA", "Hybrid Architectures", "Comparative Analysis"],
                    "pages": 6
                },
                {
                    "name": "Resource Bottleneck Analysis",
                    "subsections": ["Data Bottlenecks", "Compute Bottlenecks", "Combined Constraints", "Impact Assessment"],
                    "pages": 5
                },
                {
                    "name": "Solution Strategies and Innovations",
                    "subsections": ["Data Efficiency Techniques", "Compute Optimization Methods", "Architectural Innovations", "Case Studies"],
                    "pages": 8
                },
                {
                    "name": "Performance Analysis and Benchmarking",
                    "subsections": ["Evaluation Metrics", "Benchmark Datasets", "Performance Trends", "Resource-Performance Trade-offs"],
                    "pages": 4
                },
                {
                    "name": "Future Directions and Open Challenges",
                    "subsections": ["Emerging Trends", "Technical Challenges", "Research Opportunities", "Industry Applications"],
                    "pages": 3
                },
                {
                    "name": "Conclusion",
                    "subsections": ["Key Findings", "Implications", "Final Remarks"],
                    "pages": 2
                }
            ]
        }
    
    def classify_papers_by_themes(self, framework: Dict[str, Any]) -> Dict[str, List[Dict]]:
        """æŒ‰ä¸»é¢˜åˆ†ç±»è®ºæ–‡"""
        print("\nğŸ“š æŒ‰ä¸»é¢˜åˆ†ç±»è®ºæ–‡...")
        
        themes = {}
        
        # ä¸ºæ¯ä¸ªä¸»è¦ç« èŠ‚åˆ›å»ºåˆ†ç±»
        for section in framework.get('sections', []):
            section_name = section['name']
            if section_name in ['Introduction', 'Conclusion', 'Background and Related Work']:
                continue  # è·³è¿‡éæŠ€æœ¯ç« èŠ‚
                
            themes[section_name] = []
        
        # åŸºäºæ¶æ„ç±»å‹åˆ†ç±»
        for arch_type, arch_data in self.classification_data.get('architecture_classification', {}).items():
            if arch_data.get('paper_count', 0) > 0:
                papers_info = []
                arch_papers = self.df[self.df['VLAæ¶æ„ç±»å‹'] == arch_type]
                
                for _, paper in arch_papers.iterrows():
                    paper_info = {
                        'title': paper.get('è®ºæ–‡æ ‡é¢˜', ''),
                        'authors': paper.get('ä½œè€…', ''),
                        'year': paper.get('å‘è¡¨å¹´ä»½', ''),
                        'architecture': paper.get('VLAæ¶æ„ç±»å‹', ''),
                        'data_bottleneck': paper.get('æ•°æ®ç“¶é¢ˆ', ''),
                        'compute_bottleneck': paper.get('ç®—åŠ›ç“¶é¢ˆ', ''),
                        'innovation': paper.get('åˆ›æ–°ç‚¹', ''),
                        'contribution': paper.get('ä¸»è¦è´¡çŒ®/ç›®æ ‡', ''),
                        'performance': paper.get('æ€§èƒ½æŒ‡æ ‡', ''),
                        'limitations': paper.get('ç¼ºç‚¹/å±€é™', ''),
                        'future_work': paper.get('æœªæ¥æ–¹å‘', '')
                    }
                    papers_info.append(paper_info)
                
                # æ ¹æ®æ¶æ„ç±»å‹åˆ†é…åˆ°ç›¸åº”ä¸»é¢˜
                if 'Architecture' in themes or 'VLA Architecture Taxonomy' in themes:
                    theme_key = 'VLA Architecture Taxonomy' if 'VLA Architecture Taxonomy' in themes else 'Architecture'
                    themes[theme_key].extend(papers_info)
        
        # åŸºäºèµ„æºç“¶é¢ˆåˆ†ç±»
        bottleneck_classification = self.classification_data.get('bottleneck_classification', {})
        for bottleneck_type, bottleneck_data in bottleneck_classification.items():
            if bottleneck_data.get('paper_count', 0) > 0:
                # è¿™äº›è®ºæ–‡åº”è¯¥å½’ç±»åˆ°èµ„æºç“¶é¢ˆåˆ†æç« èŠ‚
                if 'Resource Bottleneck Analysis' in themes:
                    # è·å–å…·ä½“è®ºæ–‡ä¿¡æ¯éœ€è¦ä»åŸå§‹æ•°æ®ä¸­ç­›é€‰
                    pass  # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥ç»†åŒ–åˆ†ç±»é€»è¾‘
        
        print(f"âœ… è®ºæ–‡ä¸»é¢˜åˆ†ç±»å®Œæˆï¼Œå…±{len(themes)}ä¸ªä¸»é¢˜")
        return themes
    
    def generate_section_content(self, section_name: str, subsections: List[str], 
                                related_papers: List[Dict], max_pages: int = 5) -> str:
        """ç”Ÿæˆç« èŠ‚å†…å®¹"""
        print(f"\nâœï¸ ç”Ÿæˆç« èŠ‚: {section_name}")
        
        # å‡†å¤‡ç›¸å…³è®ºæ–‡æ‘˜è¦
        papers_summary = ""
        if related_papers:
            papers_summary = "\n## ç›¸å…³è®ºæ–‡æ‘˜è¦:\n"
            for i, paper in enumerate(related_papers[:20]):  # é™åˆ¶è®ºæ–‡æ•°é‡ä»¥é¿å…tokenè¶…é™
                # å®‰å…¨è·å–è®ºæ–‡ä¿¡æ¯ï¼Œå¤„ç†NaNå’ŒNoneå€¼
                title = self._safe_get_value(paper, 'title', 'Unknown')
                year = self._safe_get_value(paper, 'year', 'Unknown')
                architecture = self._safe_get_value(paper, 'architecture', 'Unknown')
                contribution = self._safe_get_value(paper, 'contribution', 'Unknown')
                innovation = self._safe_get_value(paper, 'innovation', 'Unknown')
                
                papers_summary += f"**{i+1}. {title}** ({year})\n"
                papers_summary += f"- æ¶æ„: {architecture}\n"
                papers_summary += f"- è´¡çŒ®: {contribution[:200]}...\n"
                papers_summary += f"- åˆ›æ–°ç‚¹: {innovation[:150]}...\n\n"
        
        # æ£€æŸ¥tokenæ•°é‡
        base_prompt_tokens = self.count_tokens(papers_summary)
        if base_prompt_tokens > 15000:  # å¦‚æœå¤ªé•¿ï¼Œæˆªæ–­è®ºæ–‡æ‘˜è¦
            papers_summary = papers_summary[:15000] + "\n...(æ›´å¤šè®ºæ–‡ä¿¡æ¯å·²æˆªæ–­)"
        
        prompt = f"""è¯·ä¸ºVLAæ¨¡å‹èµ„æºå—é™ç ”ç©¶ç»¼è¿°æ’°å†™"{section_name}"ç« èŠ‚ã€‚

## ç« èŠ‚è¦æ±‚:
- å­ç« èŠ‚: {', '.join(subsections)}
- é¢„æœŸé¡µæ•°: {max_pages}é¡µ
- å­¦æœ¯æ°´å‡†: é¡¶çº§æœŸåˆŠæ ‡å‡†
- è¯­è¨€: ä¸­æ–‡ï¼Œä¸“ä¸šæœ¯è¯­ä¿ç•™è‹±æ–‡

## å†™ä½œè¦æ±‚:
1. ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘ä¸¥è°¨
2. åŒ…å«å®šé‡åˆ†æå’Œå®šæ€§æ€»ç»“
3. çªå‡ºèµ„æºå—é™è¿™ä¸€æ ¸å¿ƒä¸»é¢˜
4. å¼•ç”¨å…·ä½“è®ºæ–‡å’Œæ•°æ®
5. ä¿æŒå®¢è§‚å’Œæ‰¹åˆ¤æ€§åˆ†æ

{papers_summary}

## ç»Ÿè®¡æ•°æ®å‚è€ƒ:
- ç«¯åˆ°ç«¯VLA: {self.classification_data.get('architecture_classification', {}).get('ç«¯åˆ°ç«¯VLA', {}).get('paper_count', 0)}ç¯‡
- æ··åˆæ¶æ„: {self.classification_data.get('architecture_classification', {}).get('æ··åˆæ¶æ„', {}).get('paper_count', 0)}ç¯‡  
- åˆ†å±‚å¼VLA: {self.classification_data.get('architecture_classification', {}).get('åˆ†å±‚å¼VLA', {}).get('paper_count', 0)}ç¯‡
- åŒé‡ç“¶é¢ˆ: {self.classification_data.get('bottleneck_classification', {}).get('both_bottlenecks', {}).get('paper_count', 0)}ç¯‡

è¯·ç”Ÿæˆå®Œæ•´çš„ç« èŠ‚å†…å®¹ï¼ŒåŒ…æ‹¬å­ç« èŠ‚åˆ’åˆ†å’Œè¯¦ç»†è®ºè¿°ã€‚"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„AIç ”ç©¶å‘˜ï¼Œä¸“é—¨ç ”ç©¶Vision-Language-Actionæ¨¡å‹ï¼Œæ“…é•¿æ’°å†™é«˜è´¨é‡çš„å­¦æœ¯ç»¼è¿°ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        content = self.call_llm(messages, max_tokens=6000, temperature=0.2)
        
        print(f"âœ… ç« èŠ‚ç”Ÿæˆå®Œæˆ: {len(content)} å­—ç¬¦")
        return content
    
    def generate_technical_analysis(self) -> str:
        """ç”ŸæˆæŠ€æœ¯åˆ†æç« èŠ‚"""
        print("\nğŸ”¬ ç”ŸæˆæŠ€æœ¯åˆ†æ...")
        
        # æå–æŠ€æœ¯å…³é”®è¯å’Œè¶‹åŠ¿
        innovations = []
        solutions = []
        
        for _, paper in self.df.iterrows():
            if paper.get('åˆ›æ–°ç‚¹') and paper.get('åˆ›æ–°ç‚¹') != 'æœªæåŠ':
                innovations.append(paper.get('åˆ›æ–°ç‚¹'))
            if paper.get('æ•°æ®ç“¶é¢ˆè§£å†³ç­–ç•¥') and paper.get('æ•°æ®ç“¶é¢ˆè§£å†³ç­–ç•¥') != 'æœªæåŠ':
                solutions.append(paper.get('æ•°æ®ç“¶é¢ˆè§£å†³ç­–ç•¥'))
            if paper.get('ç®—åŠ›ç“¶é¢ˆè§£å†³ç­–ç•¥') and paper.get('ç®—åŠ›ç“¶é¢ˆè§£å†³ç­–ç•¥') != 'æœªæåŠ':
                solutions.append(paper.get('ç®—åŠ›ç“¶é¢ˆè§£å†³ç­–ç•¥'))
        
        prompt = f"""åŸºäº163ç¯‡VLAæ¨¡å‹ç ”ç©¶è®ºæ–‡ï¼Œç”Ÿæˆæ·±åº¦æŠ€æœ¯åˆ†æç« èŠ‚ã€‚

## åˆ›æ–°ç‚¹ç»Ÿè®¡ (å‰20ä¸ª):
{str(innovations[:20])}

## è§£å†³ç­–ç•¥ç»Ÿè®¡ (å‰20ä¸ª):
{str(solutions[:20])}

## æ¶æ„åˆ†å¸ƒ:
- ç«¯åˆ°ç«¯VLA: 87ç¯‡ (53.4%)
- æ··åˆæ¶æ„: 37ç¯‡ (22.7%) 
- åˆ†å±‚å¼VLA: 37ç¯‡ (22.7%)

## èµ„æºç“¶é¢ˆç°çŠ¶:
- åŒé‡ç“¶é¢ˆ: 66ç¯‡ (40.5%)
- ä»…ç®—åŠ›ç“¶é¢ˆ: 55ç¯‡ (33.7%)
- ä»…æ•°æ®ç“¶é¢ˆ: 16ç¯‡ (9.8%)

è¯·ç”Ÿæˆä¸€ä¸ªæ·±åº¦æŠ€æœ¯åˆ†æç« èŠ‚ï¼ŒåŒ…æ‹¬ï¼š

### 5.1 æŠ€æœ¯æ¼”è¿›è·¯å¾„åˆ†æ
- VLAæ¶æ„çš„å†å²å‘å±•
- ä»ç®€å•åˆ°å¤æ‚çš„æ¼”è¿›è¶‹åŠ¿
- å…³é”®æŠ€æœ¯çªç ´ç‚¹

### 5.2 èµ„æºçº¦æŸä¸‹çš„æ¶æ„è®¾è®¡åŸåˆ™
- ç«¯åˆ°ç«¯vsåˆ†å±‚å¼çš„æƒè¡¡
- æ··åˆæ¶æ„çš„ä¼˜åŠ¿åˆ†æ
- è®¾è®¡åŸåˆ™å’Œæœ€ä½³å®è·µ

### 5.3 å…³é”®æŠ€æœ¯åˆ›æ–°åˆ†æ
- æ•°æ®æ•ˆç‡æå‡æŠ€æœ¯
- è®¡ç®—ä¼˜åŒ–æ–¹æ³•
- å¤šæ¨¡æ€èåˆåˆ›æ–°

### 5.4 æ€§èƒ½-èµ„æºæƒè¡¡ç­–ç•¥
- å®šé‡åˆ†æä¸åŒæ–¹æ³•çš„æƒè¡¡ç‚¹
- æˆæœ¬-æ•ˆç›Šåˆ†ææ¡†æ¶
- å®é™…éƒ¨ç½²è€ƒè™‘å› ç´ 

è¦æ±‚ï¼š
- æ·±åº¦æŠ€æœ¯åˆ†æï¼Œé¿å…æµ…å±‚ç½—åˆ—
- åŒ…å«å®šé‡æ•°æ®æ”¯æ’‘
- ä½“ç°æ‰¹åˆ¤æ€§æ€ç»´
- çº¦3000-4000å­—"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æŠ€æœ¯ä¸“å®¶ï¼Œä¸“é•¿äºæ·±åº¦åˆ†æAIæ¨¡å‹æ¶æ„å’Œä¼˜åŒ–æŠ€æœ¯ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        analysis = self.call_llm(messages, max_tokens=5000, temperature=0.2)
        print("âœ… æŠ€æœ¯åˆ†æç”Ÿæˆå®Œæˆ")
        return analysis
    
    def generate_future_directions(self) -> str:
        """ç”Ÿæˆæœªæ¥æ–¹å‘ç« èŠ‚"""
        print("\nğŸ”® ç”Ÿæˆæœªæ¥å‘å±•æ–¹å‘...")
        
        # æ”¶é›†æœªæ¥æ–¹å‘ä¿¡æ¯
        future_directions = []
        limitations = []
        
        for _, paper in self.df.iterrows():
            if paper.get('æœªæ¥æ–¹å‘') and paper.get('æœªæ¥æ–¹å‘') != 'æœªæåŠ':
                future_directions.append(paper.get('æœªæ¥æ–¹å‘'))
            if paper.get('ç¼ºç‚¹/å±€é™') and paper.get('ç¼ºç‚¹/å±€é™') != 'æœªæåŠ':
                limitations.append(paper.get('ç¼ºç‚¹/å±€é™'))
        
        prompt = f"""åŸºäºVLAæ¨¡å‹ç ”ç©¶ç°çŠ¶ï¼Œç”Ÿæˆå‰ç»æ€§çš„æœªæ¥å‘å±•æ–¹å‘ç« èŠ‚ã€‚

## å½“å‰å±€é™æ€§ (æ ·æœ¬):
{str(limitations[:15])}

## ç ”ç©¶è€…æå‡ºçš„æœªæ¥æ–¹å‘ (æ ·æœ¬):
{str(future_directions[:15])}

## æŠ€æœ¯ç°çŠ¶:
- å¹³å‡æˆåŠŸç‡: 62.9%
- æ ‡å‡†å·®: 28.3%
- å¹³å‡æ¨¡å‹è§„æ¨¡: 13.82Bå‚æ•°
- ä¸»è¦æŒ‘æˆ˜: æ•°æ®ç¨€ç¼ºã€è®¡ç®—å¯†é›†ã€æ³›åŒ–èƒ½åŠ›ã€å®æ—¶æ€§

è¯·ç”Ÿæˆæœªæ¥å‘å±•æ–¹å‘ç« èŠ‚ï¼ŒåŒ…æ‹¬ï¼š

### 7.1 æŠ€æœ¯å‘å±•è¶‹åŠ¿é¢„æµ‹
- çŸ­æœŸ(1-2å¹´)æŠ€æœ¯å‘å±•
- ä¸­æœŸ(3-5å¹´)æŠ€æœ¯çªç ´
- é•¿æœŸ(5-10å¹´)æŠ€æœ¯æ„¿æ™¯

### 7.2 å…³é”®æŒ‘æˆ˜ä¸æœºé‡
- å½“å‰æŠ€æœ¯ç“¶é¢ˆåˆ†æ
- æ½œåœ¨çªç ´ç‚¹è¯†åˆ«
- è·¨å­¦ç§‘åˆä½œæœºä¼š

### 7.3 æ–°å…´ç ”ç©¶æ–¹å‘
- åŸºäºåˆ†ææ•°æ®çš„æ–°å…´è¶‹åŠ¿
- æœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸ
- é«˜å½±å“åŠ›ç ”ç©¶é—®é¢˜

### 7.4 äº§ä¸šåº”ç”¨å‰æ™¯
- å®é™…éƒ¨ç½²åœºæ™¯
- å•†ä¸šåŒ–æ½œåŠ›åˆ†æ
- ç¤¾ä¼šå½±å“è¯„ä¼°

è¦æ±‚ï¼š
- åŸºäºæ•°æ®çš„å®¢è§‚é¢„æµ‹
- é¿å…è¿‡åº¦ä¹è§‚æˆ–æ‚²è§‚
- ç»“åˆå½“å‰æŠ€æœ¯é™åˆ¶
- æä¾›å¯è¡Œçš„ç ”ç©¶å»ºè®®
- çº¦2500-3000å­—"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„AIç ”ç©¶æˆ˜ç•¥ä¸“å®¶ï¼Œæ“…é•¿æŠ€æœ¯è¶‹åŠ¿é¢„æµ‹å’Œç ”ç©¶æ–¹å‘è§„åˆ’ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        future_content = self.call_llm(messages, max_tokens=4000, temperature=0.3)
        print("âœ… æœªæ¥æ–¹å‘ç”Ÿæˆå®Œæˆ")
        return future_content
    
    def generate_complete_survey(self, output_path: str = "complete_vla_survey.md") -> str:
        """ç”Ÿæˆå®Œæ•´ç»¼è¿°"""
        print("\nğŸ“ ç”Ÿæˆå®Œæ•´ç»¼è¿°...")
        
        # 1. ç”Ÿæˆæ¡†æ¶
        framework = self.generate_survey_framework()
        
        # 2. åˆ†ç±»è®ºæ–‡
        paper_themes = self.classify_papers_by_themes(framework)
        
        # 3. ç”Ÿæˆå„ç« èŠ‚
        complete_survey = f"""# {framework.get('title', 'VLA Models under Resource Constraints: A Comprehensive Survey')}

**æ‘˜è¦**: æœ¬ç»¼è¿°ç³»ç»Ÿåˆ†æäº†163ç¯‡å…³äºVision-Language-Action (VLA)æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„ç ”ç©¶è®ºæ–‡ï¼Œæ¶µç›–äº†ä»2023å¹´åˆ°2025å¹´çš„æœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬å‘ç°ç«¯åˆ°ç«¯VLAæ¶æ„å ä¸»å¯¼åœ°ä½(53.4%)ï¼Œä½†40.5%çš„ç ”ç©¶é¢ä¸´æ•°æ®å’Œç®—åŠ›åŒé‡ç“¶é¢ˆã€‚é€šè¿‡å®šé‡åˆ†æå’Œå®šæ€§æ€»ç»“ï¼Œæœ¬æ–‡ä¸ºVLAæ¨¡å‹çš„é«˜æ•ˆéƒ¨ç½²å’Œä¼˜åŒ–æä¾›äº†ç³»ç»Ÿæ€§æŒ‡å¯¼ã€‚

**å…³é”®è¯**: Vision-Language-Action, èµ„æºçº¦æŸ, æ¨¡å‹ä¼˜åŒ–, å¤šæ¨¡æ€å­¦ä¹ , å…·èº«æ™ºèƒ½

---

"""
        
        # ç”Ÿæˆä¸»è¦æŠ€æœ¯ç« èŠ‚
        sections_to_generate = [
            'VLA Architecture Taxonomy',
            'Resource Bottleneck Analysis', 
            'Solution Strategies and Innovations',
            'Performance Analysis and Benchmarking'
        ]
        
        for section in framework.get('sections', []):
            section_name = section['name']
            
            if section_name == 'Introduction':
                complete_survey += self._generate_introduction()
            elif section_name in sections_to_generate:
                subsections = section.get('subsections', [])
                max_pages = section.get('pages', 5)
                related_papers = paper_themes.get(section_name, [])
                
                content = self.generate_section_content(
                    section_name, subsections, related_papers, max_pages
                )
                complete_survey += f"\n## {section_name}\n\n{content}\n\n"
                
            elif section_name == 'Future Directions and Open Challenges':
                complete_survey += f"\n## {section_name}\n\n"
                complete_survey += self.generate_future_directions()
                complete_survey += "\n\n"
                
            elif section_name == 'Conclusion':
                complete_survey += self._generate_conclusion()
        
        # æ·»åŠ æŠ€æœ¯åˆ†æ
        complete_survey += "\n## æ·±åº¦æŠ€æœ¯åˆ†æ\n\n"
        complete_survey += self.generate_technical_analysis()
        complete_survey += "\n\n"
        
        # ä¿å­˜å®Œæ•´ç»¼è¿°
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(complete_survey)
        
        print(f"âœ… å®Œæ•´ç»¼è¿°å·²ç”Ÿæˆ: {output_path}")
        print(f"ğŸ“Š ç»¼è¿°é•¿åº¦: {len(complete_survey)} å­—ç¬¦")
        return complete_survey
    
    def _generate_introduction(self) -> str:
        """ç”Ÿæˆå¼•è¨€ç« èŠ‚"""
        return """## 1. Introduction

Vision-Language-Action (VLA) æ¨¡å‹ä½œä¸ºå…·èº«æ™ºèƒ½é¢†åŸŸçš„é‡è¦çªç ´ï¼Œé€šè¿‡æ•´åˆè§†è§‰æ„ŸçŸ¥ã€è¯­è¨€ç†è§£å’ŒåŠ¨ä½œæ‰§è¡Œèƒ½åŠ›ï¼Œä¸ºæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„è‡ªä¸»å†³ç­–æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼ŒVLAæ¨¡å‹é¢ä¸´ç€ä¸¥é‡çš„èµ„æºçº¦æŸæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®è·å–å›°éš¾ã€è®¡ç®—èµ„æºæœ‰é™ã€å­˜å‚¨å®¹é‡ä¸è¶³ç­‰é—®é¢˜ã€‚

### 1.1 ç ”ç©¶èƒŒæ™¯

éšç€æ·±åº¦å­¦ä¹ å’Œå¤§æ¨¡å‹æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒVLAæ¨¡å‹åœ¨æœºå™¨äººæ“ä½œã€å¯¼èˆªå’Œäº¤äº’ä»»åŠ¡ä¸­å±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›ã€‚ä»æ—©æœŸçš„ç«¯åˆ°ç«¯å­¦ä¹ æ–¹æ³•åˆ°ç°åœ¨çš„åˆ†å±‚å¼æ¶æ„å’Œæ··åˆç³»ç»Ÿï¼ŒVLAæ¨¡å‹çš„è®¾è®¡ç†å¿µç»å†äº†æ˜¾è‘—å˜åŒ–ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„å®é™…éƒ¨ç½²ä»ç„¶é¢ä¸´ç€èµ„æºå—é™çš„ç°å®æŒ‘æˆ˜ã€‚

### 1.2 ç ”ç©¶åŠ¨æœº

æœ¬ç»¼è¿°çš„ç ”ç©¶åŠ¨æœºæºäºä»¥ä¸‹å‡ ä¸ªå…³é”®è§‚å¯Ÿï¼š
1. **æ•°æ®ç“¶é¢ˆæ™®éå­˜åœ¨**ï¼š83ç¯‡è®ºæ–‡(50.9%)æŠ¥å‘Šäº†æ•°æ®ç“¶é¢ˆé—®é¢˜
2. **ç®—åŠ›éœ€æ±‚æŒç»­å¢é•¿**ï¼š121ç¯‡è®ºæ–‡(74.2%)é¢ä¸´ç®—åŠ›ç“¶é¢ˆ
3. **åŒé‡çº¦æŸæ—¥ç›Šçªå‡º**ï¼š66ç¯‡è®ºæ–‡(40.5%)åŒæ—¶é¢ä¸´æ•°æ®å’Œç®—åŠ›åŒé‡ç“¶é¢ˆ
4. **è§£å†³æ–¹æ¡ˆäºŸéœ€ç³»ç»ŸåŒ–**ï¼šç°æœ‰è§£å†³ç­–ç•¥ç¼ºä¹ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶

### 1.3 ä¸»è¦è´¡çŒ®

æœ¬ç»¼è¿°çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š
- é¦–æ¬¡ç³»ç»Ÿæ€§åˆ†æäº†VLAæ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„ç ”ç©¶ç°çŠ¶
- æå‡ºäº†åŸºäºèµ„æºçº¦æŸçš„VLAæ¶æ„åˆ†ç±»ä½“ç³»
- æ€»ç»“äº†æ•°æ®æ•ˆç‡å’Œè®¡ç®—ä¼˜åŒ–çš„å…³é”®æŠ€æœ¯
- é¢„æµ‹äº†æœªæ¥VLAæ¨¡å‹å‘å±•çš„æŠ€æœ¯è¶‹åŠ¿

### 1.4 æ–‡ç« ç»„ç»‡

æœ¬æ–‡å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ï¼šç¬¬2èŠ‚ä»‹ç»ç›¸å…³èƒŒæ™¯å’Œå·¥ä½œï¼›ç¬¬3èŠ‚æå‡ºVLAæ¶æ„åˆ†ç±»ä½“ç³»ï¼›ç¬¬4èŠ‚æ·±å…¥åˆ†æèµ„æºç“¶é¢ˆé—®é¢˜ï¼›ç¬¬5èŠ‚æ€»ç»“è§£å†³ç­–ç•¥å’ŒæŠ€æœ¯åˆ›æ–°ï¼›ç¬¬6èŠ‚è¿›è¡Œæ€§èƒ½åˆ†æå’ŒåŸºå‡†æµ‹è¯•ï¼›ç¬¬7èŠ‚è®¨è®ºæœªæ¥æ–¹å‘å’Œå¼€æ”¾æŒ‘æˆ˜ï¼›ç¬¬8èŠ‚æ€»ç»“å…¨æ–‡ã€‚

"""
    
    def _generate_conclusion(self) -> str:
        """ç”Ÿæˆç»“è®ºç« èŠ‚"""
        return """## 8. Conclusion

### 8.1 ä¸»è¦å‘ç°

é€šè¿‡å¯¹163ç¯‡VLAæ¨¡å‹ç ”ç©¶è®ºæ–‡çš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹ä¸»è¦å‘ç°ï¼š

1. **æ¶æ„æ¼”è¿›è¶‹åŠ¿æ˜ç¡®**ï¼šç«¯åˆ°ç«¯VLAæ¶æ„å ä¸»å¯¼åœ°ä½(53.4%)ï¼Œä½†æ··åˆæ¶æ„å’Œåˆ†å±‚å¼VLAæ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå„å 22.7%çš„ä»½é¢ã€‚

2. **èµ„æºç“¶é¢ˆé—®é¢˜ä¸¥é‡**ï¼šè¶…è¿‡74%çš„ç ”ç©¶é¢ä¸´ç®—åŠ›ç“¶é¢ˆï¼Œ50.9%é¢ä¸´æ•°æ®ç“¶é¢ˆï¼Œ40.5%é¢ä¸´åŒé‡ç“¶é¢ˆï¼Œèµ„æºçº¦æŸå·²æˆä¸ºVLAæ¨¡å‹å‘å±•çš„ä¸»è¦éšœç¢ã€‚

3. **è§£å†³æ–¹æ¡ˆé€æ¸æˆç†Ÿ**ï¼šæ•°æ®å¢å¼ºã€æ¨¡å‹å‹ç¼©ã€çŸ¥è¯†è’¸é¦ã€åˆ†å±‚è®¾è®¡ç­‰æŠ€æœ¯å·²å½¢æˆç›¸å¯¹å®Œæ•´çš„è§£å†³æ–¹æ¡ˆä½“ç³»ã€‚

4. **æ€§èƒ½æ”¹å–„æ˜¾è‘—**ï¼šå¹³å‡æˆåŠŸç‡è¾¾åˆ°62.9%ï¼Œä½†æ ‡å‡†å·®è¾ƒå¤§(28.3%)ï¼Œè¯´æ˜ä¸åŒæ–¹æ³•çš„æ€§èƒ½å·®å¼‚æ˜æ˜¾ã€‚

### 8.2 ç†è®ºè´¡çŒ®

æœ¬ç»¼è¿°åœ¨ç†è®ºå±‚é¢çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š
- å»ºç«‹äº†åŸºäºèµ„æºçº¦æŸçš„VLAæ¨¡å‹åˆ†ç±»æ¡†æ¶
- æå‡ºäº†èµ„æº-æ€§èƒ½æƒè¡¡çš„å®šé‡åˆ†ææ–¹æ³•
- æ€»ç»“äº†VLAæ¨¡å‹ä¼˜åŒ–çš„è®¾è®¡åŸåˆ™å’Œæœ€ä½³å®è·µ

### 8.3 å®è·µæŒ‡å¯¼

å¯¹äºVLAæ¨¡å‹çš„å®é™…éƒ¨ç½²ï¼Œæˆ‘ä»¬æä¾›ä»¥ä¸‹æŒ‡å¯¼å»ºè®®ï¼š
1. æ ¹æ®èµ„æºçº¦æŸé€‰æ‹©åˆé€‚çš„æ¶æ„ç±»å‹
2. ä¼˜å…ˆè€ƒè™‘æ•°æ®æ•ˆç‡ä¼˜åŒ–æŠ€æœ¯
3. é‡‡ç”¨æ¸è¿›å¼éƒ¨ç½²ç­–ç•¥
4. å»ºç«‹æ€§èƒ½-èµ„æºç›‘æ§ä½“ç³»

### 8.4 ç ”ç©¶å½±å“

æœ¬ç»¼è¿°æœ‰æœ›æ¨åŠ¨VLAæ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„è¿›ä¸€æ­¥å‘å±•ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ï¼Œä¿ƒè¿›å…·èº«æ™ºèƒ½æŠ€æœ¯çš„äº§ä¸šåŒ–åº”ç”¨ã€‚

### 8.5 æœ€ç»ˆè¯„è¿°

VLAæ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„ç ”ç©¶ä»å¤„äºå¿«é€Ÿå‘å±•é˜¶æ®µã€‚éšç€ç¡¬ä»¶æŠ€æœ¯è¿›æ­¥å’Œç®—æ³•åˆ›æ–°ï¼Œæˆ‘ä»¬æœ‰ç†ç”±ç›¸ä¿¡èµ„æºçº¦æŸé—®é¢˜å°†é€æ­¥å¾—åˆ°ç¼“è§£ï¼ŒVLAæ¨¡å‹å°†åœ¨æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚

---

*æœ¬ç»¼è¿°åŸºäº163ç¯‡ç›¸å…³è®ºæ–‡çš„ç³»ç»Ÿåˆ†æï¼Œæ•°æ®æˆªæ­¢åˆ°2025å¹´1æœˆã€‚*
"""

    def run_complete_analysis(self, output_dir: str = "intelligent_survey_results"):
        """è¿è¡Œå®Œæ•´çš„æ™ºèƒ½ç»¼è¿°ç”Ÿæˆ"""
        print("ğŸš€ å¼€å§‹æ™ºèƒ½ç»¼è¿°ç”Ÿæˆ...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆå®Œæ•´ç»¼è¿°
        survey_path = os.path.join(output_dir, "complete_vla_survey.md")
        complete_survey = self.generate_complete_survey(survey_path)
        
        # ç”Ÿæˆæ¡†æ¶æ–‡ä»¶
        framework = self.generate_survey_framework()
        framework_path = os.path.join(output_dir, "survey_framework.json")
        with open(framework_path, 'w', encoding='utf-8') as f:
            json.dump(framework, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆæŠ€æœ¯åˆ†æ
        tech_analysis = self.generate_technical_analysis()
        tech_path = os.path.join(output_dir, "technical_analysis.md")
        with open(tech_path, 'w', encoding='utf-8') as f:
            f.write(tech_analysis)
        
        # ç”Ÿæˆæœªæ¥æ–¹å‘
        future_directions = self.generate_future_directions()
        future_path = os.path.join(output_dir, "future_directions.md")
        with open(future_path, 'w', encoding='utf-8') as f:
            f.write(future_directions)
        
        print(f"\nâœ… æ™ºèƒ½ç»¼è¿°ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“ å®Œæ•´ç»¼è¿°: {survey_path}")
        print(f"ğŸ—ï¸ ç»¼è¿°æ¡†æ¶: {framework_path}")
        print(f"ğŸ”¬ æŠ€æœ¯åˆ†æ: {tech_path}")
        print(f"ğŸ”® æœªæ¥æ–¹å‘: {future_path}")

    def _safe_get_value(self, data: Dict, key: str, default: str = "æœªæåŠ") -> str:
        """å®‰å…¨è·å–å­—å…¸å€¼ï¼Œå¤„ç†NaNã€Noneå’Œå„ç§å¼‚å¸¸æƒ…å†µ"""
        try:
            value = data.get(key, default)
            # å¤„ç†pandasçš„NaNå€¼
            if pd.isna(value) or value is None:
                return default
            # å¤„ç†ç©ºå­—ç¬¦ä¸²
            if str(value).strip() == "":
                return default
            # å¤„ç†numpy.nanç­‰ç‰¹æ®Šå€¼
            if str(value).lower() in ['nan', 'none', 'null']:
                return default
            return str(value)
        except Exception:
            return default


def main():
    parser = argparse.ArgumentParser(description='VLAæ™ºèƒ½ç»¼è¿°ç”Ÿæˆå™¨')
    parser.add_argument('--csv_path', type=str, required=True, 
                       help='VLAè®ºæ–‡CSVæ•°æ®è·¯å¾„')
    parser.add_argument('--analysis_dir', type=str, required=True,
                       help='åˆ†æç»“æœç›®å½•è·¯å¾„')
    parser.add_argument('--config_path', type=str, default='apikey.ini',
                       help='APIé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='intelligent_survey_results',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ™ºèƒ½ç»¼è¿°ç”Ÿæˆå™¨
    generator = VLAIntelligentSurveyGenerator(
        csv_path=args.csv_path,
        analysis_results_dir=args.analysis_dir,
        config_path=args.config_path
    )
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    generator.run_complete_analysis(args.output_dir)


if __name__ == "__main__":
    main()
