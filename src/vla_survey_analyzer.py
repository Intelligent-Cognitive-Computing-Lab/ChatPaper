#!/usr/bin/env python3
"""
VLAç»¼è¿°åˆ†æå·¥å…· - Survey Analysis Tool
é’ˆå¯¹VLAè®ºæ–‡æ•°æ®è¿›è¡Œç³»ç»Ÿæ€§åˆ†ç±»åˆ†æå’Œç»¼è¿°æ€»ç»“

åŠŸèƒ½:
1. æ•°æ®ç»Ÿè®¡åˆ†æ - è®ºæ–‡åˆ†å¸ƒã€è¶‹åŠ¿åˆ†æ
2. æ¶æ„ç±»å‹åˆ†æ - VLAæ¶æ„åˆ†ç±»ç»Ÿè®¡
3. èµ„æºç“¶é¢ˆåˆ†æ - æ•°æ®ç“¶é¢ˆå’Œç®—åŠ›ç“¶é¢ˆåˆ†å¸ƒ
4. æŠ€æœ¯åˆ›æ–°æ€»ç»“ - åˆ›æ–°ç‚¹å’Œè§£å†³æ–¹æ¡ˆå½’çº³
5. æ€§èƒ½å¯¹æ¯”åˆ†æ - ä¸åŒæ–¹æ³•çš„æ€§èƒ½æƒè¡¡
6. ç»¼è¿°æŠ¥å‘Šç”Ÿæˆ - è‡ªåŠ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š
"""

import pandas as pd
import numpy as np
from collections import Counter, defaultdict
import re
from datetime import datetime
import json
import argparse
import os

class VLASurveyAnalyzer:
    def __init__(self, csv_path):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.csv_path = csv_path
        self.df = pd.DataFrame()  # åˆå§‹åŒ–ä¸ºç©ºDataFrame
        self.load_data()
        
    def load_data(self):
        """åŠ è½½CSVæ•°æ®"""
        try:
            self.df = pd.read_csv(self.csv_path, encoding='utf-8')
            print(f"âœ… æˆåŠŸåŠ è½½ {len(self.df)} ç¯‡è®ºæ–‡æ•°æ®")
            print(f"ğŸ“Š æ•°æ®å­—æ®µ: {list(self.df.columns)}")
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
        return True
    
    def basic_statistics(self):
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ“Š VLAè®ºæ–‡åŸºç¡€ç»Ÿè®¡åˆ†æ")
        print("="*60)
        
        # æ€»ä½“ç»Ÿè®¡
        total_papers = len(self.df)
        print(f"ğŸ“š è®ºæ–‡æ€»æ•°: {total_papers}")
        
        # å¹´ä»½åˆ†å¸ƒ
        years = self.df['å‘è¡¨å¹´ä»½'].value_counts().sort_index()
        print(f"\nğŸ“… å¹´ä»½åˆ†å¸ƒ:")
        for year, count in years.items():
            print(f"  {year}: {count}ç¯‡")
        
        # æœŸåˆŠä¼šè®®åˆ†å¸ƒ(æ’é™¤ç©ºå€¼)
        venues = self.df['æœŸåˆŠä¼šè®®'].dropna()
        venue_counts = venues.value_counts().head(10)
        print(f"\nğŸ›ï¸ ä¸»è¦å‘è¡¨å¹³å° (Top 10):")
        for venue, count in venue_counts.items():
            if venue and venue != "æœªæåŠ":
                print(f"  {venue}: {count}ç¯‡")
        
        # VLAæ¶æ„ç±»å‹åˆ†å¸ƒ
        arch_types = self.df['VLAæ¶æ„ç±»å‹'].value_counts()
        print(f"\nğŸ—ï¸ VLAæ¶æ„ç±»å‹åˆ†å¸ƒ:")
        for arch, count in arch_types.items():
            percentage = (count/total_papers)*100
            print(f"  {arch}: {count}ç¯‡ ({percentage:.1f}%)")
        
        return {
            'total_papers': total_papers,
            'year_distribution': years.to_dict(),
            'venue_distribution': venue_counts.to_dict(),
            'architecture_distribution': arch_types.to_dict()
        }
    
    def resource_bottleneck_analysis(self):
        """èµ„æºç“¶é¢ˆåˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ” èµ„æºç“¶é¢ˆæ·±åº¦åˆ†æ")
        print("="*60)
        
        # æ•°æ®ç“¶é¢ˆvsç®—åŠ›ç“¶é¢ˆåˆ†å¸ƒ
        data_bottleneck = self.df['æ•°æ®ç“¶é¢ˆ'].value_counts()
        compute_bottleneck = self.df['ç®—åŠ›ç“¶é¢ˆ'].value_counts()
        
        print("ğŸ“Š èµ„æºç“¶é¢ˆåˆ†å¸ƒ:")
        print(f"  æ•°æ®ç“¶é¢ˆ: {data_bottleneck.to_dict()}")
        print(f"  ç®—åŠ›ç“¶é¢ˆ: {compute_bottleneck.to_dict()}")
        
        # åŒé‡ç“¶é¢ˆåˆ†æ
        both_bottlenecks = self.df[
            (self.df['æ•°æ®ç“¶é¢ˆ'] == 'æ˜¯') & (self.df['ç®—åŠ›ç“¶é¢ˆ'] == 'æ˜¯')
        ]
        data_only = self.df[
            (self.df['æ•°æ®ç“¶é¢ˆ'] == 'æ˜¯') & (self.df['ç®—åŠ›ç“¶é¢ˆ'] == 'å¦')
        ]
        compute_only = self.df[
            (self.df['æ•°æ®ç“¶é¢ˆ'] == 'å¦') & (self.df['ç®—åŠ›ç“¶é¢ˆ'] == 'æ˜¯')
        ]
        no_bottleneck = self.df[
            (self.df['æ•°æ®ç“¶é¢ˆ'] == 'å¦') & (self.df['ç®—åŠ›ç“¶é¢ˆ'] == 'å¦')
        ]
        
        print(f"\nğŸ”— ç“¶é¢ˆç»„åˆåˆ†æ:")
        print(f"  åŒé‡ç“¶é¢ˆ (æ•°æ®+ç®—åŠ›): {len(both_bottlenecks)}ç¯‡ ({len(both_bottlenecks)/len(self.df)*100:.1f}%)")
        print(f"  ä»…æ•°æ®ç“¶é¢ˆ: {len(data_only)}ç¯‡ ({len(data_only)/len(self.df)*100:.1f}%)")
        print(f"  ä»…ç®—åŠ›ç“¶é¢ˆ: {len(compute_only)}ç¯‡ ({len(compute_only)/len(self.df)*100:.1f}%)")
        print(f"  æ— æ˜æ˜¾ç“¶é¢ˆ: {len(no_bottleneck)}ç¯‡ ({len(no_bottleneck)/len(self.df)*100:.1f}%)")
        
        # èµ„æºçº¦æŸç±»å‹åˆ†æ
        constraint_types = self.df['èµ„æºçº¦æŸç±»å‹'].value_counts()
        print(f"\nâš¡ èµ„æºçº¦æŸç±»å‹åˆ†å¸ƒ:")
        for constraint, count in constraint_types.items():
            if constraint and constraint != "æœªæåŠ":
                print(f"  {constraint}: {count}ç¯‡")
        
        # æ¶æ„ç±»å‹ä¸èµ„æºç“¶é¢ˆçš„å…³è”åˆ†æ
        print(f"\nğŸ—ï¸ æ¶æ„ç±»å‹ä¸èµ„æºç“¶é¢ˆå…³è”:")
        arch_bottleneck = pd.crosstab(
            self.df['VLAæ¶æ„ç±»å‹'], 
            [self.df['æ•°æ®ç“¶é¢ˆ'], self.df['ç®—åŠ›ç“¶é¢ˆ']], 
            margins=True
        )
        print(arch_bottleneck)
        
        return {
            'data_bottleneck_dist': data_bottleneck.to_dict(),
            'compute_bottleneck_dist': compute_bottleneck.to_dict(),
            'bottleneck_combinations': {
                'both': len(both_bottlenecks),
                'data_only': len(data_only),
                'compute_only': len(compute_only),
                'none': len(no_bottleneck)
            },
            'constraint_types': constraint_types.to_dict()
        }
    
    def solution_strategy_analysis(self):
        """è§£å†³ç­–ç•¥åˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ’¡ è§£å†³ç­–ç•¥ä¸åˆ›æ–°ç‚¹åˆ†æ")
        print("="*60)
        
        # æ•°æ®ç“¶é¢ˆè§£å†³ç­–ç•¥
        data_strategies = self.df['æ•°æ®ç“¶é¢ˆè§£å†³ç­–ç•¥'].dropna()
        data_strategy_keywords = []
        for strategy in data_strategies:
            if strategy and strategy != "æœªæåŠ":
                # æå–å…³é”®è¯
                keywords = re.findall(r'[\u4e00-\u9fff]+', str(strategy))
                data_strategy_keywords.extend(keywords)
        
        data_strategy_counter = Counter(data_strategy_keywords)
        common_data_strategies = data_strategy_counter.most_common(15)
        
        print("ğŸ“ˆ æ•°æ®ç“¶é¢ˆè§£å†³ç­–ç•¥å…³é”®è¯ (Top 15):")
        for keyword, count in common_data_strategies:
            if len(keyword) > 1:  # è¿‡æ»¤å•å­—
                print(f"  {keyword}: {count}æ¬¡")
        
        # ç®—åŠ›ç“¶é¢ˆè§£å†³ç­–ç•¥
        compute_strategies = self.df['ç®—åŠ›ç“¶é¢ˆè§£å†³ç­–ç•¥'].dropna()
        compute_strategy_keywords = []
        for strategy in compute_strategies:
            if strategy and strategy != "æœªæåŠ":
                keywords = re.findall(r'[\u4e00-\u9fff]+', str(strategy))
                compute_strategy_keywords.extend(keywords)
        
        compute_strategy_counter = Counter(compute_strategy_keywords)
        common_compute_strategies = compute_strategy_counter.most_common(15)
        
        print(f"\nâš¡ ç®—åŠ›ç“¶é¢ˆè§£å†³ç­–ç•¥å…³é”®è¯ (Top 15):")
        for keyword, count in common_compute_strategies:
            if len(keyword) > 1:
                print(f"  {keyword}: {count}æ¬¡")
        
        # åˆ›æ–°ç‚¹åˆ†æ
        innovations = self.df['åˆ›æ–°ç‚¹'].dropna()
        innovation_keywords = []
        for innovation in innovations:
            if innovation and innovation != "æœªæåŠ":
                keywords = re.findall(r'[\u4e00-\u9fff]+', str(innovation))
                innovation_keywords.extend(keywords)
        
        innovation_counter = Counter(innovation_keywords)
        common_innovations = innovation_counter.most_common(20)
        
        print(f"\nğŸš€ æŠ€æœ¯åˆ›æ–°å…³é”®è¯ (Top 20):")
        for keyword, count in common_innovations:
            if len(keyword) > 1:
                print(f"  {keyword}: {count}æ¬¡")
        
        return {
            'data_strategies': common_data_strategies,
            'compute_strategies': common_compute_strategies,
            'innovations': common_innovations
        }
    
    def performance_analysis(self):
        """æ€§èƒ½åˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ä¸æƒè¡¡åˆ†æ")
        print("="*60)
        
        # æå–æˆåŠŸç‡æ•°æ®
        success_rates = []
        performance_data = self.df['æ€§èƒ½æŒ‡æ ‡'].dropna()
        
        for perf in performance_data:
            if perf and perf != "æœªæåŠ":
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æˆåŠŸç‡
                rates = re.findall(r'(\d+(?:\.\d+)?)%', str(perf))
                for rate in rates:
                    try:
                        success_rates.append(float(rate))
                    except:
                        continue
        
        if success_rates:
            print(f"ğŸ¯ æˆåŠŸç‡ç»Ÿè®¡ (åŸºäº{len(success_rates)}ä¸ªæ•°æ®ç‚¹):")
            print(f"  å¹³å‡æˆåŠŸç‡: {np.mean(success_rates):.1f}%")
            print(f"  ä¸­ä½æ•°: {np.median(success_rates):.1f}%")
            print(f"  æœ€é«˜æˆåŠŸç‡: {np.max(success_rates):.1f}%")
            print(f"  æœ€ä½æˆåŠŸç‡: {np.min(success_rates):.1f}%")
            print(f"  æ ‡å‡†å·®: {np.std(success_rates):.1f}%")
        
        # æ¨¡å‹è§„æ¨¡åˆ†æ
        model_sizes = []
        size_data = self.df['æ¨¡å‹è§„æ¨¡'].dropna()
        
        for size in size_data:
            if size and size != "æœªæåŠ":
                # æå–å‚æ•°é‡
                params = re.findall(r'(\d+(?:\.\d+)?)([BMK])', str(size))
                for param, unit in params:
                    try:
                        value = float(param)
                        if unit == 'B':
                            model_sizes.append(value)
                        elif unit == 'M':
                            model_sizes.append(value / 1000)
                        elif unit == 'K':
                            model_sizes.append(value / 1000000)
                    except:
                        continue
        
        if model_sizes:
            print(f"\nğŸ§  æ¨¡å‹è§„æ¨¡ç»Ÿè®¡ (åŸºäº{len(model_sizes)}ä¸ªæ•°æ®ç‚¹, å•ä½:Bå‚æ•°):")
            print(f"  å¹³å‡è§„æ¨¡: {np.mean(model_sizes):.2f}B")
            print(f"  ä¸­ä½æ•°: {np.median(model_sizes):.2f}B")
            print(f"  æœ€å¤§è§„æ¨¡: {np.max(model_sizes):.1f}B")
            print(f"  æœ€å°è§„æ¨¡: {np.min(model_sizes):.3f}B")
        
        # èµ„æº-æ€§èƒ½æƒè¡¡åˆ†æ
        tradeoffs = self.df['èµ„æº-æ€§èƒ½æƒè¡¡'].dropna()
        tradeoff_keywords = []
        for tradeoff in tradeoffs:
            if tradeoff and tradeoff != "æœªæåŠ":
                keywords = re.findall(r'[\u4e00-\u9fff]+', str(tradeoff))
                tradeoff_keywords.extend(keywords)
        
        tradeoff_counter = Counter(tradeoff_keywords)
        common_tradeoffs = tradeoff_counter.most_common(10)
        
        print(f"\nâš–ï¸ èµ„æº-æ€§èƒ½æƒè¡¡å…³é”®è¯ (Top 10):")
        for keyword, count in common_tradeoffs:
            if len(keyword) > 1:
                print(f"  {keyword}: {count}æ¬¡")
        
        return {
            'success_rates': {
                'mean': np.mean(success_rates) if success_rates else 0,
                'median': np.median(success_rates) if success_rates else 0,
                'max': np.max(success_rates) if success_rates else 0,
                'min': np.min(success_rates) if success_rates else 0,
                'std': np.std(success_rates) if success_rates else 0
            },
            'model_sizes': {
                'mean': np.mean(model_sizes) if model_sizes else 0,
                'median': np.median(model_sizes) if model_sizes else 0,
                'max': np.max(model_sizes) if model_sizes else 0,
                'min': np.min(model_sizes) if model_sizes else 0
            },
            'tradeoffs': common_tradeoffs
        }
    
    def task_application_analysis(self):
        """ä»»åŠ¡åº”ç”¨åˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ¯ ä»»åŠ¡ç±»å‹ä¸åº”ç”¨åœºæ™¯åˆ†æ")
        print("="*60)
        
        # ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        task_types = self.df['ä»»åŠ¡ç±»å‹'].dropna()
        task_keywords = []
        for task in task_types:
            if task and task != "æœªæåŠ":
                keywords = re.findall(r'[\u4e00-\u9fff]+', str(task))
                task_keywords.extend(keywords)
        
        task_counter = Counter(task_keywords)
        common_tasks = task_counter.most_common(15)
        
        print("ğŸ¯ ä»»åŠ¡ç±»å‹å…³é”®è¯ (Top 15):")
        for keyword, count in common_tasks:
            if len(keyword) > 1:
                print(f"  {keyword}: {count}æ¬¡")
        
        # å®éªŒç¯å¢ƒåˆ†æ
        environments = self.df['å®éªŒç¯å¢ƒ'].dropna()
        env_keywords = []
        for env in environments:
            if env and env != "æœªæåŠ":
                keywords = re.findall(r'[\u4e00-\u9fff]+', str(env))
                env_keywords.extend(keywords)
        
        env_counter = Counter(env_keywords)
        common_envs = env_counter.most_common(10)
        
        print(f"\nğŸŒ å®éªŒç¯å¢ƒå…³é”®è¯ (Top 10):")
        for keyword, count in common_envs:
            if len(keyword) > 1:
                print(f"  {keyword}: {count}æ¬¡")
        
        # æ•°æ®ç±»å‹åˆ†æ
        data_types = self.df['æ•°æ®ç±»å‹'].dropna()
        datatype_keywords = []
        for dtype in data_types:
            if dtype and dtype != "æœªæåŠ":
                keywords = re.findall(r'[\u4e00-\u9fff]+', str(dtype))
                datatype_keywords.extend(keywords)
        
        datatype_counter = Counter(datatype_keywords)
        common_datatypes = datatype_counter.most_common(12)
        
        print(f"\nğŸ“Š æ•°æ®ç±»å‹å…³é”®è¯ (Top 12):")
        for keyword, count in common_datatypes:
            if len(keyword) > 1:
                print(f"  {keyword}: {count}æ¬¡")
        
        return {
            'tasks': common_tasks,
            'environments': common_envs,
            'data_types': common_datatypes
        }
    
    def generate_survey_report(self, output_path="vla_survey_report.md"):
        """ç”Ÿæˆç»¼è¿°æŠ¥å‘Š"""
        print(f"\nğŸ“ æ­£åœ¨ç”Ÿæˆç»¼è¿°æŠ¥å‘Š...")
        
        # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
        basic_stats = self.basic_statistics()
        resource_analysis = self.resource_bottleneck_analysis()
        solution_analysis = self.solution_strategy_analysis()
        performance_analysis_result = self.performance_analysis()
        task_analysis = self.task_application_analysis()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""# Vision-Language-Action (VLA) æ¨¡å‹èµ„æºå—é™ç ”ç©¶ç»¼è¿°æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}  
**æ•°æ®æº**: {self.csv_path}  
**åˆ†æè®ºæ–‡æ•°é‡**: {basic_stats['total_papers']}ç¯‡

## ğŸ“Š 1. ç ”ç©¶ç°çŠ¶æ¦‚è§ˆ

### 1.1 æ—¶é—´è¶‹åŠ¿
- **ä¸»è¦å‘è¡¨å¹´ä»½**: {max(basic_stats['year_distribution'], key=basic_stats['year_distribution'].get)}å¹´ ({basic_stats['year_distribution'][max(basic_stats['year_distribution'], key=basic_stats['year_distribution'].get)]}ç¯‡)
- **å¹´åº¦åˆ†å¸ƒ**: {dict(sorted(basic_stats['year_distribution'].items()))}

### 1.2 æ¶æ„åˆ†å¸ƒ
"""
        
        # æ¶æ„åˆ†å¸ƒ
        for arch, count in basic_stats['architecture_distribution'].items():
            percentage = (count/basic_stats['total_papers'])*100
            report += f"- **{arch}**: {count}ç¯‡ ({percentage:.1f}%)\n"
        
        report += f"""
## ğŸ” 2. èµ„æºç“¶é¢ˆæ·±åº¦åˆ†æ

### 2.1 ç“¶é¢ˆåˆ†å¸ƒæ¦‚å†µ
- **å­˜åœ¨æ•°æ®ç“¶é¢ˆ**: {resource_analysis['data_bottleneck_dist'].get('æ˜¯', 0)}ç¯‡
- **å­˜åœ¨ç®—åŠ›ç“¶é¢ˆ**: {resource_analysis['compute_bottleneck_dist'].get('æ˜¯', 0)}ç¯‡

### 2.2 ç“¶é¢ˆç»„åˆåˆ†æ
- **åŒé‡ç“¶é¢ˆ(æ•°æ®+ç®—åŠ›)**: {resource_analysis['bottleneck_combinations']['both']}ç¯‡ ({resource_analysis['bottleneck_combinations']['both']/basic_stats['total_papers']*100:.1f}%)
- **ä»…æ•°æ®ç“¶é¢ˆ**: {resource_analysis['bottleneck_combinations']['data_only']}ç¯‡ ({resource_analysis['bottleneck_combinations']['data_only']/basic_stats['total_papers']*100:.1f}%)
- **ä»…ç®—åŠ›ç“¶é¢ˆ**: {resource_analysis['bottleneck_combinations']['compute_only']}ç¯‡ ({resource_analysis['bottleneck_combinations']['compute_only']/basic_stats['total_papers']*100:.1f}%)
- **æ— æ˜æ˜¾ç“¶é¢ˆ**: {resource_analysis['bottleneck_combinations']['none']}ç¯‡ ({resource_analysis['bottleneck_combinations']['none']/basic_stats['total_papers']*100:.1f}%)

### 2.3 ä¸»è¦çº¦æŸç±»å‹
"""
        
        # çº¦æŸç±»å‹
        for constraint, count in list(resource_analysis['constraint_types'].items())[:10]:
            if constraint and constraint != "æœªæåŠ":
                report += f"- **{constraint}**: {count}ç¯‡\n"
        
        report += f"""
## ğŸ’¡ 3. è§£å†³ç­–ç•¥ä¸æŠ€æœ¯åˆ›æ–°

### 3.1 æ•°æ®ç“¶é¢ˆè§£å†³ç­–ç•¥ (Top 10)
"""
        
        # æ•°æ®ç­–ç•¥
        for keyword, count in solution_analysis['data_strategies'][:10]:
            if len(keyword) > 1:
                report += f"- **{keyword}**: {count}æ¬¡æåŠ\n"
        
        report += f"""
### 3.2 ç®—åŠ›ç“¶é¢ˆè§£å†³ç­–ç•¥ (Top 10)
"""
        
        # ç®—åŠ›ç­–ç•¥
        for keyword, count in solution_analysis['compute_strategies'][:10]:
            if len(keyword) > 1:
                report += f"- **{keyword}**: {count}æ¬¡æåŠ\n"
        
        report += f"""
### 3.3 ä¸»è¦æŠ€æœ¯åˆ›æ–° (Top 15)
"""
        
        # åˆ›æ–°ç‚¹
        for keyword, count in solution_analysis['innovations'][:15]:
            if len(keyword) > 1:
                report += f"- **{keyword}**: {count}æ¬¡æåŠ\n"
        
        report += f"""
## ğŸ“ˆ 4. æ€§èƒ½åˆ†æ

### 4.1 æˆåŠŸç‡ç»Ÿè®¡
- **å¹³å‡æˆåŠŸç‡**: {performance_analysis_result['success_rates']['mean']:.1f}%
- **ä¸­ä½æ•°æˆåŠŸç‡**: {performance_analysis_result['success_rates']['median']:.1f}%
- **æœ€é«˜æˆåŠŸç‡**: {performance_analysis_result['success_rates']['max']:.1f}%
- **æˆåŠŸç‡æ ‡å‡†å·®**: {performance_analysis_result['success_rates']['std']:.1f}%

### 4.2 æ¨¡å‹è§„æ¨¡åˆ†æ
- **å¹³å‡æ¨¡å‹è§„æ¨¡**: {performance_analysis_result['model_sizes']['mean']:.2f}Bå‚æ•°
- **ä¸­ä½æ•°æ¨¡å‹è§„æ¨¡**: {performance_analysis_result['model_sizes']['median']:.2f}Bå‚æ•°
- **æœ€å¤§æ¨¡å‹è§„æ¨¡**: {performance_analysis_result['model_sizes']['max']:.1f}Bå‚æ•°

### 4.3 èµ„æº-æ€§èƒ½æƒè¡¡ç­–ç•¥ (Top 8)
"""
        
        # æƒè¡¡ç­–ç•¥
        for keyword, count in performance_analysis_result['tradeoffs'][:8]:
            if len(keyword) > 1:
                report += f"- **{keyword}**: {count}æ¬¡æåŠ\n"
        
        report += f"""
## ğŸ¯ 5. åº”ç”¨åœºæ™¯åˆ†æ

### 5.1 ä¸»è¦ä»»åŠ¡ç±»å‹ (Top 12)
"""
        
        # ä»»åŠ¡ç±»å‹
        for keyword, count in task_analysis['tasks'][:12]:
            if len(keyword) > 1:
                report += f"- **{keyword}**: {count}æ¬¡\n"
        
        report += f"""
### 5.2 å®éªŒç¯å¢ƒåˆ†å¸ƒ (Top 8)
"""
        
        # å®éªŒç¯å¢ƒ
        for keyword, count in task_analysis['environments'][:8]:
            if len(keyword) > 1:
                report += f"- **{keyword}**: {count}æ¬¡\n"
        
        report += f"""
### 5.3 æ•°æ®æ¨¡æ€ç±»å‹ (Top 10)
"""
        
        # æ•°æ®ç±»å‹
        for keyword, count in task_analysis['data_types'][:10]:
            if len(keyword) > 1:
                report += f"- **{keyword}**: {count}æ¬¡\n"
        
        report += f"""
## ğŸ”® 6. ç ”ç©¶è¶‹åŠ¿ä¸å‘å±•æ–¹å‘

### 6.1 ä¸»è¦å‘ç°
1. **æ¶æ„æ¼”è¿›**: {max(basic_stats['architecture_distribution'], key=basic_stats['architecture_distribution'].get)}æ˜¯å½“å‰ä¸»æµæ¶æ„({basic_stats['architecture_distribution'][max(basic_stats['architecture_distribution'], key=basic_stats['architecture_distribution'].get)]}ç¯‡)
2. **ç“¶é¢ˆç°çŠ¶**: {resource_analysis['bottleneck_combinations']['both']}ç¯‡è®ºæ–‡é¢ä¸´æ•°æ®å’Œç®—åŠ›åŒé‡ç“¶é¢ˆ
3. **è§£å†³è¶‹åŠ¿**: å¤šæ¨¡æ€èåˆã€æ¨¡å‹å‹ç¼©ã€æ•°æ®å¢å¼ºæˆä¸ºä¸»è¦è§£å†³æ–¹å‘
4. **æ€§èƒ½æ°´å¹³**: å¹³å‡æˆåŠŸç‡è¾¾åˆ°{performance_analysis_result['success_rates']['mean']:.1f}%ï¼Œä½†æ–¹å·®è¾ƒå¤§({performance_analysis_result['success_rates']['std']:.1f}%)

### 6.2 æŠ€æœ¯æŒ‘æˆ˜
1. **æ•°æ®ç¨€ç¼º**: é«˜è´¨é‡æœºå™¨äººæ¼”ç¤ºæ•°æ®è·å–å›°éš¾
2. **è®¡ç®—å¯†é›†**: å¤§æ¨¡å‹è®­ç»ƒå’Œæ¨ç†éœ€è¦å¤§é‡è®¡ç®—èµ„æº  
3. **æ³›åŒ–èƒ½åŠ›**: ä»ä»¿çœŸåˆ°ç°å®çš„è¿ç§»ä»å­˜åœ¨gap
4. **å®æ—¶æ€§**: å¤æ‚ä»»åŠ¡çš„æ¨ç†å»¶è¿Ÿé—®é¢˜

### 6.3 æœªæ¥æ–¹å‘
1. **æ•°æ®æ•ˆç‡**: å°‘æ ·æœ¬å­¦ä¹ ã€æ•°æ®åˆæˆã€è¿ç§»å­¦ä¹ 
2. **è®¡ç®—æ•ˆç‡**: æ¨¡å‹å‹ç¼©ã€çŸ¥è¯†è’¸é¦ã€è½»é‡åŒ–æ¶æ„
3. **å¤šæ¨¡æ€èåˆ**: è§†è§‰-è¯­è¨€-è§¦è§‰-æœ¬ä½“æ„ŸçŸ¥æ•´åˆ
4. **é•¿æ—¶ç¨‹æ¨ç†**: åˆ†å±‚è§„åˆ’ã€è®°å¿†æœºåˆ¶ã€æŒç»­å­¦ä¹ 

## ğŸ“‹ 7. æ•°æ®è¯´æ˜

- **æ•°æ®æº**: {os.path.basename(self.csv_path)}
- **ç»Ÿè®¡ç»´åº¦**: 25ä¸ªä¸“ä¸šå­—æ®µ
- **åˆ†ææ–¹æ³•**: å…³é”®è¯æå–ã€ç»Ÿè®¡åˆ†æã€è¶‹åŠ¿å½’çº³
- **æ›´æ–°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d')}

---
*æœ¬æŠ¥å‘Šç”±VLAç»¼è¿°åˆ†æå·¥å…·è‡ªåŠ¨ç”Ÿæˆï¼ŒåŸºäº{basic_stats['total_papers']}ç¯‡ç›¸å…³è®ºæ–‡çš„ç³»ç»Ÿåˆ†æ*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… ç»¼è¿°æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
        return report
    
    def generate_classification_summary(self, output_path="vla_classification_summary.json"):
        """ç”Ÿæˆåˆ†ç±»æ€»ç»“JSON"""
        print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆåˆ†ç±»æ€»ç»“...")
        
        # æŒ‰æ¶æ„åˆ†ç±»ç»Ÿè®¡
        arch_classification = {}
        for arch in self.df['VLAæ¶æ„ç±»å‹'].unique():
            if pd.isna(arch):
                continue
            arch_papers = self.df[self.df['VLAæ¶æ„ç±»å‹'] == arch]
            
            arch_classification[arch] = {
                'paper_count': len(arch_papers),
                'data_bottleneck_ratio': len(arch_papers[arch_papers['æ•°æ®ç“¶é¢ˆ'] == 'æ˜¯']) / len(arch_papers),
                'compute_bottleneck_ratio': len(arch_papers[arch_papers['ç®—åŠ›ç“¶é¢ˆ'] == 'æ˜¯']) / len(arch_papers),
                'avg_success_rate': self._extract_avg_metric(arch_papers['æ€§èƒ½æŒ‡æ ‡'], r'(\d+(?:\.\d+)?)%'),
                'common_tasks': self._get_top_keywords(arch_papers['ä»»åŠ¡ç±»å‹'], 5),
                'common_innovations': self._get_top_keywords(arch_papers['åˆ›æ–°ç‚¹'], 5),
                'representative_papers': arch_papers['è®ºæ–‡æ ‡é¢˜'].head(3).tolist()
            }
        
        # æŒ‰ç“¶é¢ˆç±»å‹åˆ†ç±»
        bottleneck_classification = {
            'data_bottleneck_only': self._analyze_bottleneck_group(
                self.df[(self.df['æ•°æ®ç“¶é¢ˆ'] == 'æ˜¯') & (self.df['ç®—åŠ›ç“¶é¢ˆ'] == 'å¦')]
            ),
            'compute_bottleneck_only': self._analyze_bottleneck_group(
                self.df[(self.df['æ•°æ®ç“¶é¢ˆ'] == 'å¦') & (self.df['ç®—åŠ›ç“¶é¢ˆ'] == 'æ˜¯')]
            ),
            'both_bottlenecks': self._analyze_bottleneck_group(
                self.df[(self.df['æ•°æ®ç“¶é¢ˆ'] == 'æ˜¯') & (self.df['ç®—åŠ›ç“¶é¢ˆ'] == 'æ˜¯')]
            ),
            'no_bottleneck': self._analyze_bottleneck_group(
                self.df[(self.df['æ•°æ®ç“¶é¢ˆ'] == 'å¦') & (self.df['ç®—åŠ›ç“¶é¢ˆ'] == 'å¦')]
            )
        }
        
        # ç»¼åˆåˆ†ç±»æ€»ç»“
        classification_summary = {
            'metadata': {
                'total_papers': len(self.df),
                'analysis_date': datetime.now().isoformat(),
                'data_source': self.csv_path
            },
            'architecture_classification': arch_classification,
            'bottleneck_classification': bottleneck_classification,
            'overall_trends': {
                'most_common_architecture': self.df['VLAæ¶æ„ç±»å‹'].value_counts().index[0] if len(self.df['VLAæ¶æ„ç±»å‹'].value_counts()) > 0 else "æœªçŸ¥",
                'bottleneck_distribution': {
                    'data_bottleneck_ratio': len(self.df[self.df['æ•°æ®ç“¶é¢ˆ'] == 'æ˜¯']) / len(self.df),
                    'compute_bottleneck_ratio': len(self.df[self.df['ç®—åŠ›ç“¶é¢ˆ'] == 'æ˜¯']) / len(self.df)
                },
                'emerging_keywords': self._get_top_keywords(self.df['åˆ›æ–°ç‚¹'], 10)
            }
        }
        
        # ä¿å­˜JSON
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(classification_summary, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†ç±»æ€»ç»“å·²ç”Ÿæˆ: {output_path}")
        return classification_summary
    
    def _analyze_bottleneck_group(self, group_df):
        """åˆ†æç‰¹å®šç“¶é¢ˆç»„çš„ç‰¹å¾"""
        if len(group_df) == 0:
            return {'paper_count': 0}
            
        return {
            'paper_count': len(group_df),
            'architecture_distribution': group_df['VLAæ¶æ„ç±»å‹'].value_counts().to_dict(),
            'common_solutions': self._get_top_keywords(
                pd.concat([group_df['æ•°æ®ç“¶é¢ˆè§£å†³ç­–ç•¥'], group_df['ç®—åŠ›ç“¶é¢ˆè§£å†³ç­–ç•¥']]), 5
            ),
            'avg_model_size': self._extract_avg_metric(group_df['æ¨¡å‹è§„æ¨¡'], r'(\d+(?:\.\d+)?)[BMK]'),
            'representative_papers': group_df['è®ºæ–‡æ ‡é¢˜'].head(3).tolist()
        }
    
    def _get_top_keywords(self, series, top_n=5):
        """ä»pandas Seriesä¸­æå–çƒ­é—¨å…³é”®è¯"""
        keywords = []
        for item in series.dropna():
            if item and item != "æœªæåŠ":
                extracted = re.findall(r'[\u4e00-\u9fff]+', str(item))
                keywords.extend([kw for kw in extracted if len(kw) > 1])
        
        counter = Counter(keywords)
        return [{'keyword': kw, 'count': count} for kw, count in counter.most_common(top_n)]
    
    def _extract_avg_metric(self, series, pattern):
        """ä»ç³»åˆ—æ•°æ®ä¸­æå–æ•°å€¼æŒ‡æ ‡çš„å¹³å‡å€¼"""
        values = []
        for item in series.dropna():
            if item and item != "æœªæåŠ":
                matches = re.findall(pattern, str(item))
                for match in matches:
                    try:
                        values.append(float(match))
                    except:
                        continue
        return np.mean(values) if values else 0
    
    def run_full_analysis(self, report_path="vla_survey_report.md", 
                         classification_path="vla_classification_summary.json"):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹VLAç»¼è¿°åˆ†æ...")
        
        # è¿è¡Œæ‰€æœ‰åˆ†æ
        self.basic_statistics()
        self.resource_bottleneck_analysis()
        self.solution_strategy_analysis()
        self.performance_analysis()
        self.task_application_analysis()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_survey_report(report_path)
        self.generate_classification_summary(classification_path)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ ç»¼è¿°æŠ¥å‘Š: {report_path}")
        print(f"ğŸ“Š åˆ†ç±»æ€»ç»“: {classification_path}")
    
    def generate_comprehensive_report(self, output_dir="analysis_results"):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š - ç»Ÿä¸€æ¥å£æ–¹æ³•
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸç”ŸæˆæŠ¥å‘Š
        """
        try:
            print(f"\nğŸ“Š å¼€å§‹ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_path = os.path.join(output_dir, f"vla_comprehensive_report_{timestamp}.md")
            classification_path = os.path.join(output_dir, f"vla_classification_summary_{timestamp}.json")
            
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            print(f"ğŸ“ æŠ¥å‘Šæ–‡ä»¶: {os.path.basename(report_path)}")
            print(f"ğŸ“‹ åˆ†ç±»æ–‡ä»¶: {os.path.basename(classification_path)}")
            
            # ç”Ÿæˆä¸»æŠ¥å‘Š
            report_content = self.generate_survey_report(report_path)
            
            # ç”Ÿæˆåˆ†ç±»æ‘˜è¦
            classification_content = self.generate_classification_summary(classification_path)
            
            print(f"\nâœ… ç»¼åˆåˆ†æreportç”Ÿæˆå®Œæˆ!")
            print(f"   - ä¸»æŠ¥å‘Š: {report_path}")
            print(f"   - åˆ†ç±»æ‘˜è¦: {classification_path}")
            print(f"   - åˆ†æè®ºæ–‡æ•°: {len(self.df)}ç¯‡")
            
            return True
            
        except Exception as e:
            print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return False
def main():
    parser = argparse.ArgumentParser(description='VLAç»¼è¿°åˆ†æå·¥å…·')
    parser.add_argument('--csv_path', type=str, required=True, 
                       help='VLAè®ºæ–‡CSVæ•°æ®è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./analysis_results',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--report_name', type=str, default='vla_survey_report.md',
                       help='ç»¼è¿°æŠ¥å‘Šæ–‡ä»¶å')
    parser.add_argument('--classification_name', type=str, default='vla_classification_summary.json',
                       help='åˆ†ç±»æ€»ç»“æ–‡ä»¶å')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # å®Œæ•´è·¯å¾„
    report_path = os.path.join(args.output_dir, args.report_name)
    classification_path = os.path.join(args.output_dir, args.classification_name)
    
    # è¿è¡Œåˆ†æ
    analyzer = VLASurveyAnalyzer(args.csv_path)
    analyzer.run_full_analysis(report_path, classification_path)


if __name__ == "__main__":
    main()
