{
  "VLA Architecture Taxonomy": [
    {
      "title": "Towards Safe Robot Foundation Models",
      "authors": "未提及",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "未提及",
      "contribution": "增加安全层约束通用策略动作空间",
      "performance": "安全约束与策略泛化的平衡",
      "limitations": "自动化安全约束生成",
      "future_work": "安全层模块化设计"
    },
    {
      "title": "Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation",
      "authors": "Sanping Zhoua, Yizhe Lia, Ye Dengd, Le Wanga",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "时间统一去噪过程和动作识别训练方法",
      "contribution": "设计时间统一的速度场和动作识别训练方法",
      "performance": "82.6%平均成功率",
      "limitations": "密集交互场景适应性差，参数调整复杂",
      "future_work": "优化超参数选择和适应更复杂场景"
    },
    {
      "title": "iManip: Skill-Incremental Learning for Robotic Manipulation",
      "authors": "Zexin Zheng, Jia-Feng Cai, Xiao-Ming Wu, Yi-Lin Wei, Yu-Ming Tang et al.",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "动作提示扩展+熵最大化采样",
      "contribution": "技能增量学习框架解决遗忘问题+可扩展PerceiverIO",
      "performance": "平均成功率45.5%(仿真)/未提及(真实)",
      "limitations": "未评估长期增量效果",
      "future_work": "更大规模技能扩展"
    },
    {
      "title": "CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation",
      "authors": "未提及",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "视觉语言动作协同表示",
      "contribution": "使用2D视觉语言提示驱动机器人操作",
      "performance": "成功率74%(seen)/72%(unseen)",
      "limitations": "依赖深度信息/无避障",
      "future_work": "提升泛化能力/处理遮挡"
    },
    {
      "title": "DexTOG: Learning Task-Oriented Dexterous Grasp with Language Condition",
      "authors": "Jieyi Zhang, Wenqiang Xu, Zhenjun Yu, Pengfei Xie",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "仿真验证闭环数据引擎",
      "contribution": "提出语言引导的扩散框架DexTOG和数据集DexTOG-80K",
      "performance": "Q1指标提升30%",
      "limitations": "泛化性受限+仿真到现实差距",
      "future_work": "集成VLM+领域随机化"
    },
    {
      "title": "TRIVLA: A UNIFIED TRIPLE-SYSTEM-BASED UNI- FIED VISION-LANGUAGE-ACTION MODEL FOR GEN- ERAL ROBOT CONTROL",
      "authors": "Zhenyang Liu1,2 Yongchong Gu1,2 Sixiao Zheng1,2 Xiangyang Xue1† Yanwei Fu1,2†",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "视频扩散模型首次应用于VLA架构",
      "contribution": "统一三系统架构整合世界知识与世界模型",
      "performance": "Calvin ABC→D 4.37平均任务长度,LIBERO 87%成功率",
      "limitations": "依赖预训练基座模型",
      "future_work": "扩展更多机器人形态"
    },
    {
      "title": "RoboBrain: A Unified Brain Model for Robotic Manipulation",
      "authors": "未提及",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首创A-LoRA/T-LoRA模块处理affordance与轨迹预测",
      "contribution": "提出ShareRobot数据集与RoboBrain模型（LLaVA框架+SigLIP视觉编码器+Qwen2.5-7B）解决机器人长时规划问题",
      "performance": "SOTA RoboVQA(55.05 BLEU4)",
      "limitations": "依赖预训练数据质量，空间感知不足",
      "future_work": "增强空间理解与安全推理"
    },
    {
      "title": "LUMOS: Language-Conditioned Imitation Learning with World Models",
      "authors": "Iman Nematollahi, Akshay L Chandra, Nick Hawes, Wolfram Burgard, Ingmar Posner",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "未提及",
      "contribution": "世界模型潜在空间的语言条件模仿学习",
      "performance": "牺牲分辨率换取训练效率",
      "limitations": "提升世界模型鲁棒性",
      "future_work": "潜在空间对齐+内在奖励设计"
    },
    {
      "title": "Universal Actions for Enhanced Embodied Foundation Models",
      "authors": "Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang et al.",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首次提出可迁移的跨平台universal action space",
      "contribution": "提出Universal Action Space统一异构机器人动作表征",
      "performance": "超越14×规模基线模型",
      "limitations": "仅限于单臂机器人评估",
      "future_work": "扩展到双手机器人/自动驾驶"
    },
    {
      "title": "ACTLLM: Action Consistency Tuned Large Language Model",
      "authors": "Jing Bi, Lianggong Bruce Wen, Zhang Liu, Chenliang Xu",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "将MDP重构为多轮视觉对话框架",
      "contribution": "提出动作一致性损失函数和多轮视觉对话框架",
      "performance": "VIMA-Bench L1-L4平均成功率90.5%",
      "limitations": "2D图像难以精确3D定位",
      "future_work": "探索真实场景应用和上下文学习能力"
    },
    {
      "title": "From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment",
      "authors": "未提及",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "潜在状态与VLM的专利对齐方法",
      "contribution": "使用潜在世界模型预测动作结果并用VLM评估实现策略引导",
      "performance": "成功率提升50%-70%",
      "limitations": "世界模型预测不精确、推理延迟高",
      "future_work": "改进世界模型、减少推理延迟"
    },
    {
      "title": "Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing",
      "authors": "未提及",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "首个整合RAG+VLA的液体操作机器人系统",
      "contribution": "集成视觉模块(YOLOv8/EasyOCR)、语音模块(Whisper-1)、动作模块(双臂UR3机器人)的鸡尾酒调制系统",
      "performance": "视觉模块91%准确率/语音模块93%准确率/整体系统100%成功率",
      "limitations": "视觉模块在俄语OCR表现有限/噪声环境语音识别有待提升",
      "future_work": "多语言OCR增强/噪声鲁棒性优化/拓展至制药等新领域"
    },
    {
      "title": "A Generative System for Robot-to-Human Handovers: from Intent Inference to Spatial Configuration Imagery",
      "authors": "Hanxin Zhang, Abdulqader Dhafer",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "扩散模型在VLA任务中的创新应用",
      "contribution": "基于MLLMs的意图推理和扩散模型生成空间配置",
      "performance": "80-93%成功率",
      "limitations": "侧视角手势识别准确率低",
      "future_work": "优化实时性能"
    },
    {
      "title": "An Atomic Skill Library Construction Method for Data-Efficient Embodied Manipulation",
      "authors": "Dongjiang Li, Bo Peng, Chang Li, Ning Qiao, Qi Zheng et al.",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "三阶段自驱动技能库构建方法",
      "contribution": "基于原子技能库的数据高效VLA框架",
      "performance": "任务成功率提升20-40%",
      "limitations": "依赖预训练VLA性能/空间泛化有限",
      "future_work": "原子技能自动化扩展"
    },
    {
      "title": "THINK SMALL, ACT BIG: PRIMITIVE-LEVEL SKILL PROMPT LEARNING FOR LIFELONG ROBOT MANIPU-",
      "authors": "未提及",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "未提及",
      "contribution": "提出分层式技能提示学习框架实现终身机器人操作",
      "performance": "计算效率优于MOE/LoRA",
      "limitations": "扩展到更多日常任务和改进光照鲁棒性",
      "future_work": "运动感知提示与终身技能扩展机制"
    },
    {
      "title": "VLA Model-Expert Collaboration for Bi-directional Manipulation Learning",
      "authors": "Ao-Qun Jin, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu et al.",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "专家-VLA协作的双向学习机制",
      "contribution": "专家-VLA协作双向学习框架",
      "performance": "成功率提升15.3%（MT50基准）",
      "limitations": "依赖专家知识且任务泛化有限",
      "future_work": "扩展至更多机器人平台和应用场景"
    },
    {
      "title": "SAM2Act SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation",
      "authors": "Ranjay Krishna, Jiafei Duan",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "SAM2特征与记忆组件的创新结合",
      "contribution": "整合SAM2视觉基础模型与记忆架构",
      "performance": "RLBench平均成功率86.8%, MemoryBench 94.3%",
      "limitations": "固定记忆窗口长度,无法处理连续控制",
      "future_work": "扩展记忆机制适应不同长度任务"
    },
    {
      "title": "DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control",
      "authors": "Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen et al.",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "10亿参数扩散专家首次应用于VLA",
      "contribution": "提出10亿参数扩散专家模块+三阶段课程学习策略",
      "performance": "0.92分衬衫折叠任务得分",
      "limitations": "需要高质量演示数据+计算成本高",
      "future_work": "优化计算效率+降低数据需求"
    },
    {
      "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
      "authors": "Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang et al.",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "稀疏上下文记忆机制+FAV自由视角",
      "contribution": "结合视频扩散模型与4D高斯泼溅的机器人规划框架",
      "performance": "LIBERO基准88.5分",
      "limitations": "边界动作空间局限、透明物体处理",
      "future_work": "缩小sim-to-real差距"
    },
    {
      "title": "Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration",
      "authors": "Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo et al.",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "自监督运动-语言对齐与跨模态注意力机制",
      "contribution": "集成语言理解、场景感知与运动控制的统一框架",
      "performance": "FID 0.467,DIV 4.585",
      "limitations": "RL策略鲁棒性不足，高质量数据有限",
      "future_work": "扩展通用人形机器人任务数据集"
    },
    {
      "title": "Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better",
      "authors": "未提及",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "知识隔离架构创新",
      "contribution": "知识隔离训练方法保持VLM语义知识",
      "performance": "LIBERO-90 SOTA",
      "limitations": "需额外动作专家模块",
      "future_work": "扩展更多机器人场景"
    },
    {
      "title": "TLA: Tactile-Language-Action Model for Contact-Rich Manipulation",
      "authors": "Peng Hao, Chaofan Zhang, Dingzhe Li, Xiaoge Cao, Xiaoshuai Hao et al.",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "触觉模态首次对齐语言-动作空间",
      "contribution": "首个基于触觉-语言-动作模态的机器人操作模型，实现85%跨形状装配成功率",
      "performance": "85%跨形状成功率，L1误差降低78%",
      "limitations": "对垂直于夹具方向的触觉感知不足",
      "future_work": "3D触觉表征改进，多传感器融合"
    },
    {
      "title": "PointVLA: Injecting the 3D World into Vision-Language-Action Models",
      "authors": "Chengmeng Li, Yichen Zhu, Junjie Wen, Yan Peng, Yaxin Peng et al.",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "模块化3D特征注入+跳过块分析",
      "contribution": "注入3D点云数据增强预训练VLA模型",
      "performance": "超越OpenVLA/DexVLA基线",
      "limitations": "依赖2D预训练、3D数据有限",
      "future_work": "改进点云编码器"
    },
    {
      "title": "EVO-0: VISION-LANGUAGE-ACTION MODEL WITH IMPLICIT SPATIAL UNDERSTANDING",
      "authors": "Tao Lin, Gen Li, Yilei Zhong, Yanwen Zou, Bo Zhao",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "隐式几何特征注入VLA框架的创新设计",
      "contribution": "通过VGGT隐式注入3D几何特征提升空间理解能力",
      "performance": "在5项任务中均超越基线",
      "limitations": "依赖预训练VGGT模型,增加计算开销",
      "future_work": "探索更轻量级几何特征集成"
    },
    {
      "title": "MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real",
      "authors": "Renhao Wang, Haoran Geng, Tingle Li, Feishi Wang, Gopala Anumanchipalli, Philipp Wu, Trevor Darrell, Boyi Li, Pieter Abbeel, Jitendra Malik, Alexei A. Efros",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "物理仿真与生成模型混合架构",
      "contribution": "通过生成模型增强物理模拟器实现多模态仿真",
      "performance": "NMAE降低23.3%",
      "limitations": "极端容器几何泛化有限",
      "future_work": "自动化real-to-sim对齐"
    },
    {
      "title": "OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation",
      "authors": "Ankit Goyal, Stan Birchfield, Dieter Fox, Animesh Garg, Valts Blukis",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首个3D感知的VLA框架+正交视图动作解码",
      "contribution": "结合3D感知与VLA优势，通过正交图像生成提升泛化能力",
      "performance": "ARNOLD相对提升40%, COLOSSEUM 10.5%成功率",
      "limitations": "计算密集型扩散采样+长序列任务表现差",
      "future_work": "扩展多模态交互+大规模机器人数据集预训练"
    },
    {
      "title": "OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction",
      "authors": "Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam et al.",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "未提及",
      "contribution": "使用预训练CLIP模型构建文本感知视觉特征提取框架",
      "performance": "以更高推理效率实现零样本泛化",
      "limitations": "扩展复杂任务处理能力",
      "future_work": "利用预训练VLM知识避免微调"
    },
    {
      "title": "SmolVLA: A vision-language-action model for affordable and efficient robotics",
      "authors": "Mustafa Shukor, Dana Aubakirova, Francesco Capuano",
      "year": 2025,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "轻量化VLA+社区驱动数据方案",
      "contribution": "紧凑型VLA架构设计+异步推理方法",
      "performance": "LIBERO 87.3%, MetaWorld 57.3%",
      "limitations": "跨平台泛化能力不足",
      "future_work": "自动化数据集标准化"
    },
    {
      "title": "RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model",
      "authors": "Shunlei Li, Jin Wang, Rui Dai, Wanyu Ma, Wing Yin Ng et al.",
      "year": 2024,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "SAM 2集成解决细长物体抓取问题",
      "contribution": "首次将SAM 2和Llama 2整合用于手术器械交接任务",
      "performance": "90%成功率(已知工具)",
      "limitations": "依赖预训练检测器",
      "future_work": "临床环境适配"
    },
    {
      "title": "Diffusion-VLA: Generalizable and Interpretable Robot Foundation Model via Self-Generated Reasoning",
      "authors": "Yichen Zhu, Minjie Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou et al.",
      "year": 2024,
      "architecture": "混合架构",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "FiLM注入推理模块设计",
      "contribution": "结合自回归推理与扩散策略的统一框架",
      "performance": "排序任务成功率66.2%,零样本抓取63.7%",
      "limitations": "未报告功耗指标/量化性能下降",
      "future_work": "更大规模预训练"
    },
    {
      "title": "CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation",
      "authors": "Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao et al.",
      "year": 2024,
      "architecture": "混合架构",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "未提及",
      "contribution": "专门的动作模块(DiT)结合VLM处理机器人动作",
      "performance": "Google Robot成功率74.8%,超越RT-2-X 18%",
      "limitations": "未提及",
      "future_work": "组件化VLA架构与扩散动作建模"
    },
    {
      "title": "SpatialBot: Precise Spatial Understanding with Vision Language Models",
      "authors": "Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang et al.",
      "year": 2024,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "深度API设计+三阶段渐进式训练框架",
      "contribution": "提出的SpatialBot通过整合RGB-D数据和专用API增强VLA模型的3D空间理解能力",
      "performance": "在SpatialBench上深度理解准确率>99%",
      "limitations": "需要精确深度标注",
      "future_work": "扩展至更多机器人场景"
    },
    {
      "title": "RoboNurse-VLA: Robotic Scrub Nurse System based on Vision-Language-Action Model",
      "authors": "Shunlei Li, Jin Wang, Rui Dai, Wanyu Ma, Wing Yin Ng et al.",
      "year": 2024,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "未提及",
      "contribution": "手术器械交接任务的VLA创新应用",
      "performance": "高精度需求与5Hz实时性平衡",
      "limitations": "临床环境适配/障碍物规避",
      "future_work": "首个手术室场景SAM 2+Llama 2整合方案"
    },
    {
      "title": "Depth Helps: Improving Pre-trained RGB-based Policy with Depth Information Injection",
      "authors": "Wenke Xia, Zhigang Wang, Bin Zhao, Di Hu, Dong Wang et al.",
      "year": 2024,
      "architecture": "混合架构",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "多模态特征离散化+跨模态知识蒸馏",
      "contribution": "通过深度信息注入增强RGB预训练策略的3D感知能力",
      "performance": "LIBERO基准测试平均成功率63.95%",
      "limitations": "长期轨迹误差累积问题",
      "future_work": "探索其他感知模态扩展"
    },
    {
      "title": "Prompt a Robot to Walk with Large Language Models",
      "authors": "Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath",
      "year": 2023,
      "architecture": "混合架构",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "LLM直接输出低级控制信号",
      "contribution": "使用LLM作为低层反馈控制器",
      "performance": "Quadruped机器人成功行走",
      "limitations": "仅GPT-4可用，低推理频率",
      "future_work": "探索更高效prompt设计"
    },
    {
      "title": "VISION-LANGUAGE FOUNDATION MODELS AS EFFEC- TIVE ROBOT IMITATORS",
      "authors": "未提及",
      "year": 2023,
      "architecture": "混合架构",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首个在资源受限条件下微调开源VLM的机器人控制框架",
      "contribution": "基于OpenFlamingo构建RoboFlamingo框架，通过微调实现低成本机器人控制",
      "performance": "ABCD→D成功率96.4%(5任务序列成功率66%)",
      "limitations": "依赖预训练VLM、实时性未验证、未部署实物机器人",
      "future_work": "实物机器人部署、结合大规模真实机器人数据"
    },
    {
      "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
      "authors": "Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao",
      "year": 2023,
      "architecture": "混合架构",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "VLM直接用于闭环规划",
      "contribution": "利用GPT-4V实现机器人视觉-语言任务规划",
      "performance": "80%成功率优于基线SayCan(13%)",
      "limitations": "依赖大规模VLM+即时API成本",
      "future_work": "改进基础技能获取+扩展到复杂场景"
    },
    {
      "title": "ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy",
      "authors": "Shuai Tian, Shugao Liu, Yingting Zhou",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "一致性策略+RL微调整合",
      "contribution": "基于一致性策略的强化微调方法",
      "performance": "平均成功率96.3% 30.7步长",
      "limitations": "依赖人类干预",
      "future_work": "扩展至多任务场景"
    },
    {
      "title": "GRAPE: Generalizing Robot Policy via Preference Alignment",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "多阶段时空约束与轨迹级偏好优化",
      "contribution": "基于轨迹偏好的VLA策略泛化方法",
      "performance": "成功率提升51.79%(域内)/58.20%(域外)",
      "limitations": "依赖预训练VLAs、未量化推理效率",
      "future_work": "偏好对齐机制扩展"
    },
    {
      "title": "SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model",
      "authors": "Delin Qu, Haoming Song, Qizhi Chen, Dong Wang, Yuanqi Yao et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "创新动作离散化方法",
      "contribution": "提出Ego3D位置编码和自适应动作网格",
      "performance": "超越RT-2-X等SOTA模型",
      "limitations": "需依赖深度估计,高维动作扩展性不足",
      "future_work": "共享动作网格方案"
    },
    {
      "title": "VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers",
      "authors": "Tokenizers Yating Wang, Haoyi Zhu, Mingyu Liu, Jiange Yang, Shu Fang et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "未提及",
      "contribution": "使用大规模向量量化动作分词器提升VLA性能",
      "performance": "牺牲11.84Hz推理速度换取成功率提升",
      "limitations": "扩大tokenizer适用场景",
      "future_work": "卷积残差VQ-VAE动作分词器设计"
    },
    {
      "title": "Task Reconstruction and Extrapolation for π0 using Text Latent",
      "authors": "Quanyi Li",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "隐藏空间干预创新",
      "contribution": "文本潜在空间操控实现任务重构与外推",
      "performance": "83%成功率",
      "limitations": "空间过拟合问题、依赖训练数据位置映射",
      "future_work": "文本潜在空间的进一步解释与优化"
    },
    {
      "title": "RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "RGB-D数据标准化与跨模态注意力机制",
      "contribution": "深度-RGB模态融合与跨注意力特征集成",
      "performance": "任务成功率提升10-20%",
      "limitations": "依赖预训练ViT,计算成本高",
      "future_work": "仿真到实物的迁移"
    },
    {
      "title": "Gemini Robotics: Bringing AI into the Physical",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "具身推理基准ERQA与通用VLA架构创新",
      "contribution": "基于Gemini 2.0的具身推理与机器人控制系统",
      "performance": "79%成功率长视野任务",
      "limitations": "3D空间理解精度不足、跨视频推理能力有限",
      "future_work": "提升多步推理与执行精度、仿真数据利用、零样本跨实体迁移"
    },
    {
      "title": "ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model",
      "authors": "Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "Phased Alignment Training+MoE的创新组合架构",
      "contribution": "提出Phased Alignment Training和Mixture-of-Experts架构解决VLA模型遗忘问题和任务干扰",
      "performance": "MMMU 37.4分比ECoT提升6倍,机器人任务成功率55/107",
      "limitations": "专业领域视觉语言数据不足",
      "future_work": "探索更适合的专业数据训练"
    },
    {
      "title": "MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models",
      "authors": "Han Zhao, Wenxuan Song, Donglin Wang, Xinyang Tong, Pengxiang Ding et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首次将MoE架构应用于大规模端到端VLA模型",
      "contribution": "混合LoRA专家架构与RL训练目标",
      "performance": "6项任务成功率提升20%",
      "limitations": "需预训练大模型基础、未量化推理效率",
      "future_work": "扩展更多机器人任务与场景"
    },
    {
      "title": "Real-Time Execution of Action Chunking Flow Policies",
      "authors": "Kevin Black, Manuel Y. Galliker, Sergey Levine",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "inpainting思想迁移",
      "contribution": "实时推理算法RTC,解决动作分块延迟问题",
      "performance": "延迟下最高成功率+20%任务吞吐量",
      "limitations": "需额外计算开销",
      "future_work": "优化分布式推理"
    },
    {
      "title": "Refined Policy Distillation: From VLA Generalists to RL Experts",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首次实现VLA到RL的精细化蒸馏",
      "contribution": "将通用VLA蒸馏为专用RL策略",
      "performance": "超越教师VLA 20-40%成功率",
      "limitations": "依赖教师VLA质量、任务泛化有限",
      "future_work": "改进跨任务蒸馏能力"
    },
    {
      "title": "AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "扩散Transformer在VLA任务的应用创新",
      "contribution": "整合移动与操作协调的扩散Transformer框架",
      "performance": "55.6%平均成功率(MSHab)",
      "limitations": "依赖演示数据质量+移动控制累积误差",
      "future_work": "提升模型泛化能力+错误补偿机制"
    },
    {
      "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
      "authors": "Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "频域动作压缩",
      "contribution": "离散余弦变换动作tokenization方法",
      "performance": "匹配diffusion VLA性能",
      "limitations": "推理延迟较高/依赖预训练VLM",
      "future_work": "加速推理优化/扩展到更多机器人形态"
    },
    {
      "title": "Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation",
      "authors": "Zetao Cai, Yang Tian, Jia Zeng, Jiangmiao Pang",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "创新接口设计提升VLA空间定位能力",
      "contribution": "结合目标夹爪姿态和物体点流预测的连续动作生成框架",
      "performance": "RLBench2上80.8%成功率,+16.1% SOTA提升",
      "limitations": "计算成本较高(扩散变换器)",
      "future_work": "轻量化视觉模型+跨硬件评估"
    },
    {
      "title": "TrackVLA: Embodied Visual Tracking in the Wild",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "跨模态融合架构",
      "contribution": "联合训练目标识别与轨迹规划",
      "performance": "500步骤成功率100%",
      "limitations": "依赖单视角输入",
      "future_work": "多视角整合"
    },
    {
      "title": "Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding",
      "authors": "Grounding Joshua Jones, Oier Mees, Carmelo Sferrazza, Kyle Stachowicz, Pieter Abbeel et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "语言锚定的跨模态特征对齐架构",
      "contribution": "利用语言作为跨模态桥梁扩展预训练VLA模型的多模态感知能力",
      "performance": "成功率提升20%",
      "limitations": "训练资源需求高、观测历史受限(0.4秒)",
      "future_work": "提升训练效率以支持更长上下文"
    },
    {
      "title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "提出首个系统评估VLM到VLA能力迁移的基准测试",
      "contribution": "建立包含50个任务的INT-ACT基准测试套件评估VLA泛化能力",
      "performance": "意图正确率80-100%/任务成功率21-68%",
      "limitations": "感知与动作执行gap/语言能力退化",
      "future_work": "扩展到更多机器人形态/自动化任务生成"
    },
    {
      "title": "Training Strategies for Efficient Embodied Reasoning",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "创新推理训练范式提升计算效率",
      "contribution": "ECoT-Lite训练策略实现3倍推理加速",
      "performance": "LIBERO-90 SOTA(90.8%), Bridge推理提速3倍",
      "limitations": "需要专门的推理标注数据",
      "future_work": "探索非配对推理数据利用"
    },
    {
      "title": "Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success",
      "authors": "Moo Jin, Chelsea Finn, Percy Liang",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "创新微调配方(并行解码+连续动作+L1回归)",
      "contribution": "提出优化微调方法OFT提升VLA模型速度和成功率",
      "performance": "97.1%成功率(LIBERO),100分(ALOHA)",
      "limitations": "多模态动作生成能力有限",
      "future_work": "改进多模态动作处理,预训练研究"
    },
    {
      "title": "cVLA: Towards Efficient Camera-Space VLAs",
      "authors": "Max Argus, Houman Masnavi, Abhinav Valada, Thomas Brox",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "使用图像坐标系替代传统机器人坐标系",
      "contribution": "利用图像坐标系预测轨迹关键点提升训练效率",
      "performance": "CLEVR-hard模拟成功率28%/Objaverse-easy 52%",
      "limitations": "仅支持桌面任务/旋转预测精度不足/模拟训练限制",
      "future_work": "提升鲁棒性/跨任务扩展/更高精度深度融合"
    },
    {
      "title": "Improving Vision-Language-Action Model with Online Reinforcement Learning",
      "authors": "Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "迭代RL+SL交替优化架构",
      "contribution": "提出iRe-VLA迭代强化学习框架稳定大规模VLA训练",
      "performance": "MetaWorld成功率从0.35提升至0.80",
      "limitations": "无法学习全新技能",
      "future_work": "稀疏奖励下新技能学习"
    },
    {
      "title": "VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation",
      "authors": "Chaofan Zhang, Peng Hao, Xiaoge Cao, Xiaoshuai Hao, Shaowei Cui et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "VTLA模型首次实现视觉-触觉-语言三模态融合的端到端策略生成",
      "contribution": "融合视觉-触觉-语言的接触密集型操作策略生成",
      "performance": "90%+成功率(0.6mm间隙),95%真实世界成功率",
      "limitations": "未提及真实世界训练数据需求",
      "future_work": "扩展偏好数据多样性"
    },
    {
      "title": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks",
      "authors": "Yi Yang, Jiaxuan Sun, Siqi Kou, Yihan Wang, Zhijie Deng",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "共享视觉语言主干实现多任务泛化",
      "contribution": "集成高层任务规划与低层运动控制的统一VLA框架",
      "performance": "相比基线提升30-50%准确率",
      "limitations": "动作离散化精度限制",
      "future_work": "提升实时性"
    },
    {
      "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning",
      "authors": "Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首次展示VLA的推理规模定律",
      "contribution": "使用强化学习提升预训练VLA模型的在线探索能力",
      "performance": "超越SFT基线4.5%,匹配π0-FAST性能",
      "limitations": "伪奖励标注可能不完善",
      "future_work": "扩展到diffusion策略+真实世界应用"
    },
    {
      "title": "Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions",
      "authors": "Interleaved Image, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首创交错指令VLA范式+自动数据集生成Pipeline",
      "contribution": "提出首个支持交错图文指令的VLA框架并构建首个大规模真实世界交错具身数据集（210K episodes）",
      "performance": "OOD泛化能力提升2-3倍",
      "limitations": "训练计算成本高",
      "future_work": "支持交错输入输出的统一VLA模型"
    },
    {
      "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首款支持1000+原子任务的Minecraft VLA模型",
      "contribution": "视觉语言后训练范式(ActVLP)提升VLA决策能力",
      "performance": "超越基准STEVE-1 40%成功率",
      "limitations": "40Hz推理速度限制",
      "future_work": "MoE架构优化"
    },
    {
      "title": "FP3: A 3D Foundation Policy for Robotic Manipulation",
      "authors": "Rujia Yang*1, Geng Chen*2,4† , Chuan Wen‡1,2,3, and Yang Gao‡1,2,3",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "扩散变换器+3D点云编码器",
      "contribution": "首个基于3D点云的大规模策略基础模型,支持高效微调和泛化",
      "performance": "90%成功率(领域内)+82.5%(跨领域)",
      "limitations": "依赖DROID数据集+计算成本较高",
      "future_work": "扩展多模态输入+更高效架构"
    },
    {
      "title": "InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning",
      "authors": "Ji Zhang, Shihan Wu, Xu Luo, Hao Wu, Lianli Gao et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "空间推理VQA作为观察-动作间的桥梁语言",
      "contribution": "通过空间推理VQA任务增强VLA的因果推理能力，减少虚假关联",
      "performance": "提升成功率：仿真任务+6.2%(seen)/+10%(unseen)，真实任务+25%(seen)/+26%(unseen)",
      "limitations": "未验证扩散策略类VLA的适配性；不同VLA间虚假关联差异未研究",
      "future_work": "探索扩散策略类VLA的适配方案；虚假关联的跨模型分析"
    },
    {
      "title": "SafeVLA: Towards Safety Alignment of Vision- Language-Action Model via Constrained Learning",
      "authors": "Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首个系统整合SafeRL与VLA的研究",
      "contribution": "安全约束深度强化学习框架",
      "performance": "83.58%安全提升+3.85%任务性能",
      "limitations": "无真实世界验证,计算需求高",
      "future_work": "扩展到真实机器人部署"
    },
    {
      "title": "CEED-VLA : Consistency Vision-Language-Action Model with Early-Exit Decoding",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "一致性蒸馏首次应用于VLA加速",
      "contribution": "并行解码加速与一致性蒸馏",
      "performance": "77.5%成功率(真实世界)",
      "limitations": "不适合连续动作空间",
      "future_work": "探索连续动作收敛条件"
    },
    {
      "title": "Temporal Representation Alignment: Successor Features Enable Emergent Compositionality in Robot Instruction Following",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "通过时间对比对齐实现隐式组合性",
      "contribution": "提出Temporal Representation Alignment方法提升机器人指令跟随的组合泛化能力",
      "performance": "组合任务成功率提升>40%",
      "limitations": "依赖行为克隆的Gaussian Policy限制",
      "future_work": "改进多模态行为处理"
    },
    {
      "title": "VLAS: VISION-LANGUAGE-ACTION MODEL SPEECH INSTRUCTIONS FOR CUSTOMIZED ROBOT MANIPULATION",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "语音-文本对齐+Voice RAG",
      "contribution": "直接整合语音输入的VLA模型",
      "performance": "CALVIN基准86.5%成功率",
      "limitations": "语音理解精度待提升",
      "future_work": "探索语音中更多辅助信息"
    },
    {
      "title": "OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning",
      "authors": "Fanqi Lin,Ruiqian Nai,Yingdong Hu,Jiacheng You,Junming Zhao,Yang Gao",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "未提及",
      "contribution": "统一模型自适应切换推理模式与动作生成",
      "performance": "自适应推理框架、联合训练合成数据、多模态动作生成",
      "limitations": "首次实现VLA模型动态切换System1/System2模式",
      "future_work": "未提及"
    },
    {
      "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
      "authors": "Qiao Gu, Yuanliang Ju, Shengxiang Sun, Igor Gilitschenski, Haruki Nishimura et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "利用VLA内部特征实现高效故障检测",
      "contribution": "基于VLA内部特征的多任务故障检测器",
      "performance": "ROC-AUC 64.16-86.76 (Real-world)",
      "limitations": "仅使用最后一层特征、未测试跨本体/跨模态泛化能力",
      "future_work": "多层特征融合、跨本体泛化"
    },
    {
      "title": "Pre-training Auto-regressive Robotic Models with 4D Representations",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "未提及",
      "contribution": "利用人类视频预训练4D表示迁移到机器人控制",
      "performance": "牺牲数据通用性换取跨机器人适应性",
      "limitations": "3D世界坐标跟踪+多视图融合",
      "future_work": "未提及"
    },
    {
      "title": "HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model",
      "authors": "Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "统一LLM框架下的协同训练方案",
      "contribution": "在单一大语言模型中整合扩散模型和自回归方法用于连续动作预测",
      "performance": "仿真74%成功率,真实世界83%成功率",
      "limitations": "双臂协调任务存在动作预测冲突",
      "future_work": "优化实时推理效率"
    },
    {
      "title": "Diffusion Transformer Policy",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "Diffusion与Transformer的协同设计",
      "contribution": "Diffusion Transformer架构直接去噪连续动作序列",
      "performance": "Calvin ABC→D 50.0%成功率",
      "limitations": "多步去噪导致推理延迟",
      "future_work": "优化推理步骤"
    },
    {
      "title": "A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation",
      "authors": "Multitask Dexterous",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首次系统评估LBM在灵巧操作中的资源-性能平衡",
      "contribution": "验证多任务预训练提升策略鲁棒性及数据效率",
      "performance": "多任务微调相比单任务基线性能提升30%",
      "limitations": "未明确解决语言引导脆弱性问题",
      "future_work": "扩展更大规模VLA验证语言能力"
    },
    {
      "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
      "authors": "Yuan Zhang, Shikai Geng, Gaowen Liu, Joschka Boedecker, Chris Xiaoxuan Lu",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "推理级加速技术",
      "contribution": "思想复用与推理并行化加速ECoT",
      "performance": "80%成功率(仿真),70%成功率(真实),7.5×加速",
      "limitations": "高动态场景适应性有限,固定推理结构",
      "future_work": "动态推理更新策略,多模态数据增强"
    },
    {
      "title": "RoboBERT:",
      "authors": "Sicheng Wang, Sheng Liu, Weiheng Wang, Jianhua Shan, Bin Fang",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "冻结视觉编码器+语言指令分层对齐",
      "contribution": "两阶段训练范式+数据增强提升资源受限场景性能",
      "performance": "CALVIN ABCD→D 4.52平均ep长度, ABC→D 3.79",
      "limitations": "未提及长期规划能力",
      "future_work": "扩展到更多机器人平台"
    },
    {
      "title": "Magma: A Foundation Model for Multimodal AI Agents",
      "authors": "Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "Set-of-Mark/Trace-of-Mark标注创新",
      "contribution": "首个支持数字和物理环境的多模态智能体基础模型",
      "performance": "ScreenSpot 96.3%/SimperEnv 52.3%",
      "limitations": "未提及",
      "future_work": "扩展视频理解能力"
    },
    {
      "title": "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "双系统VLA架构+数据金字塔训练策略",
      "contribution": "双系统架构Vision-Language-Action模型+跨机器人形态支持",
      "performance": "RoboCasa任务49.6%成功率,现实任务76.8%成功率",
      "limitations": "长视野任务支持不足+物理约束生成",
      "future_work": "长视野locomanipulation+增强合成数据质量"
    },
    {
      "title": "RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour",
      "authors": "Valerii Serpiva, Artem Lykov, Artyom Myshlyaev, Muhammad Haris Khan, Ali Alridha Abdulkarim",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首个将VLA应用于高速竞速无人机",
      "contribution": "首个面向竞速无人机的VLA模型，实现视觉-语言-动作直接映射",
      "performance": "平均速度1.04m/s,最大2.02m/s",
      "limitations": "视觉泛化下降(79.6 vs 87.0)",
      "future_work": "提升视觉/物理泛化能力"
    },
    {
      "title": "Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "Transformer扩散策略直接建模动作序列",
      "contribution": "引入in-context conditioning机制统一多模态输入",
      "performance": "LIBERO-LONG任务77.9%成功率",
      "limitations": "依赖第三方预训练模型(CLIP/DINOv2)、语言标注有限",
      "future_work": "扩大模型规模提升OXE数据集利用率"
    },
    {
      "title": "RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models",
      "authors": "Yuxuan Chen, Xiao Li",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首次将RL应用于压缩VLA的性能恢复，混合剪枝+量化方案",
      "contribution": "针对VLA模型的三阶段压缩恢复方法(结构化剪枝+SFT+RL调优+量化)",
      "performance": "剪枝后成功率90.62%(提升1.56%)，内存降低8×",
      "limitations": "依赖额外训练开销(SFT+RL)，量化引入解压延迟",
      "future_work": "探索更高效的无训练压缩方案"
    },
    {
      "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首次实现全1-bit VLA架构+视觉编码器量化蒸馏",
      "contribution": "首创1-bit三元值参数VLA模型,提出蒸馏感知训练策略压缩视觉编码器",
      "performance": "LIBERO基准94.8%成功率,比4-bit OpenVLA-OFT节省70.2%内存",
      "limitations": "长任务泛化能力较弱(比OpenVLA-OFT低6.9%)",
      "future_work": "探索更大规模机器人预训练的数据效率"
    },
    {
      "title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models",
      "authors": "Qingqing Zhao, Yao Lu, Moo Jin, Zipeng Fu, Zhuoyang Zhang et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "视觉思维链机制创新",
      "contribution": "引入视觉思维链推理机制提升VLA决策能力",
      "performance": "SOTA性能提升17%真实任务/6%仿真",
      "limitations": "推理速度慢/图像生成质量低",
      "future_work": "加速推理算法/提升图像生成质量"
    },
    {
      "title": "WorldVLA: Towards Autoregressive Action World",
      "authors": "未提及",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "离散自回归动作-世界统一建模",
      "contribution": "统一动作模型与世界模型的双向增强框架",
      "performance": "LIBERO基准提升4%抓取成功率+10% FVD降低",
      "limitations": "需要高分辨率输入(512×512)+多动作序列生成效率低",
      "future_work": "优化动作tokenizer+扩展多模态预训练"
    },
    {
      "title": "Learning to Act Anywhere with Task-centric Latent Actions",
      "authors": "Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "潜在动作空间与视觉语言模型融合",
      "contribution": "提出任务中心潜在动作表示实现跨具身策略学习",
      "performance": "LIBERO 95.2%成功率",
      "limitations": "固定粒度潜在动作，单臂操纵限制",
      "future_work": "自适应动作粒度，双臂系统扩展"
    },
    {
      "title": "NORA: A SMALL OPEN-SOURCED GENERALIST VISION LANGUAGE ACTION MODEL FOR EMBODIED TASKS",
      "authors": "Chia-Yu Hung*, Qi Sun*, Pengfei Hong*, Amir Zadeh, Chuan Li, U-Xuan Tan, Navonil Majumder, Soujanya Poria",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "未提及",
      "contribution": "提出3B参数的轻量化VLA模型，优化实时机器人任务效率",
      "performance": "小模型适合消费级GPU微调、FAST+ tokenizer加速",
      "limitations": "首个开源的3B级VLA模型+轻量化方案",
      "future_work": "未提及"
    },
    {
      "title": ": Dexterous Vision-Language-Grasp Model at Scale",
      "authors": "Jiawei He, Danshi Li, Xinqiang Yu, Zekun Qi, Wenyao Zhang et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "引入能量函数优化语义对齐抓取",
      "contribution": "构建大规模DexGraspNet3.0数据集与DexVLG模型",
      "performance": "仿真成功率87.7%，真实世界80%",
      "limitations": "硬件执行空间限制，缺少姿势排序方法",
      "future_work": "改进安全性验证与姿势排序"
    },
    {
      "title": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation",
      "authors": "Rongyu Zhang, Menghang Dong, Yuan Zhang, Liang Heng, Xiaowei Chi et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "动态层选择+生物启发设计",
      "contribution": "动态层跳跃机制提高VLA模型效率",
      "performance": "60.8%成功率(RLBench)",
      "limitations": "空间理解精度不足",
      "future_work": "优化路由器决策机制"
    },
    {
      "title": "ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration",
      "authors": "Yichen Zhu, Jinming Li, Zhongyi Zhou, Junjie Wen, Xiaoyu Liu",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "定位感知推理机制+数据效率优化",
      "contribution": "通过视觉语言数据协同训练实现开放世界物体操控",
      "performance": "64% OOD物体识别成功率+87/150 OOD bin-picking任务",
      "limitations": "对背景/光照变化泛化性不足",
      "future_work": "探索互联网图像数据的适用性"
    },
    {
      "title": "Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation",
      "authors": "Hao Li, Shuai Yang, Yilun Chen, Yang Tian, Xiaoda Yang et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "分层特征聚合+扩散动作解码",
      "contribution": "引入多帧编码和跨帧解码的高效时序建模",
      "performance": "70.9%成功率(SimplerEnv),86.2%(LIBERO)",
      "limitations": "缺少多视角输入,依赖单目RGB观测",
      "future_work": "支持多模态输入,增强语言推理能力"
    },
    {
      "title": "Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding",
      "authors": "Jiayi Chen, Pengxiang Ding, Han Zhao, Wei Zhao, Zhide Zhong et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "非线性系统并行化求解",
      "contribution": "提出并行解码框架PD-VLA加速带动作分块的VLA模型推理",
      "performance": "94.1%成功率 3.64平均任务长度",
      "limitations": "仅验证7B模型/需人工设置解码视界",
      "future_work": "优化解码算法减少冗余迭代"
    },
    {
      "title": "ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation",
      "authors": "Jiawen Yu, Hairuo Liu, Qiaojun Yu, Jieji Ren, Ce Hao et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "MoE-based多模态晚期融合",
      "contribution": "集成6维力反馈的MoE模块(FVLMoE)",
      "performance": "平均成功率60.5%(比baseline↑23.2%)",
      "limitations": "依赖估计的力矩值",
      "future_work": "高保真力觉传感器集成"
    },
    {
      "title": "SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models",
      "authors": "Meng Li, Zhen Zhao, Zhengping Che, Fei Liao, Kun Wu et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "多模态融合与条件行为调制",
      "contribution": "提出执行感知的条件行为调制框架，实现动态任务切换",
      "performance": "单任务成功率93-100%，切换任务成功率50.9-100%",
      "limitations": "依赖接触状态作为任务进展代理，行为模式有限",
      "future_work": "融入语义时序信号，扩展连续行为表征"
    },
    {
      "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation",
      "authors": "Yunke Wang, Chenghao Xia, Dihao Zhu, Tao Huang, Chang Xu",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首次将token重用应用于VLA动作生成",
      "contribution": "提出自适应token缓存机制加速VLA推理",
      "performance": "成功率74.7% vs 基准75.0%",
      "limitations": "仅适用于连续帧相似场景",
      "future_work": "扩展至多模态token缓存"
    },
    {
      "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models",
      "authors": "Peiyan Li, Yixiang Chen, Hongtao Wu, Xiao Ma, Xiangnan Wu et al.",
      "year": 2025,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "创新的3D-2D投影对齐方法+热图预测统一输入输出",
      "contribution": "通过2D热图预测实现3D输入-输出对齐的全新VLA架构",
      "performance": "RLBench成功率88.2% COLOSSEUM鲁棒性64.0%少量样本(3轨迹)成功率96.8%",
      "limitations": "第三视角预训练与机器人视角差异+长时任务处理能力不足",
      "future_work": "扩展预训练数据+结合LLM的任务分解"
    },
    {
      "title": "RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulation",
      "authors": "未提及",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "统一视角表示创新与预训练策略",
      "contribution": "提出统一视角表示方法和UVFormer模块来解耦视觉特征提取与动作学习",
      "performance": "D→D任务成功率96.2%",
      "limitations": "高分辨率查询延迟显著增加",
      "future_work": "训练更大规模的通用机器人模型"
    },
    {
      "title": "Octo: An Open-Source Generalist Robot Policy",
      "authors": "Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "未提及",
      "contribution": "开源通用机器人策略，支持多机器人零样本控制和高效微调",
      "performance": "未提及",
      "limitations": "扩大训练数据范围，改进语言条件处理",
      "future_work": "基于Transformer的统一架构，扩散动作解码器"
    },
    {
      "title": "LATENT ACTION PRETRAINING FROM VIDEOS",
      "authors": "KAIST; University of Washington; Microsoft Research; NVIDIA; Allen Institute for AI",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首次实现无动作标签的VLA预训练",
      "contribution": "无监督预训练VLA模型，无需动作标签",
      "performance": "优于使用动作标签的SOTA VLA模型",
      "limitations": "对精细动作(抓取)表现不足",
      "future_work": "扩展潜在动作空间"
    },
    {
      "title": "Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust",
      "authors": "未提及",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首次提出VLA模型的运行时视觉鲁棒性增强方案",
      "contribution": "提出BYOVLA运行时干预框架，无需微调即可提升VLA模型在视觉干扰下的鲁棒性",
      "performance": "任务成功率提升25-40%",
      "limitations": "依赖VLM和分割模型，阈值τ需调优",
      "future_work": "动态环境适应性优化，自动阈值选择"
    },
    {
      "title": "QUAR-VLA: Vision-Language-Action Model for Quadruped Robots",
      "authors": "Han Zhao, Wenxuan Song, Wenjie Zhang",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "首次将VLA应用于四足机器人领域",
      "contribution": "提出四足机器人VLA新范式QUAR-VLA，融合视觉与语言指令",
      "performance": "中型任务成功率60%",
      "limitations": "缺少复杂地形数据",
      "future_work": "自动化数据采集与更优的sim2real方法"
    },
    {
      "title": "RoboMM: All-in-One Multimodal Large Model for Robotic Manipulation",
      "authors": "Feng Yan, Fanfan Liu, Liming Zheng, Yufeng Zhong, Yiyang Huang et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "Modality-Isolation-Mask创新设计",
      "contribution": "集成视觉-语言-动作的统一模型与空间对齐数据集",
      "performance": "CALVIN平均序列长度1.7→3.3",
      "limitations": "依赖仿真数据/训练成本高",
      "future_work": "扩展真实机器人验证"
    },
    {
      "title": "Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos",
      "authors": "Videos Yi Chen, Yuying Ge, Weiliang Tang, Yizhuo Li, Yixiao Ge et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "离散运动token作为通用动作接口",
      "contribution": "提出隐式运动token作为跨视频与机器人动作的统一表示语言",
      "performance": "SIMPLER基准0.614成功率,CALVIN 3.10平均连续任务",
      "limitations": "仅测试简单操作任务,未验证复杂场景",
      "future_work": "扩展人类视频预训练,探索强化学习应用"
    },
    {
      "title": "OpenVLA: An Open-Source Vision-Language-Action Model",
      "authors": "未提及",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "融合SigLIP+DinoV2视觉编码器",
      "contribution": "开源7B参数VLA模型在机器人控制的应用",
      "performance": "29个任务上比RT-2-X高16.5%成功率",
      "limitations": "仅支持单图像输入、高频控制受限",
      "future_work": "支持多模态输入、提高推理速度"
    },
    {
      "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution",
      "authors": "Yang Yue, Yulin Wang, Bingyi Kang, Yizeng Han, Shenzhi Wang et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "多尺度自适应推理与量化结合",
      "contribution": "动态多出口推理框架降低计算成本",
      "performance": "78.9%任务成功率",
      "limitations": "视觉编码器计算未优化",
      "future_work": "轻量化视觉编码器集成"
    },
    {
      "title": "LLARA: SUPERCHARGING ROBOT LEARNING DATA FOR VISION-LANGUAGE POLICY",
      "authors": "Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "首次将视觉指令调优框架应用于机器人策略转换的创新方法",
      "contribution": "将预训练VLM通过指令调优框架转化为机器人策略，并提出自监督辅助数据集",
      "performance": "VIMA-8k上L1 90.0% L2 88.1% L3 79.2%",
      "limitations": "依赖2D到2D映射，复杂3D运动受限",
      "future_work": "扩展多图像输入处理能力"
    },
    {
      "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation",
      "authors": "Yichen Zhu, Member, Jinming Li, Minjie Zhu, Zhibin Tang et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "预训练多模态模型与扩散策略的创新结合",
      "contribution": "高效紧凑的VLA模型，无需机器人数据预训练阶段",
      "performance": "94%平均成功率(TinyVLA-H)",
      "limitations": "未探讨计算资源对训练影响",
      "future_work": "扩展至更多机器人平台"
    },
    {
      "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
      "authors": "Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Xin Yan, Yilun Du et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "首次实现3D-VLA生成式世界模型",
      "contribution": "提出首个结合3D感知、推理与动作生成的生成式世界模型",
      "performance": "在RLBench/CALVIN上优于基线模型",
      "limitations": "依赖仿真数据，真实世界泛化能力待验证",
      "future_work": "扩展到更大规模3D数据集和真实机器人部署"
    },
    {
      "title": "BAKU: An Efficient Transformer for Multi-Task Policy Learning",
      "authors": "未提及",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "轻量级Transformer+多模态动作预测",
      "contribution": "高效多任务策略学习框架",
      "performance": "LIBERO提升36%,真实任务91%成功率",
      "limitations": "精确操作任务性能不足",
      "future_work": "技能链式组合+任务规模扩展"
    },
    {
      "title": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
      "authors": "Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首个开源百万级跨具身机器人数据集+VLA策略验证",
      "contribution": "大规模跨具身机器人策略迁移",
      "performance": "RT-1-X平均提升50%成功率 RT-2-X 3倍泛化能力提升",
      "limitations": "异构机器人适配困难、实时控制挑战",
      "future_work": "扩展数据集规模、提升异构机器人兼容性"
    },
    {
      "title": "GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation",
      "authors": "Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "视频生成与动作预测联合训练",
      "contribution": "基于互联网视频预训练的通用机器人操作策略",
      "performance": "97.7%平均成功率(105任务)+85.9%成功率(5任务连续)",
      "limitations": "未见物体操作成功率较低(55.8%)",
      "future_work": "改进未见物体操作能力"
    },
    {
      "title": "π0: A Vision-Language-Action Flow Model for General Robot Control",
      "authors": "Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首次将flow matching应用于VLA模型",
      "contribution": "基于flow matching的VLA动作生成算法",
      "performance": "优于OpenVLA/Octo等基线",
      "limitations": "需要高性能GPU、移动端部署困难",
      "future_work": "轻量化部署与量化研究"
    },
    {
      "title": "RDT-1B: A DIFFUSION FOUNDATION MODEL FOR BIMANUAL MANIPULATION",
      "authors": "未提及",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "统一动作空间设计与扩散Transformer架构创新",
      "contribution": "基于扩散Transformer的双手机器人基础模型",
      "performance": "成功率提升56%(相比基线)",
      "limitations": "依赖大规模预训练数据",
      "future_work": "扩展到更多机器人形态"
    },
    {
      "title": "Robotic Control via Embodied Chain-of-Thought Reasoning",
      "authors": "Michał Zawalski∗1,2, William Chen∗1, Karl Pertsch1,3, Oier Mees1, Chelsea Finn3, Sergey Levine1",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首次将CoT引入VLA并融入具身特征",
      "contribution": "Embodied CoT reasoning方法提升VLA任务泛化能力28%",
      "performance": "成功率提升28%",
      "limitations": "推理速度慢、跨机器人泛化有限",
      "future_work": "自动优化reasoning步骤结构、扩大训练数据集"
    },
    {
      "title": "An Embodied Generalist Agent in 3D World",
      "authors": "Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "未提及",
      "contribution": "3D通用智能体LEO整合视觉-语言-动作",
      "performance": "数据质量与模型规模平衡",
      "limitations": "3D数据扩展+VLA能力融合",
      "future_work": "空间Transformer+对象链式思考(O-CoT)"
    },
    {
      "title": "RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics",
      "authors": "Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "未提及",
      "contribution": "自动合成数据生成指令微调VLMs实现精确空间可操作性预测",
      "performance": "未提及",
      "limitations": "置信度估计方法/可控输出",
      "future_work": "合成数据增强VLA微调"
    },
    {
      "title": "Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models",
      "authors": "Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu et al.",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首个系统性VLA设计指南",
      "contribution": "系统研究了VLA架构设计选择与数据使用策略",
      "performance": "CALVIN基准SOTA 82.6%成功率",
      "limitations": "计算成本高,实时控制挑战",
      "future_work": "专用架构设计与动作表征优化"
    },
    {
      "title": "Actra: Optimized Transformer Architecture for Vision-Language-Action Models in Robot Learning",
      "authors": "Yueen Ma1,Dafeng Chi2,Shiguang Wu2,Yuecheng Liu2,Yuzheng Zhuang2,Jianye Hao2,Irwin King1",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首个针对VLA轨迹特性的Transformer架构优化",
      "contribution": "轨迹注意力机制+可学习动作查询,VLA对比学习",
      "performance": "Maniskill2上76.25%成功率超越baselines 19-45.62个百分点",
      "limitations": "1. 未评估扩散模型 2. 仅使用Franka embodiment",
      "future_work": "多embodiment扩展"
    },
    {
      "title": "Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals",
      "authors": "未提及",
      "year": 2024,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "未提及",
      "contribution": "提出MDT框架处理多模态目标+改进稀疏标注学习",
      "performance": "少量标注高效学习",
      "limitations": "扩展更多模态，Open-X pretrain",
      "future_work": "扩散Transformer+自监督目标MGF/CLA"
    },
    {
      "title": "UNLEASHING LARGE-SCALE VIDEO GENERATIVE PRE-TRAINING FOR VISUAL ROBOT MANIPULATION",
      "authors": "Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu et al.",
      "year": 2023,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "未提及",
      "contribution": "利用大规模视频生成预训练增强视觉机器人操作",
      "performance": "扩展至无语言标签视频,增加机器人数据多样性",
      "limitations": "未提及",
      "future_work": "未提及"
    },
    {
      "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "authors": "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen et al.",
      "year": 2023,
      "architecture": "端到端VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首次将动作表示为语言token实现VLA模型统一训练",
      "contribution": "将互联网规模训练的视觉语言模型直接融入机器人低层控制",
      "performance": "未见任务上成功率提高2-6倍",
      "limitations": "新运动技能无法通过VLM获得",
      "future_work": "探索人类视频数据采集方法"
    },
    {
      "title": "VIMA: General Robot Manipulation with Multimodal Prompts",
      "authors": "Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou et al.",
      "year": 2023,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "结合LLM的开放式任务理解",
      "contribution": "多模态提示的通用机器人操作框架",
      "performance": "L4任务成功率49.6%",
      "limitations": "依赖外部物体检测器",
      "future_work": "扩展到真实世界"
    },
    {
      "title": "PaLM-E: An Embodied Multimodal Language Model",
      "authors": "未提及",
      "year": 2023,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首次实现具身推理与通用视觉语言任务统一建模",
      "contribution": "多模态大语言模型在具身决策任务中的应用",
      "performance": "OK-VQA SOTA+多任务正向迁移",
      "limitations": "计算成本高+需低层策略配合",
      "future_work": "探索更高效架构+3D感知表示"
    },
    {
      "title": "RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE",
      "authors": "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis et al.",
      "year": 2023,
      "architecture": "端到端VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "FiLM-EfficientNet视觉编码+TokenLearner压缩",
      "contribution": "提出高效Transformer架构RT-1实现大规模机器人控制",
      "performance": "97%训练任务成功率+76%新任务泛化能力",
      "limitations": "无法处理全新动作指令+模仿学习性能上限",
      "future_work": "扩展指令集+提升环境多样性"
    },
    {
      "title": "HAMSTER: HIERARCHICAL ACTION MODELS FOR OPEN-WORLD ROBOT MANIPULATION",
      "authors": "Joel Jang, Marius Memmel, Raymond Yu, Caelan Garrett, Fabio Ramos et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "新型分层架构+跨模态数据训练范式",
      "contribution": "利用分层架构和2D路径表征实现跨领域泛化",
      "performance": "比OpenVLA提升20%成功率",
      "limitations": "仅支持2D预测/无法实时调整轨迹",
      "future_work": "研究可学习中间表征/利用人类视频数据"
    },
    {
      "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning",
      "authors": "Understanding, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "联合嵌入预测架构+表示空间规划",
      "contribution": "提出两阶段训练方法(自监督视频预训练+小规模动作条件微调)",
      "performance": "SSv2正确率77.3+EK100预测提升44%+80%抓取成功率",
      "limitations": "长时程预测能力不足+语言目标支持有限",
      "future_work": "分层抽象+语言目标支持+更大规模模型"
    },
    {
      "title": "GeoManip: Geometric Constraints as General Interfaces for Robot Manipulation",
      "authors": "Weiliang Tang, Jia-Hui Pan, Yun-Hui Liu, Masayoshi Tomizuka, Li Erran Li et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "提出几何约束作为通用中间表示",
      "contribution": "利用几何约束作为通用接口连接语言指令与机器人动作",
      "performance": "71.1%成功率(Meta-World),60%成功率(OmniGibson),65%成功率(真实环境)",
      "limitations": "依赖预训练大模型、几何约束生成可能出错",
      "future_work": "改进几何约束生成模块、扩展到更复杂任务"
    },
    {
      "title": "DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping",
      "authors": "Yifan Zhong, Xuchuan Huang, Ruochong Li, Ceyao Zhang, Yitao Liang et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "利用基础模型转换输入为领域不变表征实现高效模仿学习",
      "contribution": "基于预训练VLMs的分层框架实现通用灵巧抓取",
      "performance": "90.8%零样本成功率",
      "limitations": "未集成触觉反馈，后续操作能力有限",
      "future_work": "扩展触觉反馈与任务导向控制器"
    },
    {
      "title": "SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation",
      "authors": "Jingkai Xu, Xiangli Nie",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "动态技能编码本设计",
      "contribution": "基于技能提示和分层策略的持续模仿学习框架",
      "performance": "LIBERO基准FWT提升17%, AUC提升27%",
      "limitations": "依赖高质量演示数据",
      "future_work": "扩展至真实机器人部署"
    },
    {
      "title": "OPENHELIX: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation",
      "authors": "Can Cui, Pengxiang Ding, Wenxuan Song, Shuanghao Bai, Xinyang Tong et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "双系统异步集成方法+辅助任务设计",
      "contribution": "开源双系统VLA框架,结合LLaVA-7B和3D Diffusion Actor",
      "performance": "93.3%连续任务完成率(5任务)",
      "limitations": "动态场景适应性不足",
      "future_work": "部署人形机器人+真实世界验证"
    },
    {
      "title": "A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation",
      "authors": "Rongtao Xu, Jian Zhang, Minghao Guo, Youpeng Wen, Haoting Yang et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "融合扩散模型与物体中心空间表征",
      "contribution": "提出了面向机器人操作的具身无关层次化模型，结合扩散模型预测接触点轨迹",
      "performance": "Franka:62.5%成功率, Kinova:53.75%",
      "limitations": "依赖深度传感器/VLM辅助",
      "future_work": "改进抓取姿态估计/高度预测"
    },
    {
      "title": "VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation",
      "authors": "Boyang Sun, Anran Zhang, Marc Pollefeys, Stefan Leutenegger",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "从单目RGB视频重建3D交互轨迹+测试时优化",
      "contribution": "从野外2D人类视频学习可泛化的3D操控动作",
      "performance": "88.2%平均成功率(零样本)",
      "limitations": "依赖深度估计和SfM质量",
      "future_work": "探索穿戴式设备多模态数据"
    },
    {
      "title": "ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow",
      "authors": "Flow Changhe Chen, Quantao Yang, Xiaohao Xu, Nima Fazeli, Olov Andersson",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "未提及",
      "contribution": "提出语义动作流作为中间表示实现跨域迁移",
      "performance": "低数据下性能超越100%数据方法",
      "limitations": "扩展多模态输入",
      "future_work": "语义动作流表征学习"
    },
    {
      "title": "Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning",
      "authors": "Hao Chen, Jiaming Liu, Chenyang Gu, Zhuoyang Liu, Renrui Zhang et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "整合双系统于统一VLA模型",
      "contribution": "嵌入System 1执行模块到VLM-based System 2实现双系统协调",
      "performance": "69%(仿真),68-74%(真实世界)",
      "limitations": "静态配置协作频率",
      "future_work": "动态适应频率调整"
    },
    {
      "title": "Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents",
      "authors": "Embodied Agents, Zhejian Yang, Yongchao Chen, Xueyang Zhou, Jiangyue Yan et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首个将SOP概念引入机器人协调协议",
      "contribution": "标准化动作流程(SAP)协调三大组件模块化架构",
      "performance": "79.6%平均成功率(LIBERO基准)",
      "limitations": "物理平台部署适应性未验证,Moka-Moka任务空间协调不足",
      "future_work": "自适应验证机制优化,复杂恢复策略开发"
    },
    {
      "title": "MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation",
      "authors": "Zhenyu Wu, Yuheng Zhou, Xiuwei Xu, Ziwei Wang, Haibin Yan",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "固定基座VLA模型向移动操作的高效迁移",
      "contribution": "将固定基座VLA模型迁移至移动操作任务",
      "performance": "模拟环境成功率66.1% 比SOTA高4.2%",
      "limitations": "部署空间受限+缺乏长时任务规划",
      "future_work": "集成任务规划模块+扩展运动空间"
    },
    {
      "title": "MinD: Unified Visual Imagination and Control via Hierarchical World Model",
      "authors": "Xiaowei Chi, Kuangzhi Ge, Jiaming Liu, Siyuan Zhou, Peidong Jia et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "双噪声调度对齐机制",
      "contribution": "双系统扩散模型架构(LoDiff可视化+HiDiff策略)完成视觉模拟与实时控制",
      "performance": "63.0%成功率(RL-Bench)",
      "limitations": "依赖领域数据+泛化性受限",
      "future_work": "增强数据多样性+多模态预训练"
    },
    {
      "title": "AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems",
      "authors": "未提及",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "首次实现潜在动作规划器与VLM的层级融合",
      "contribution": "提出ViLLA框架(视觉-语言-潜在动作)和三阶段训练范式",
      "performance": "复杂任务成功率60%+超出RDT方法32%",
      "limitations": "需人工验证数据质量+部署大规模机器人成本高",
      "future_work": "扩展跨具身数据共享和持续学习"
    },
    {
      "title": "GEVRM: GOAL-EXPRESSIVE VIDEO GENERATION MODEL FOR ROBUST VISUAL MANIPULATION",
      "authors": "Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, Donglin Wang",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首次将IMC控制原理实例化到VLA框架",
      "contribution": "将经典IMC控制原理与VLA融合增强鲁棒性",
      "performance": "CALVIN基准SOTA",
      "limitations": "计算开销大/依赖预训练模型",
      "future_work": "更通用的视频生成模型融入VLA"
    },
    {
      "title": "GEVRM: GOAL-EXPRESSIVE VIDEO GENERATION MODEL FOR ROBUST VISUAL MANIPULATION",
      "authors": "Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, Donglin Wang",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "将经典IMC控制原理融入现代VLA框架",
      "contribution": "整合IMC原理增强VLA鲁棒性,提出目标表达视频生成模型",
      "performance": "CALVIN基准SOTA",
      "limitations": "真实场景部署成功率待提升",
      "future_work": "提升真实场景感知与执行精度"
    },
    {
      "title": "LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction",
      "authors": "未提及",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "结构化潜空间/CVAE架构",
      "contribution": "首个结合Vision-Language-Action的类人机器人全身控制框架",
      "performance": "58.5%平均成功率",
      "limitations": "长时规划缺失/快速视觉反馈不足",
      "future_work": "强化学习微调/扩展动作词汇"
    },
    {
      "title": "ROBOGROUND: Robotic Manipulation with Grounded Vision-Language Priors",
      "authors": "Haifeng Huang, Xinyi Chen, Yilun Chen, Hao Li, Xiaoshen Han et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "中间表示创新+自动化数据管道",
      "contribution": "提出基于grounding masks的中间表示增强策略泛化能力",
      "performance": "成功率提升30-100%(随任务类型变化)",
      "limitations": "抓取成功率不足(接触率89% vs 成功率43%)",
      "future_work": "集成抓取姿态估计网络+端到端架构探索"
    },
    {
      "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model",
      "authors": "Haoming Song, Delin Qu, Yuanqi Yao, Qizhi Chen, Qi Lv et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "双系统动作降噪与价值引导思考的创新设计",
      "contribution": "双系统架构引入System-2思维进行机器人控制",
      "performance": "LIBERO基准SOTA(98.6%成功率)",
      "limitations": "未提及价值函数与语义的对齐问题",
      "future_work": "优化采样策略提高高价值候选动作比例"
    },
    {
      "title": "UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation",
      "authors": "Oleg Sautenkov,Yasheerah Yaqoot,Artem Lykov,Muhammad Ahsan Mustafa,Grik Tadevosyan,Aibek Akhmetkazy,Miguel Altamirano Cabrera,Mikhail Martynov,Sausar Karaf,Dzmitry Tsetserukou",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "首个卫星图像+VLM+LLM的无人机任务规划系统",
      "contribution": "集成卫星图像处理+VLM+GPT实现无人机自然语言任务规划",
      "performance": "轨迹误差34.22m(KNN),速度快6.5倍人工",
      "limitations": "卫星图像分辨率限制(1.5米/像素)",
      "future_work": "构建专用数据集+开发端到端模型"
    },
    {
      "title": "Integrating LMM Planners and 3D Skill Policies for Generalizable Manipulation",
      "authors": "Yuelei Li, Ge Yan, Annabella Macaluso, Mazeyu Ji, Xueyan Zou et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "未提及",
      "contribution": "融合LMM规划器与3D语义感知策略",
      "performance": "few-shot数据效率",
      "limitations": "扩展技能库+泛化新物品",
      "future_work": "3D特征场与语言嵌入联合注意力"
    },
    {
      "title": "Pixel Motion as Universal Representation for Robot Control LangToMo",
      "authors": "Xiang Li, Cristina Mata, Jongwoo Park",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "dual-system架构与像素运动表示",
      "contribution": "使用像素运动作为中间表示的VLA框架",
      "performance": "MetaWorld 53.6%成功率，真实世界68.8%成功率",
      "limitations": "Diffusion模型推理成本高、缺乏深度信息",
      "future_work": "3D运动表示、计算优化"
    },
    {
      "title": "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs",
      "authors": "Artem Lykov, Valerii Serpiva, Muhammad Haris Khan, Oleg Sautenkov, Artyom Myshlyaev et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首个无人机认知VLA基准测试",
      "contribution": "无人机认知任务VLA模型与首个专用基准测试",
      "performance": "基础模型59.6%成功率/R1变体77.2%",
      "limitations": "双大模型导致高内存占用(20GB)",
      "future_work": "优化模型压缩技术"
    },
    {
      "title": "RationalVLA: A Rational Vision-Language-Action Model with Dual System",
      "authors": "未提及",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "引入<ACT>与<REJ>标记的混合架构",
      "contribution": "提出双系统VLA架构(RationalVLA)与RAMA新基准",
      "performance": "RationalVLA相比基线成功率提升14.5%",
      "limitations": "依赖大型预训练模型/实时性未验证",
      "future_work": "扩展到多模态输入/优化推理效率"
    },
    {
      "title": "Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models",
      "authors": "Lucy Xiaoyang, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "结合VLM的原生语义理解与机器人控制",
      "contribution": "使用分层VLA架构实现开放指令跟随与实时反馈处理",
      "performance": "指令准确率显著高于GPT-4o和平面VLA",
      "limitations": "依赖Prompt engineering生成合成数据",
      "future_work": "统一高低层模型+自适应推理频率"
    },
    {
      "title": "π0.5: a Vision-Language-Action Model with Open-World Generalization",
      "authors": "Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail et al.",
      "year": 2025,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "离散+连续动作表示融合",
      "contribution": "利用跨模态协同训练实现开放世界泛化",
      "performance": "mock环境80%成功率",
      "limitations": "对部分场景(如遮挡)处理不足",
      "future_work": "增强记忆机制处理部分可观测性"
    },
    {
      "title": "RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation",
      "authors": "Soroush Nasiriany, Sean Kirmani, Tianli Ding, Yuke Zhu, Danny Driess et al.",
      "year": 2024,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "否",
      "innovation": "通过affordance桥接网络数据与机器人控制",
      "contribution": "引入affordance作为中间表征连接视觉语言与动作",
      "performance": "比RT-2提升50%成功率",
      "limitations": "无法泛化到全新运动技能",
      "future_work": "探索多表征融合的VLA模型"
    },
    {
      "title": "HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers",
      "authors": "Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi et al.",
      "year": 2024,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "VLM异步条件化机制",
      "contribution": "提出层级式VLM-策略架构解决大模型延迟问题",
      "performance": "仿真任务成功率80.8%,真实动态任务75%",
      "limitations": "未测试高速动态场景",
      "future_work": "适应更复杂动态任务"
    },
    {
      "title": "RT-H: Action Hierarchies Using Language",
      "authors": "Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quan Vuong, Jonathan Tompson, Yevgen Chebotar∗, Debidatta Dwibedi∗, Dorsa Sadigh∗",
      "year": 2024,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "语言动作作为中间表示层的创新设计",
      "contribution": "使用语言动作层级提升多任务数据共享效率",
      "performance": "平均任务成功率提升15%",
      "limitations": "绝对成功率仍有提升空间",
      "future_work": "扩展离线数据集和修正流程"
    },
    {
      "title": "Bi-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Dexterous Manipulations",
      "authors": "Artem Lykov",
      "year": 2024,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "结合RAG系统实现知识增强",
      "contribution": "整合视觉-语言-动作模块实现双手机器人精准操作",
      "performance": "场景1正确率83.4%/场景2正确率71.22%",
      "limitations": "视觉模块在复杂场景性能下降",
      "future_work": "提升视觉模块鲁棒性"
    },
    {
      "title": "Yell At Your Robot Improving On-the-Fly from Language Corrections",
      "authors": "Lucy Xiaoyang, Zheyuan Hu, Archit Sharma, Karl Pertsch, Jianlan Luo et al.",
      "year": 2024,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "未提及",
      "innovation": "基于语言的分层策略持续学习",
      "contribution": "利用人类语言纠正实时优化高层策略",
      "performance": "20%成功率提升",
      "limitations": "依赖高质量低层策略,数据收集耗时",
      "future_work": "结合预训练VLM进行高层规划"
    },
    {
      "title": "General Flow as Foundation Affordance for Scalable Robot Learning",
      "authors": "Chuan Wen, Tong Zhang, Yang Gao",
      "year": 2024,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "未提及",
      "contribution": "提出3D流预测作为跨体现可迁移的affordance表示",
      "performance": "未提及",
      "limitations": "扩大数据集规模、用Diffusion替代VAE、自动化抓取策略",
      "future_work": "首次从大规模人类视频直接学习3D流预测并实现zero-shot迁移"
    },
    {
      "title": "Any-point Trajectory Modeling for Policy Learning",
      "authors": "Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou et al.",
      "year": 2024,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "轨迹结构化表示替代视频生成",
      "contribution": "利用无动作视频预训练任意点轨迹模型指导策略学习",
      "performance": "LIBERO基准80%提升",
      "limitations": "依赖标注演示轨迹、视频领域受限",
      "future_work": "无监督强化学习扩展、野外视频适应"
    },
    {
      "title": "Learning Universal Policies via Text-Guided Video Generation",
      "authors": "Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum",
      "year": 2023,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "将视频生成作为通用策略表示",
      "contribution": "将策略表示为文本条件视频生成模型，实现跨任务泛化",
      "performance": "59.1%任务完成率",
      "limitations": "生成速度慢",
      "future_work": "视频模型蒸馏加速"
    },
    {
      "title": "Compositional Foundation Models for Hierarchical Planning",
      "authors": "未提及",
      "year": 2023,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "利用现有多模态基础模型组合实现分层规划",
      "contribution": "组合独立训练的视觉/语言/动作模型进行分层规划",
      "performance": "74.3%/72.8%/85.8%任务成功率",
      "limitations": "依赖仿真数据，视频生成计算成本高",
      "future_work": "开发更大规模视频基础模型"
    },
    {
      "title": "LEARNING TO ACT FROM ACTIONLESS VIDEOS THROUGH DENSE CORRESPONDENCES",
      "authors": "Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, Joshua B. Tenenbaum",
      "year": 2023,
      "architecture": "分层式VLA",
      "data_bottleneck": "是",
      "compute_bottleneck": "是",
      "innovation": "视频扩散模型+稠密对应动作推理",
      "contribution": "通过稠密对应实现无动作标注的视频策略学习",
      "performance": "43.1%成功率(Meta-World),31.3%成功率(iTHOR)",
      "limitations": "光照敏感+小物体跟踪误差",
      "future_work": "稠密流模型优化+力觉信息融合"
    },
    {
      "title": "ZERO-SHOT ROBOTIC MANIPULATION WITH PRETRAINED IMAGE-EDITING DIFFUSION MODELS",
      "authors": "Kevin Black∗1,Mitsuhiko Nakamoto∗1,Pranav Atreya1,Homer Walke1,Chelsea Finn2,3,Aviral Kumar1,3,Sergey Levine1,3",
      "year": 2023,
      "architecture": "分层式VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "首次将图像编辑扩散模型用于机器人子目标生成",
      "contribution": "利用预训练图像编辑扩散模型生成子目标指导低级策略",
      "performance": "CALVIN SOTA 87%成功率;真实世界88%成功率",
      "limitations": "1. 依赖预训练模型 2. 实时性受限",
      "future_work": "1. 扩展多模态输入 2. 优化推理速度"
    },
    {
      "title": "RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon Robotic Manipulation",
      "authors": "Jing Huo",
      "year": 2025,
      "architecture": "Hierarchical VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "是",
      "innovation": "Integration of LLMs with multi-view world models",
      "contribution": "LLM-assisted reward generation + multi-view keyframe representation learning for long-horizon tasks",
      "performance": "23.35%-29.23% success rate improvement",
      "limitations": "Requires GPT-4o for reliable reward generation",
      "future_work": "Incorporating human feedback for reward refinement"
    },
    {
      "title": "CoinFT: A Coin-Sized, Capacitive 6-Axis Force Torque Sensor for Robotic Applications",
      "authors": "Jun En Low, Tae Myung Huh, Gabriela A. Uribe, Seongheon Hong, Julia Di et al.",
      "year": 2025,
      "architecture": "非VLA",
      "data_bottleneck": "否",
      "compute_bottleneck": "否",
      "innovation": "基于PCB的电容传感创新设计",
      "contribution": "开发微型电容式六维力传感器",
      "performance": "平均MSE: 0.11N(力)/0.84mNm(力矩)",
      "limitations": "带宽较低(97Hz)/长期蠕变问题",
      "future_work": "定制化传感器参数"
    }
  ],
  "Resource Bottleneck Analysis": [],
  "Solution Strategies and Innovations": [],
  "Performance Analysis and Benchmarking": [],
  "Future Directions and Open Challenges": []
}