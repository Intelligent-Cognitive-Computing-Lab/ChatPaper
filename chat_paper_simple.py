import argparse
import configparser
import datetime
import os
import re
import openai
import tiktoken
import fitz  # PyMuPDF
import json
import hashlib
import threading
import queue
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from paper_info_extractor import PaperInfoExtractor, enhance_paper_with_local_info


class Paper:
    def __init__(self, path, title=''):
        """åˆå§‹åŒ–Paperå¯¹è±¡ï¼Œåªå¤„ç†æœ¬åœ°PDFæ–‡ä»¶"""
        self.path = path
        self.section_names = []
        self.section_texts = {}
        self.title_page = 0
        
        # è§£æPDF
        self.pdf = fitz.open(path)
        self.title = self.get_title() if title == '' else title
        self.parse_pdf()
        
    def parse_pdf(self):
        """è§£æPDFå†…å®¹"""
        self.text_list = [page.get_text() for page in self.pdf]
        self.all_text = ' '.join(self.text_list)
        self.section_page_dict = self._get_all_page_index()
        print("section_page_dict", self.section_page_dict)
        self.section_text_dict = self._get_all_page()
        self.section_text_dict.update({"title": self.title})
        self.section_text_dict.update({"paper_info": self.get_paper_info()})
        self.pdf.close()
        
    def get_paper_info(self):
        """è·å–è®ºæ–‡åŸºæœ¬ä¿¡æ¯"""
        first_page_text = self.pdf[self.title_page].get_text()
        if "Abstract" in self.section_text_dict.keys():
            abstract_text = self.section_text_dict['Abstract']
        else:
            abstract_text = ""
        first_page_text = first_page_text.replace(abstract_text, "")
        return first_page_text
        
    def get_title(self):
        """ä»PDFä¸­æå–æ ‡é¢˜"""
        doc = self.pdf
        max_font_size = 0
        max_string = ""
        max_font_sizes = [0]
        
        for page_index, page in enumerate(doc):
            text = page.get_text("dict")
            blocks = text["blocks"]
            for block in blocks:
                if block["type"] == 0 and len(block['lines']):
                    if len(block["lines"][0]["spans"]):
                        font_size = block["lines"][0]["spans"][0]["size"]
                        max_font_sizes.append(font_size)
                        if font_size > max_font_size:
                            max_font_size = font_size
                            max_string = block["lines"][0]["spans"][0]["text"]
        
        max_font_sizes.sort()
        print("max_font_sizes", max_font_sizes[-10:])
        
        cur_title = ''
        for page_index, page in enumerate(doc):
            text = page.get_text("dict")
            blocks = text["blocks"]
            for block in blocks:
                if block["type"] == 0 and len(block['lines']):
                    if len(block["lines"][0]["spans"]):
                        cur_string = block["lines"][0]["spans"][0]["text"]
                        font_size = block["lines"][0]["spans"][0]["size"]
                        
                        if abs(font_size - max_font_sizes[-1]) < 0.3 or abs(font_size - max_font_sizes[-2]) < 0.3:
                            if len(cur_string) > 4 and "arXiv" not in cur_string:
                                if cur_title == '':
                                    cur_title += cur_string
                                else:
                                    cur_title += ' ' + cur_string
                            self.title_page = page_index
        
        title = cur_title.replace('\n', ' ')
        return title

    def _get_all_page_index(self):
        """è·å–å„ç« èŠ‚åœ¨PDFä¸­çš„é¡µç """
        section_list = ["Abstract", 
                        'Introduction', 'Related Work', 'Background', 
                        "Preliminary", "Problem Formulation",
                        'Methods', 'Methodology', "Method", 'Approach', 'Approaches',
                        "Materials and Methods", "Experiment Settings",
                        'Experiment', "Experimental Results", "Evaluation", "Experiments",                        
                        "Results", 'Findings', 'Data Analysis',                                                                        
                        "Discussion", "Results and Discussion", "Conclusion",
                        'References']
        
        section_page_dict = {}
        for page_index, page in enumerate(self.pdf):
            cur_text = page.get_text()
            for section_name in section_list:
                section_name_upper = section_name.upper()
                if "Abstract" == section_name and section_name in cur_text:
                    section_page_dict[section_name] = page_index
                else:
                    if section_name + '\n' in cur_text:
                        section_page_dict[section_name] = page_index
                    elif section_name_upper + '\n' in cur_text:
                        section_page_dict[section_name] = page_index
        
        return section_page_dict

    def _get_all_page(self):
        """è·å–å„ç« èŠ‚çš„æ–‡æœ¬å†…å®¹"""
        section_dict = {}
        text_list = [page.get_text() for page in self.pdf]
        
        for sec_index, sec_name in enumerate(self.section_page_dict):
            print(sec_index, sec_name, self.section_page_dict[sec_name])
            
            start_page = self.section_page_dict[sec_name]
            if sec_index < len(list(self.section_page_dict.keys()))-1:
                end_page = self.section_page_dict[list(self.section_page_dict.keys())[sec_index+1]]
            else:
                end_page = len(text_list)
            
            print("start_page, end_page:", start_page, end_page)
            cur_sec_text = ''
            
            if end_page - start_page == 0:
                if sec_index < len(list(self.section_page_dict.keys()))-1:
                    next_sec = list(self.section_page_dict.keys())[sec_index+1]
                    if text_list[start_page].find(sec_name) == -1:
                        start_i = text_list[start_page].find(sec_name.upper())
                    else:
                        start_i = text_list[start_page].find(sec_name)
                    if text_list[start_page].find(next_sec) == -1:
                        end_i = text_list[start_page].find(next_sec.upper())
                    else:
                        end_i = text_list[start_page].find(next_sec)                        
                    cur_sec_text += text_list[start_page][start_i:end_i]
            else:
                for page_i in range(start_page, end_page):                    
                    if page_i == start_page:
                        if text_list[start_page].find(sec_name) == -1:
                            start_i = text_list[start_page].find(sec_name.upper())
                        else:
                            start_i = text_list[start_page].find(sec_name)
                        cur_sec_text += text_list[page_i][start_i:]
                    elif page_i < end_page:
                        cur_sec_text += text_list[page_i]
                    elif page_i == end_page:
                        if sec_index < len(list(self.section_page_dict.keys()))-1:
                            next_sec = list(self.section_page_dict.keys())[sec_index+1]
                            if text_list[start_page].find(next_sec) == -1:
                                end_i = text_list[start_page].find(next_sec.upper())
                            else:
                                end_i = text_list[start_page].find(next_sec)  
                            cur_sec_text += text_list[page_i][:end_i]
            
            section_dict[sec_name] = cur_sec_text.replace('-\n', '').replace('\n', ' ')
        
        return section_dict


class Reader:
    def __init__(self, key_word, args=None):
        """åˆå§‹åŒ–Readerï¼Œåªç”¨äºæœ¬åœ°PDFæ€»ç»“"""
        self.key_word = key_word
        self.language = 'Chinese' if args and args.language == 'zh' else 'English'
        
        # è¯»å–é…ç½®æ–‡ä»¶
        self.config = configparser.ConfigParser()
        self.config.read('apikey.ini')
        
        # OpenAIé…ç½®
        OPENAI_KEY = os.environ.get("OPENAI_KEY", "")
        openai.api_base = self.config.get('OpenAI', 'OPENAI_API_BASE')
        self.chat_api_list = self.config.get('OpenAI', 'OPENAI_API_KEYS')[1:-1].replace('\'', '').split(',')
        self.chat_api_list.append(OPENAI_KEY)
        self.chat_api_list = [api.strip() for api in self.chat_api_list if len(api) > 20]
        self.chatgpt_model = self.config.get('OpenAI', 'CHATGPT_MODEL')

        # å¦‚æœæ²¡æœ‰OpenAI keyï¼Œä½¿ç”¨Azure
        if not self.chat_api_list:
            self.chat_api_list.append(self.config.get('AzureOPenAI', 'OPENAI_API_KEYS'))
            self.chatgpt_model = self.config.get('AzureOPenAI', 'CHATGPT_MODEL')
            openai.api_base = self.config.get('AzureOPenAI', 'OPENAI_API_BASE')
            openai.api_type = 'azure'
            openai.api_version = self.config.get('AzureOPenAI', 'OPENAI_API_VERSION')

        self.cur_api = 0
        self.file_format = args.file_format if args else 'md'
        self.max_token_num = args.max_tokens if args else 60000  # ä¿®æ”¹é»˜è®¤å€¼ä»¥å……åˆ†åˆ©ç”¨64kä¸Šä¸‹æ–‡
        self.encoding = tiktoken.get_encoding("gpt2")
        self.token_manager = TokenManager(max_token_num=self.max_token_num, model=self.chatgpt_model)
        
        # è¿›åº¦è·Ÿè¸ª
        self.progress_file = 'processing_progress.json'
        self.export_path = os.path.join('./', 'export')
        if not os.path.exists(self.export_path):
            os.makedirs(self.export_path)
        
        # CSVåˆå¹¶ç›¸å…³
        self.merged_csv_file = None
        self.csv_header_written = False
        self.csv_lock = threading.Lock()  # æ·»åŠ é”ä¿æŠ¤CSVå†™å…¥

        # æ·»åŠ æœ¬åœ°ä¿¡æ¯æå–å™¨
        self.info_extractor = PaperInfoExtractor()

    def get_paper_hash(self, paper_path):
        """ç”Ÿæˆè®ºæ–‡æ–‡ä»¶çš„å”¯ä¸€æ ‡è¯†"""
        with open(paper_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        return file_hash

    def load_progress(self):
        """åŠ è½½å¤„ç†è¿›åº¦"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰åˆå¹¶æ–‡ä»¶ï¼Œå¦‚æœæœ‰åˆ™æ ‡è®°è¡¨å¤´å·²å†™å…¥
                if os.path.exists(self.merged_csv_file if hasattr(self, 'merged_csv_file') else ''):
                    self.csv_header_written = True
                    print("ğŸ“‹ æ£€æµ‹åˆ°ç°æœ‰åˆå¹¶æ–‡ä»¶ï¼Œå°†ç»§ç»­è¿½åŠ æ•°æ®")
                return progress
            except:
                return {}
        return {}

    def save_progress(self, progress):
        """ä¿å­˜å¤„ç†è¿›åº¦"""
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)

    def is_paper_processed(self, paper_path, progress):
        """æ£€æŸ¥è®ºæ–‡æ˜¯å¦å·²ç»å¤„ç†è¿‡"""
        paper_hash = self.get_paper_hash(paper_path)
        return paper_hash in progress and progress[paper_hash].get('status') == 'completed'

    def mark_paper_completed(self, paper_path, output_file, progress):
        """æ ‡è®°è®ºæ–‡å¤„ç†å®Œæˆ"""
        paper_hash = self.get_paper_hash(paper_path)
        progress[paper_hash] = {
            'status': 'completed',
            'paper_path': paper_path,
            'output_file': output_file,
            'completed_time': str(datetime.datetime.now())
        }
        self.save_progress(progress)

    def validateTitle(self, title):
        """ä¿®æ­£æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
        rstr = r"[\/\\\:\*\?\"\<\>\|]"
        new_title = re.sub(rstr, "_", title)
        return new_title

    def summary_with_chat(self, pdf_paths, truncation_strategy="balanced"):
        """é€ç¯‡é¢„å¤„ç†+æ€»ç»“è®ºæ–‡ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ """
        print(f"\n=== å¼€å§‹å¤„ç†PDFæ–‡ä»¶ï¼Œæ€»å…± {len(pdf_paths)} ä¸ªæ–‡ä»¶ï¼Œç­–ç•¥: {truncation_strategy} ===")
        
        # åˆå§‹åŒ–åˆå¹¶çš„CSVæ–‡ä»¶
        date_str = str(datetime.datetime.now())[:13].replace(' ', '-')
        self.merged_csv_file = os.path.join(self.export_path, f"{date_str}-merged_papers.csv")
        
        # åŠ è½½è¿›åº¦ï¼ˆä¼šæ£€æŸ¥ç°æœ‰æ–‡ä»¶ï¼‰
        progress = self.load_progress()
        
        # æ£€æŸ¥åˆå¹¶æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰å†…å®¹
        if os.path.exists(self.merged_csv_file):
            with open(self.merged_csv_file, 'r', encoding='utf-8') as f:
                existing_content = f.read().strip()
                if existing_content:
                    self.csv_header_written = True
                    print(f"ğŸ“„ æ£€æµ‹åˆ°ç°æœ‰åˆå¹¶æ–‡ä»¶ï¼Œå°†ç»§ç»­è¿½åŠ : {self.merged_csv_file}")
                else:
                    self.csv_header_written = False
                    print(f"ğŸ“„ åˆ›å»ºæ–°çš„åˆå¹¶æ–‡ä»¶: {self.merged_csv_file}")
        else:
            self.csv_header_written = False
            print(f"ğŸ“„ åˆ›å»ºæ–°çš„åˆå¹¶æ–‡ä»¶: {self.merged_csv_file}")
        
        processed_count = 0
        skipped_count = 0
        failed_count = 0
        
        for pdf_index, pdf_path in enumerate(pdf_paths):
            print(f"\n--- å¤„ç†ç¬¬ {pdf_index + 1}/{len(pdf_paths)} ä¸ªPDFæ–‡ä»¶ ---")
            print(f"æ–‡ä»¶è·¯å¾„: {pdf_path}")
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
            if self.is_paper_processed(pdf_path, progress):
                print(f"âœ… PDFå·²å¤„ç†è¿‡ï¼Œè·³è¿‡")
                skipped_count += 1
                continue
            
            try:
                # é€ç¯‡é¢„å¤„ç†+æ€»ç»“
                success = self.process_single_pdf_complete(pdf_path, truncation_strategy, progress)
                if success:
                    processed_count += 1
                    print(f"âœ… ç¬¬ {pdf_index + 1} ä¸ªPDFå¤„ç†å®Œæˆ")
                else:
                    failed_count += 1
                    print(f"âŒ ç¬¬ {pdf_index + 1} ä¸ªPDFå¤„ç†å¤±è´¥")
                    
            except KeyboardInterrupt:
                print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
                print(f"å·²å¤„ç†: {processed_count} ç¯‡ï¼Œè·³è¿‡: {skipped_count} ç¯‡ï¼Œå¤±è´¥: {failed_count} ç¯‡")
                print(f"è¿›åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†ä»ä¸­æ–­å¤„ç»§ç»­")
                print(f"ğŸ“„ å½“å‰ç»“æœå·²ä¿å­˜åˆ°: {self.merged_csv_file}")
                return
            except Exception as e:
                failed_count += 1
                print(f"âŒ å¤„ç†PDFæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue
        
        print(f"\n=== æ‰€æœ‰PDFå¤„ç†å®Œæˆ ===")
        print(f"æ–°å¤„ç†: {processed_count} ç¯‡ï¼Œè·³è¿‡: {skipped_count} ç¯‡ï¼Œå¤±è´¥: {failed_count} ç¯‡ï¼Œæ€»è®¡: {len(pdf_paths)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“„ åˆå¹¶ç»“æœå·²ä¿å­˜åˆ°: {self.merged_csv_file}")

    def summary_with_chat_parallel(self, pdf_paths, truncation_strategy="balanced", max_workers=3):
        """å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†PDFæ–‡ä»¶"""
        print(f"\n=== å¼€å§‹å¹¶è¡Œå¤„ç†PDFæ–‡ä»¶ï¼Œæ€»å…± {len(pdf_paths)} ä¸ªæ–‡ä»¶ï¼Œç­–ç•¥: {truncation_strategy}ï¼Œçº¿ç¨‹æ•°: {max_workers} ===")
        
        # åˆå§‹åŒ–åˆå¹¶çš„CSVæ–‡ä»¶
        date_str = str(datetime.datetime.now())[:13].replace(' ', '-')
        self.merged_csv_file = os.path.join(self.export_path, f"{date_str}-merged_papers.csv")
        
        # åŠ è½½è¿›åº¦ï¼ˆä¼šæ£€æŸ¥ç°æœ‰æ–‡ä»¶ï¼‰
        progress = self.load_progress()
        
        # æ£€æŸ¥åˆå¹¶æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰å†…å®¹
        if os.path.exists(self.merged_csv_file):
            with open(self.merged_csv_file, 'r', encoding='utf-8') as f:
                existing_content = f.read().strip()
                if existing_content:
                    self.csv_header_written = True
                    print(f"ğŸ“„ æ£€æµ‹åˆ°ç°æœ‰åˆå¹¶æ–‡ä»¶ï¼Œå°†ç»§ç»­è¿½åŠ : {self.merged_csv_file}")
                else:
                    self.csv_header_written = False
                    print(f"ğŸ“„ åˆ›å»ºæ–°çš„åˆå¹¶æ–‡ä»¶: {self.merged_csv_file}")
        else:
            self.csv_header_written = False
            print(f"ğŸ“„ åˆ›å»ºæ–°çš„åˆå¹¶æ–‡ä»¶: {self.merged_csv_file}")
        
        # è¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„PDF
        pdf_to_process = []
        skipped_count = 0
        
        for pdf_path in pdf_paths:
            if self.is_paper_processed(pdf_path, progress):
                print(f"âœ… {os.path.basename(pdf_path)} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡")
                skipped_count += 1
            else:
                pdf_to_process.append(pdf_path)
        
        if not pdf_to_process:
            print("æ‰€æœ‰PDFéƒ½å·²å¤„ç†å®Œæˆ")
            return
        
        print(f"éœ€è¦å¤„ç†çš„PDF: {len(pdf_to_process)} ä¸ªï¼Œå·²è·³è¿‡: {skipped_count} ä¸ª")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        processed_count = 0
        failed_count = 0
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_pdf = {
                executor.submit(self.process_single_pdf_complete_thread_safe, pdf_path, truncation_strategy, progress): pdf_path 
                for pdf_path in pdf_to_process
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_pdf):
                pdf_path = future_to_pdf[future]
                pdf_name = os.path.basename(pdf_path)
                
                try:
                    success = future.result()
                    if success:
                        processed_count += 1
                        elapsed = time.time() - start_time
                        avg_time = elapsed / processed_count if processed_count > 0 else 0
                        remaining = len(pdf_to_process) - processed_count - failed_count
                        est_remaining = avg_time * remaining
                        
                        print(f"âœ… [{processed_count + failed_count}/{len(pdf_to_process)}] {pdf_name} å¤„ç†å®Œæˆ")
                        print(f"   â±ï¸  å·²ç”¨æ—¶: {elapsed:.1f}s, å¹³å‡: {avg_time:.1f}s/ç¯‡, é¢„è®¡å‰©ä½™: {est_remaining:.1f}s")
                    else:
                        failed_count += 1
                        print(f"âŒ [{processed_count + failed_count}/{len(pdf_to_process)}] {pdf_name} å¤„ç†å¤±è´¥")
                        
                except Exception as e:
                    failed_count += 1
                    print(f"âŒ [{processed_count + failed_count}/{len(pdf_to_process)}] {pdf_name} å¤„ç†å¼‚å¸¸: {e}")
        
        total_time = time.time() - start_time
        print(f"\n=== å¹¶è¡Œå¤„ç†å®Œæˆ ===")
        print(f"æˆåŠŸ: {processed_count} ç¯‡ï¼Œå¤±è´¥: {failed_count} ç¯‡ï¼Œè·³è¿‡: {skipped_count} ç¯‡")
        print(f"æ€»è€—æ—¶: {total_time:.1f}sï¼Œå¹³å‡: {total_time/len(pdf_to_process):.1f}s/ç¯‡")
        print(f"ğŸ“„ åˆå¹¶ç»“æœå·²ä¿å­˜åˆ°: {self.merged_csv_file}")

    def process_single_pdf_complete(self, pdf_path, truncation_strategy, progress):
        """å®Œæ•´å¤„ç†å•ä¸ªPDFï¼šé¢„å¤„ç†+æ€»ç»“"""
        paper = None
        try:
            print(f"ğŸ“– å¼€å§‹è§£æPDF...")
            # é¢„å¤„ç†ï¼šè§£æPDF
            paper = Paper(path=pdf_path)
            print(f"ğŸ“„ PDFè§£æå®Œæˆï¼Œæ ‡é¢˜: {paper.title[:80]}...")
            
            # æ€»ç»“å¤„ç†
            success = self.process_single_paper(paper, truncation_strategy, progress)
            
            # æ¸…ç†å†…å­˜
            if paper and hasattr(paper, 'pdf'):
                try:
                    paper.pdf.close()
                except:
                    pass
            del paper
            
            return success
            
        except Exception as e:
            print(f"âŒ PDFé¢„å¤„ç†å¤±è´¥: {e}")
            # ç¡®ä¿æ¸…ç†èµ„æº
            if paper and hasattr(paper, 'pdf'):
                try:
                    paper.pdf.close()
                except:
                    pass
            return False

    def process_single_pdf_complete_thread_safe(self, pdf_path, truncation_strategy, progress):
        """çº¿ç¨‹å®‰å…¨çš„å•ä¸ªPDFå¤„ç†æ–¹æ³•"""
        paper = None
        try:
            # çº¿ç¨‹IDç”¨äºæ—¥å¿—åŒºåˆ†
            thread_id = threading.current_thread().name
            pdf_name = os.path.basename(pdf_path)
            
            print(f"ğŸ”„ [{thread_id}] å¼€å§‹å¤„ç†: {pdf_name}")
            
            # é¢„å¤„ç†ï¼šè§£æPDF
            paper = Paper(path=pdf_path)
            
            # æ€»ç»“å¤„ç†
            success = self.process_single_paper_thread_safe(paper, truncation_strategy, progress, thread_id)
            
            # æ¸…ç†å†…å­˜
            if paper and hasattr(paper, 'pdf'):
                try:
                    paper.pdf.close()
                except:
                    pass
            del paper
            
            return success
            
        except Exception as e:
            print(f"âŒ [{threading.current_thread().name}] PDFé¢„å¤„ç†å¤±è´¥: {e}")
            # ç¡®ä¿æ¸…ç†èµ„æº
            if paper and hasattr(paper, 'pdf'):
                try:
                    paper.pdf.close()
                except:
                    pass
            return False

    def append_to_merged_csv(self, csv_content, pdf_path):
        """å°†CSVå†…å®¹è¿½åŠ åˆ°åˆå¹¶æ–‡ä»¶ä¸­ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.csv_lock:  # ä½¿ç”¨é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
            try:
                lines = csv_content.strip().split('\n')
                
                # è¿‡æ»¤æ‰æ‰€æœ‰åŒ…å«"è®ºæ–‡æ ‡é¢˜"çš„è¡Œï¼ˆè¡¨å¤´è¡Œï¼‰
                data_lines = []
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('è®ºæ–‡æ ‡é¢˜') and 'è®ºæ–‡æ ‡é¢˜,ä½œè€…,å‘è¡¨å¹´ä»½' not in line:
                        data_lines.append(line)
                
                if not data_lines:
                    print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®è¡Œï¼Œè·³è¿‡")
                    return
                
                # å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆæ•°æ®è¡Œ
                data_line = data_lines[0]
                
                # è·å–æ–‡ä»¶å
                file_name = os.path.basename(pdf_path)
                
                # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å†™å…¥ï¼Œå†™å…¥è¡¨å¤´ï¼ˆåŒ…å«æ–‡ä»¶ååˆ—ï¼‰
                if not self.csv_header_written:
                    header = "æ–‡ä»¶å,è®ºæ–‡æ ‡é¢˜,ä½œè€…,å‘è¡¨å¹´ä»½,æœŸåˆŠä¼šè®®,DOI,arXiv ID,VLAæ¶æ„ç±»å‹,ä¸»è¦è´¡çŒ®/ç›®æ ‡,æ•°æ®ç“¶é¢ˆ,ç®—åŠ›ç“¶é¢ˆ,èµ„æºçº¦æŸç±»å‹,æ•°æ®ç±»å‹,æ•°æ®è§„æ¨¡,æ•°æ®è·å–æ–¹æ³•,æ•°æ®ç“¶é¢ˆè§£å†³ç­–ç•¥,æ¨¡å‹è§„æ¨¡,è®­ç»ƒèµ„æºéœ€æ±‚,æ¨ç†æ•ˆç‡,ç®—åŠ›ç“¶é¢ˆè§£å†³ç­–ç•¥,ä»»åŠ¡ç±»å‹,å®éªŒç¯å¢ƒ,æ€§èƒ½æŒ‡æ ‡,èµ„æº-æ€§èƒ½æƒè¡¡,ä¼˜ç‚¹,ç¼ºç‚¹/å±€é™,æœªæ¥æ–¹å‘,åˆ›æ–°ç‚¹"
                    with open(self.merged_csv_file, 'w', encoding='utf-8') as f:
                        f.write(header + '\n')
                    self.csv_header_written = True
                    print("ğŸ“‹ CSVè¡¨å¤´å·²å†™å…¥ï¼ˆåŒ…å«æ–‡ä»¶åã€æœŸåˆŠä¼šè®®ã€DOIã€arXiv IDåˆ—ï¼‰")
                
                # è¿½åŠ æ•°æ®è¡Œï¼ˆLLMè¾“å‡ºå·²åŒ…å«æ–‡ä»¶åï¼‰
                with open(self.merged_csv_file, 'a', encoding='utf-8') as f:
                    f.write(f'{data_line}\n')
                
                print(f"ğŸ“ æ•°æ®å·²è¿½åŠ åˆ°åˆå¹¶æ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰")
                
            except Exception as e:
                print(f"âŒ è¿½åŠ CSVå†…å®¹å¤±è´¥: {e}")

    def process_single_paper(self, paper, truncation_strategy, progress):
        """å¤„ç†å•ç¯‡å·²è§£æçš„è®ºæ–‡"""
        try:
            # æœ¬åœ°æå–åŸºç¡€ä¿¡æ¯
            print("ğŸ“‹ æœ¬åœ°æå–åŸºç¡€ä¿¡æ¯...")
            enhanced_info = enhance_paper_with_local_info(paper, self.info_extractor)
            
            # å‡†å¤‡æ‰€æœ‰æ–‡æœ¬å†…å®¹
            all_text = ''
            all_text += 'Title: ' + paper.title + '\n'
            all_text += 'Paper_info: ' + paper.section_text_dict['paper_info'] + '\n'
            
            # æ·»åŠ å„ä¸ªç« èŠ‚å†…å®¹
            for section_name, section_content in paper.section_text_dict.items():
                if section_name not in ['title', 'paper_info']:
                    all_text += f'{section_name}: {section_content}\n'
            
            original_length = len(all_text)
            original_tokens = self.token_manager.count_tokens(all_text)
            print(f"ğŸ“Š è®ºæ–‡å†…å®¹é•¿åº¦: {original_length} å­—ç¬¦, {original_tokens} tokens")
            
            # ç”Ÿæˆç»“æ„åŒ–æ€»ç»“
            print("ğŸ¤– å¼€å§‹ç”Ÿæˆç»“æ„åŒ–æ€»ç»“...")
            chat_conclusion_text = ""
            retry_count = 0
            max_retries = 3
            
            while retry_count < max_retries:
                try:
                    print(f"ğŸ”„ è°ƒç”¨API (å°è¯• {retry_count + 1}/{max_retries})")
                    chat_conclusion_text = self.chat_conclusion_with_local_info(
                        text=all_text, 
                        enhanced_info=enhanced_info,
                        truncation_strategy=truncation_strategy
                    )
                    print("âœ… ç»“æ„åŒ–æ€»ç»“ç”ŸæˆæˆåŠŸ")
                    break
                except Exception as e:
                    retry_count += 1
                    print(f"âŒ APIè°ƒç”¨å¤±è´¥ (å°è¯• {retry_count}): {e}")
                    
                    if retry_count >= max_retries:
                        print("âš ï¸  è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡æ­¤è®ºæ–‡")
                        return False

            # ä¿å­˜ç»“æœåˆ°åˆå¹¶çš„CSVæ–‡ä»¶
            print("ğŸ’¾ ä¿å­˜ç»“æœåˆ°åˆå¹¶CSVæ–‡ä»¶...")
            self.append_to_merged_csv(chat_conclusion_text, paper.path)
            
            # åŒæ—¶ä¿å­˜å•ç‹¬çš„æ–‡ä»¶ä½œä¸ºå¤‡ä»½
            date_str = str(datetime.datetime.now())[:13].replace(' ', '-')
            backup_file = os.path.join(self.export_path, 'individual_backups',
                                     date_str + '-' + self.validateTitle(paper.title[:80]) + ".csv")
            
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_dir = os.path.dirname(backup_file)
            if not os.path.exists(backup_dir):
                os.makedirs(backup_dir)
            
            self.export_to_markdown(chat_conclusion_text, file_name=backup_file)
            
            # æ ‡è®°å®Œæˆå¹¶ä¿å­˜è¿›åº¦
            self.mark_paper_completed(paper.path, self.merged_csv_file, progress)
            
            return True
            
        except Exception as e:
            print(f"âŒ å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {e}")
            return False

    def process_single_paper_thread_safe(self, paper, truncation_strategy, progress, thread_id):
        """çº¿ç¨‹å®‰å…¨çš„å•ç¯‡è®ºæ–‡å¤„ç†"""
        try:
            # æœ¬åœ°æå–åŸºç¡€ä¿¡æ¯
            enhanced_info = enhance_paper_with_local_info(paper, self.info_extractor)
            
            # å‡†å¤‡æ‰€æœ‰æ–‡æœ¬å†…å®¹
            all_text = ''
            all_text += 'Title: ' + paper.title + '\n'
            all_text += 'Paper_info: ' + paper.section_text_dict['paper_info'] + '\n'
            
            # æ·»åŠ å„ä¸ªç« èŠ‚å†…å®¹
            for section_name, section_content in paper.section_text_dict.items():
                if section_name not in ['title', 'paper_info']:
                    all_text += f'{section_name}: {section_content}\n'
            
            original_tokens = self.token_manager.count_tokens(all_text)
            
            # ç”Ÿæˆç»“æ„åŒ–æ€»ç»“
            chat_conclusion_text = ""
            retry_count = 0
            max_retries = 3
            
            while retry_count < max_retries:
                try:
                    chat_conclusion_text = self.chat_conclusion_with_local_info(
                        text=all_text, 
                        enhanced_info=enhanced_info,
                        truncation_strategy=truncation_strategy
                    )
                    break
                except Exception as e:
                    retry_count += 1
                    print(f"âŒ [{thread_id}] APIè°ƒç”¨å¤±è´¥ (å°è¯• {retry_count}): {e}")
                    if retry_count < max_retries:
                        time.sleep(2 ** retry_count)  # æŒ‡æ•°é€€é¿
                    
                    if retry_count >= max_retries:
                        print(f"âš ï¸  [{thread_id}] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡æ­¤è®ºæ–‡")
                        return False

            # ä¿å­˜ç»“æœåˆ°åˆå¹¶çš„CSVæ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
            self.append_to_merged_csv(chat_conclusion_text, paper.path)
            
            # åŒæ—¶ä¿å­˜å•ç‹¬çš„æ–‡ä»¶ä½œä¸ºå¤‡ä»½
            date_str = str(datetime.datetime.now())[:13].replace(' ', '-')
            backup_file = os.path.join(self.export_path, 'individual_backups',
                                     date_str + '-' + self.validateTitle(paper.title[:80]) + ".csv")
            
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_dir = os.path.dirname(backup_file)
            if not os.path.exists(backup_dir):
                os.makedirs(backup_dir)
            
            self.export_to_markdown(chat_conclusion_text, file_name=backup_file)
            
            # æ ‡è®°å®Œæˆå¹¶ä¿å­˜è¿›åº¦ï¼ˆéœ€è¦ä¿æŠ¤å…±äº«èµ„æºï¼‰
            with self.csv_lock:
                self.mark_paper_completed(paper.path, self.merged_csv_file, progress)
            
            return True
            
        except Exception as e:
            print(f"âŒ [{thread_id}] å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {e}")
            return False

    def chat_conclusion_with_local_info(self, text, enhanced_info, conclusion_prompt_token=2000, truncation_strategy="balanced"):
        """ç»“åˆæœ¬åœ°ä¿¡æ¯çš„ç»“æ„åŒ–æ€»ç»“"""
        # ç¡®ä¿APIé…ç½®æ­£ç¡®
        openai.api_key = self.chat_api_list[self.cur_api]
        
        self.cur_api += 1
        self.cur_api = 0 if self.cur_api >= len(self.chat_api_list) - 1 else self.cur_api
        
        text_token = self.token_manager.count_tokens(text)
        print(f"åŸå§‹æ–‡æœ¬tokenæ•°: {text_token}")
        
        # ä½¿ç”¨æ™ºèƒ½æˆªæ–­
        processed_text = self.token_manager.smart_truncate(
            text, 
            reserved_tokens=conclusion_prompt_token, 
            strategy=truncation_strategy
        )
        processed_token = self.token_manager.count_tokens(processed_text)
        print(f"å¤„ç†åæ–‡æœ¬tokenæ•°: {processed_token}, ç­–ç•¥: {truncation_strategy}")
        
        return self._single_call_conclusion_with_local_info(processed_text, enhanced_info)

    def _single_call_conclusion_with_local_info(self, text, enhanced_info):
        """ç»“åˆæœ¬åœ°ä¿¡æ¯çš„å•æ¬¡LLMè°ƒç”¨"""
        
        # æ„å»ºæœ¬åœ°ä¿¡æ¯æç¤º
        local_info_prompt = f"""
å·²æå–çš„è®ºæ–‡åŸºç¡€ä¿¡æ¯:
- æ–‡ä»¶å: {enhanced_info['file_name']}
- æå–æ ‡é¢˜: {enhanced_info['extracted_title']}
- æå–ä½œè€…: {enhanced_info['extracted_authors']}
- æå–å¹´ä»½: {enhanced_info['extracted_year']}
- æå–æœŸåˆŠ/ä¼šè®®: {enhanced_info['extracted_venue']}
- DOI: {enhanced_info['extracted_doi']}
- arXiv ID: {enhanced_info['extracted_arxiv_id'] if enhanced_info['extracted_arxiv_id'] else 'æœªæåŠ'}
"""
        
        messages = [
            {"role": "system",
             "content": f"You are a research expert specializing in Vision Language Action (VLA) models and resource-constrained AI systems. Your expertise focuses on analyzing data bottlenecks and computational bottlenecks in VLA models. You are conducting a comprehensive survey on 'Resource-Constrained Vision Language Action Models' in the field of [{self.key_word}]."},
            {"role": "assistant",
             "content": f"I'll analyze this VLA-related paper with special attention to resource constraints, data bottlenecks, and computational bottlenecks. {local_info_prompt}\n\nHere's the complete paper content: {text}"},
            {"role": "user", "content": f"""
Please analyze this paper focusing on Vision Language Action (VLA) models and resource constraints. Output results in CSV format with ONLY ONE DATA LINE (no headers).

IMPORTANT: Use the provided extracted information for basic fields:
- File Name: Use "{enhanced_info['file_name']}"
- Title: Use "{enhanced_info['extracted_title']}" if available, otherwise extract from content
- Authors: Use "{enhanced_info['extracted_authors']}" if available
- Year: Use "{enhanced_info['extracted_year']}" if available
- Venue: Use "{enhanced_info['extracted_venue']}" if available
- DOI: Use "{enhanced_info['extracted_doi']}" if available
- arXiv ID: Use "{enhanced_info['extracted_arxiv_id'] if enhanced_info['extracted_arxiv_id'] else 'æœªæåŠ'}"

Expected format (single line):
"File Name","Paper Title","Authors","Year","Venue","DOI","arXiv ID","VLA Type","Main Contribution","Data Bottleneck","Compute Bottleneck","Resource Constraint Type","Data Type","Data Scale","Data Collection","Data Solution","Model Scale","Training Resources","Inference Efficiency","Compute Solution","Task Type","Environment","Performance","Resource-Performance Tradeoff","Advantages","Limitations","Future Work","Innovation"

ENHANCED Analysis Guidelines:

1. **VLAæ¶æ„ç±»å‹**: ä¸¥æ ¼åˆ†ç±»
   - "ç«¯åˆ°ç«¯VLA": ç»Ÿä¸€æ¨¡å‹å¤„ç†è§†è§‰-è¯­è¨€-åŠ¨ä½œ
   - "åˆ†å±‚å¼VLA": æ˜ç¡®çš„é«˜å±‚è§„åˆ’+ä½å±‚æ§åˆ¶åˆ†ç¦»
   - "æ··åˆæ¶æ„": ç»“åˆå¤šç§æ–¹æ³•æˆ–æ¨¡å—åŒ–è®¾è®¡
   - "éVLA": ä¸å±äºVLAèŒƒç•´çš„å·¥ä½œ

2. **æ•°æ®ç“¶é¢ˆ/ç®—åŠ›ç“¶é¢ˆ**: åªèƒ½æ˜¯"æ˜¯"æˆ–"å¦"
   - æ•°æ®ç“¶é¢ˆ: è®ºæ–‡ä¸­æ˜ç¡®æåˆ°æ•°æ®ç¨€ç¼ºã€æ ‡æ³¨æˆæœ¬é«˜ã€æ•°æ®è´¨é‡é—®é¢˜
   - ç®—åŠ›ç“¶é¢ˆ: è®ºæ–‡ä¸­æ˜ç¡®æåˆ°è®¡ç®—èµ„æºé™åˆ¶ã€è®­ç»ƒ/æ¨ç†æ—¶é—´é•¿ã€å†…å­˜ä¸è¶³

3. **æ ‡å‡†åŒ–æ ¼å¼è¦æ±‚**:
   - **å‘è¡¨å¹´ä»½**: ä¼˜å…ˆä½¿ç”¨æå–çš„å¹´ä»½ï¼Œæ ¼å¼ä¸º4ä½æ•°å­—
   - **ä½œè€…**: ä¼˜å…ˆä½¿ç”¨æå–çš„ä½œè€…ä¿¡æ¯
   - **æœŸåˆŠ/ä¼šè®®**: ä¼˜å…ˆä½¿ç”¨æå–çš„venueä¿¡æ¯
   - **DOI**: ä¼˜å…ˆä½¿ç”¨æå–çš„DOIä¿¡æ¯
   - **arXiv ID**: ä¼˜å…ˆä½¿ç”¨æå–çš„arXiv IDä¿¡æ¯
   - **æ•°æ®è§„æ¨¡**: ç»Ÿä¸€æ ¼å¼"æ•°é‡+å•ä½"ï¼Œå¦‚"100Kè½¨è¿¹"ã€"50å°æ—¶è§†é¢‘"
   - **æ¨¡å‹è§„æ¨¡**: ä¼˜å…ˆå†™å‚æ•°é‡ï¼Œå¦‚"7Bå‚æ•°"ã€"1.2Bå‚æ•°"
   - **è®­ç»ƒèµ„æº**: æ ¼å¼"GPUæ•°é‡Ã—GPUå‹å·ï¼Œè®­ç»ƒæ—¶é—´"
   - **æ¨ç†æ•ˆç‡**: æ ¼å¼"é¢‘ç‡/å»¶è¿Ÿ"ï¼Œå¦‚"10Hz"ã€"100mså»¶è¿Ÿ"

CRITICAL REQUIREMENTS:
- Output ONLY ONE line of data (no header row)  
- Use Chinese for descriptions, English for technical terms/model names
- Use "æœªæåŠ" for missing information consistently
- Prioritize extracted local information for basic fields
- Focus on quantitative rather than qualitative descriptions
"""},
        ]

        try:
            if hasattr(openai, 'api_type') and openai.api_type == 'azure':
                response = openai.ChatCompletion.create(
                    engine=self.chatgpt_model,
                    messages=messages,
                    temperature=0.0,
                    max_tokens=2000
                )
            else:
                response = openai.ChatCompletion.create(
                    model=self.chatgpt_model,
                    messages=messages,
                    temperature=0.0,
                    max_tokens=2000
                )
                
            result = ''
            for choice in response.choices:
                result += choice.message.content
            
            print(f"Tokenä½¿ç”¨æƒ…å†µ - è¾“å…¥: {response.usage.prompt_tokens}, è¾“å‡º: {response.usage.completion_tokens}, æ€»è®¡: {response.usage.total_tokens}")
            return result
            
        except Exception as api_error:
            print(f"APIè°ƒç”¨é”™è¯¯è¯¦æƒ…: {api_error}")
            raise api_error

    def export_to_markdown(self, text, file_name, mode='w'):
        """å¯¼å‡ºåˆ°markdownæ–‡ä»¶"""
        with open(file_name, mode, encoding="utf-8") as f:
            f.write(text)


class TokenManager:
    """æ™ºèƒ½Tokenç®¡ç†å™¨"""
    
    def __init__(self, max_token_num=4096, model="gpt-3.5-turbo"):
        self.max_token_num = max_token_num
        self.encoding = tiktoken.get_encoding("gpt2")
        self.model = model
        
    def count_tokens(self, text):
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        return len(self.encoding.encode(text))
    
    def smart_truncate(self, text, reserved_tokens=1000, strategy="balanced"):
        """æ™ºèƒ½æˆªæ–­æ–‡æœ¬"""
        text_tokens = self.count_tokens(text)
        max_content_tokens = self.max_token_num - reserved_tokens
        
        if text_tokens <= max_content_tokens:
            return text
        
        if strategy == "front":
            return self._truncate_front(text, max_content_tokens)
        elif strategy == "balanced":
            return self._truncate_balanced(text, max_content_tokens)
        elif strategy == "sections":
            return self._truncate_sections_priority(text, max_content_tokens)
        else:
            return self._truncate_balanced(text, max_content_tokens)
    
    def _truncate_front(self, text, max_tokens):
        """åªä¿ç•™å‰åŠéƒ¨åˆ†"""
        tokens = self.encoding.encode(text)
        if len(tokens) > max_tokens:
            truncated_tokens = tokens[:max_tokens]
            truncated_text = self.encoding.decode(truncated_tokens)
            return self._clean_truncation(truncated_text)
        return text
    
    def _truncate_balanced(self, text, max_tokens):
        """å¹³è¡¡æˆªæ–­ï¼šä¿ç•™å¼€å¤´å’Œç»“å°¾"""
        front_ratio = 0.4
        back_ratio = 0.4
        
        tokens = self.encoding.encode(text)
        if len(tokens) <= max_tokens:
            return text
            
        front_tokens = int(max_tokens * front_ratio)
        back_tokens = int(max_tokens * back_ratio)
        
        front_part = self.encoding.decode(tokens[:front_tokens])
        back_part = self.encoding.decode(tokens[-back_tokens:])
        
        separator = "\n\n[...å†…å®¹å› é•¿åº¦é™åˆ¶æœ‰æ‰€çœç•¥...]\n\n"
        combined = front_part + separator + back_part
        
        if self.count_tokens(combined) > max_tokens:
            separator_tokens = self.count_tokens(separator)
            available_tokens = max_tokens - separator_tokens
            front_tokens = int(available_tokens * 0.5)
            back_tokens = available_tokens - front_tokens
            
            front_part = self.encoding.decode(tokens[:front_tokens])
            back_part = self.encoding.decode(tokens[-back_tokens:])
            combined = front_part + separator + back_part
        
        return self._clean_truncation(combined)
    
    def _truncate_sections_priority(self, text, max_tokens):
        """ç« èŠ‚ä¼˜å…ˆæˆªæ–­ï¼šä¼˜å…ˆä¿ç•™é‡è¦ç« èŠ‚çš„å®Œæ•´å†…å®¹"""
        # å®šä¹‰ç« èŠ‚é‡è¦æ€§ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰
        priority_sections = [
            'Title:', 'Paper_info:', 'Abstract:', 
            'Introduction:', 'Method:', 'Methods:', 'Methodology:', 'Approach:', 'Approaches:',
            'Results:', 'Experimental Results:', 'Findings:', 
            'Conclusion:', 'Discussion:', 'Results and Discussion:',
            'Related Work:', 'Background:', 'Preliminary:', 'Problem Formulation:',
            'Experiments:', 'Experiment:', 'Evaluation:', 'Experiment Settings:',
            'References:'
        ]
        
        # æŒ‰ç« èŠ‚åˆ†å‰²æ–‡æœ¬
        sections = {}
        current_section = "Header"
        current_content = ""
        
        lines = text.split('\n')
        for line in lines:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°ç« èŠ‚
            is_new_section = False
            for section_name in priority_sections:
                if line.strip().startswith(section_name):
                    # ä¿å­˜å½“å‰ç« èŠ‚
                    if current_content.strip():
                        sections[current_section] = current_content.strip()
                    # å¼€å§‹æ–°ç« èŠ‚
                    current_section = section_name.rstrip(':')
                    current_content = line + '\n'
                    is_new_section = True
                    break
            
            if not is_new_section:
                current_content += line + '\n'
        
        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_content.strip():
            sections[current_section] = current_content.strip()
        
        # æŒ‰ä¼˜å…ˆçº§é€ä¸ªæ·»åŠ ç« èŠ‚
        result_text = ""
        used_tokens = 0
        
        for section_name in priority_sections:
            section_key = section_name.rstrip(':')
            if section_key in sections:
                section_content = sections[section_key]
                section_tokens = self.count_tokens(section_content)
                
                if used_tokens + section_tokens <= max_tokens:
                    result_text += section_content + '\n\n'
                    used_tokens += section_tokens
                else:
                    # å¦‚æœç« èŠ‚å¤ªé•¿ï¼Œå°è¯•æˆªæ–­è¯¥ç« èŠ‚
                    remaining_tokens = max_tokens - used_tokens
                    if remaining_tokens > 100:  # è‡³å°‘ä¿ç•™100ä¸ªtoken
                        truncated_section = self._truncate_front(section_content, remaining_tokens)
                        result_text += truncated_section + '\n\n[...è¯¥ç« èŠ‚å› é•¿åº¦é™åˆ¶è¢«æˆªæ–­...]\n\n'
                    break
        
        # å¦‚æœè¿˜æœ‰ç©ºé—´ï¼Œæ·»åŠ å…¶ä»–æœªå¤„ç†çš„ç« èŠ‚
        for section_key, section_content in sections.items():
            if section_key + ':' not in priority_sections:
                section_tokens = self.count_tokens(section_content)
                if used_tokens + section_tokens <= max_tokens:
                    result_text += section_content + '\n\n'
                    used_tokens += section_tokens
                else:
                    break
        
        return self._clean_truncation(result_text)
    
    def _clean_truncation(self, text):
        """æ¸…ç†æˆªæ–­åçš„æ–‡æœ¬"""
        sentences = text.split('.')
        if len(sentences) > 1 and sentences[-1].strip() and len(sentences[-1]) < 50:
            text = '.'.join(sentences[:-1]) + '.'
        return text.strip()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--pdf_path", type=str, default=r'demo.pdf', help="PDFæ–‡ä»¶è·¯å¾„æˆ–åŒ…å«PDFæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--key_word", type=str, default='reinforcement learning', help="ç ”ç©¶é¢†åŸŸå…³é”®è¯")
    parser.add_argument("--file_format", type=str, default='csv', help="å¯¼å‡ºçš„æ–‡ä»¶æ ¼å¼ (csv)")
    parser.add_argument("--language", type=str, default='zh', help="è¾“å‡ºè¯­è¨€ (zh/en)")
    parser.add_argument("--truncation_strategy", "--strategy", type=str, default="sections", 
                        choices=['front', 'balanced', 'sections'],
                        help="é•¿è®ºæ–‡å¤„ç†ç­–ç•¥: front(å‰åŠéƒ¨åˆ†) / balanced(å¹³è¡¡ä¿ç•™) / sections(ç« èŠ‚ä¼˜å…ˆ)")
    parser.add_argument("--max_tokens", type=int, default=60000, 
                        help="æœ€å¤§tokenæ•°é™åˆ¶ï¼ˆé»˜è®¤60000ä»¥å……åˆ†åˆ©ç”¨64kä¸Šä¸‹æ–‡ï¼‰")
    parser.add_argument("--resume", action="store_true", 
                        help="ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­å¤„ç†")
    parser.add_argument("--parallel", action="store_true", 
                        help="å¯ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼ˆæ¨èç”¨äºå¤šä¸ªPDFæ–‡ä»¶ï¼‰")
    parser.add_argument("--max_workers", type=int, default=3, 
                        help="å¹¶è¡Œå¤„ç†çš„æœ€å¤§çº¿ç¨‹æ•°ï¼ˆé»˜è®¤3ï¼‰")

    args = parser.parse_args()
    
    print(f"=== ChatPaper æœ¬åœ°PDFæ€»ç»“å·¥å…· ===")
    print(f"PDFè·¯å¾„: {args.pdf_path}")
    print(f"ç ”ç©¶é¢†åŸŸ: {args.key_word}")
    print(f"å¤„ç†ç­–ç•¥: {args.truncation_strategy}")
    print(f"æœ€å¤§tokens: {args.max_tokens}")
    print(f"æ–­ç‚¹ç»­ä¼ : {'å¯ç”¨' if args.resume else 'ç¦ç”¨'}")
    print(f"å¹¶è¡Œå¤„ç†: {'å¯ç”¨' if args.parallel else 'ç¦ç”¨'}")
    if args.parallel:
        print(f"çº¿ç¨‹æ•°: {args.max_workers}")
    
    reader = Reader(key_word=args.key_word, args=args)
    reader.max_token_num = args.max_tokens
    reader.token_manager.max_token_num = args.max_tokens
    
    # æ”¶é›†PDFæ–‡ä»¶è·¯å¾„ï¼Œä½†ä¸é¢„å¤„ç†
    pdf_paths = []
    if args.pdf_path.endswith(".pdf"):
        pdf_paths.append(args.pdf_path)
    else:
        for root, dirs, files in os.walk(args.pdf_path):
            print("root:", root, "dirs:", dirs, 'files:', files)
            for filename in files:
                if filename.endswith(".pdf"):
                    pdf_paths.append(os.path.join(root, filename))
    
    print(f"------------------æ‰¾åˆ°PDFæ–‡ä»¶æ•°: {len(pdf_paths)}------------------")
    
    # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
    if args.resume:
        progress = reader.load_progress()
        completed_count = sum(1 for p in progress.values() if p.get('status') == 'completed')
        print(f"å·²å®Œæˆè®ºæ–‡æ•°: {completed_count}")
    
    [print(f"{pdf_index + 1}. {pdf_path.split('/')[-1]}") for pdf_index, pdf_path in enumerate(pdf_paths)]
    
    if pdf_paths:
        if args.parallel and len(pdf_paths) > 1:
            # å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
            reader.summary_with_chat_parallel(pdf_paths, 
                                            truncation_strategy=args.truncation_strategy,
                                            max_workers=args.max_workers)
        else:
            # å•çº¿ç¨‹é¡ºåºå¤„ç†
            if args.parallel and len(pdf_paths) == 1:
                print("åªæœ‰ä¸€ä¸ªPDFæ–‡ä»¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å•çº¿ç¨‹æ¨¡å¼")
            reader.summary_with_chat(pdf_paths, truncation_strategy=args.truncation_strategy)
        print("\n=== å¤„ç†å®Œæˆ ===")
    else:
        print("æœªæ‰¾åˆ°PDFæ–‡ä»¶ï¼")
